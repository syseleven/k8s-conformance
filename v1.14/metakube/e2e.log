I0516 11:52:32.300933      16 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-005417164
I0516 11:52:32.301243      16 e2e.go:240] Starting e2e run "15e3616e-77d1-11e9-8275-42209be91bd2" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1558007550 - Will randomize all specs
Will run 204 of 3584 specs

May 16 11:52:32.487: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 11:52:32.489: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 16 11:52:32.518: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 16 11:52:32.569: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 16 11:52:32.569: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
May 16 11:52:32.569: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 16 11:52:32.583: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
May 16 11:52:32.583: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 16 11:52:32.583: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
May 16 11:52:32.583: INFO: e2e test version: v1.14.1
May 16 11:52:32.585: INFO: kube-apiserver version: v1.14.1
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:52:32.586: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
May 16 11:52:32.991: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 11:52:33.041: INFO: Waiting up to 5m0s for pod "downwardapi-volume-170a9b7b-77d1-11e9-8275-42209be91bd2" in namespace "projected-5102" to be "success or failure"
May 16 11:52:33.048: INFO: Pod "downwardapi-volume-170a9b7b-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.999252ms
May 16 11:52:35.085: INFO: Pod "downwardapi-volume-170a9b7b-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043357671s
May 16 11:52:37.102: INFO: Pod "downwardapi-volume-170a9b7b-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060875597s
May 16 11:52:39.227: INFO: Pod "downwardapi-volume-170a9b7b-77d1-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.18542756s
STEP: Saw pod success
May 16 11:52:39.227: INFO: Pod "downwardapi-volume-170a9b7b-77d1-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 11:52:39.234: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh pod downwardapi-volume-170a9b7b-77d1-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 11:52:39.654: INFO: Waiting for pod downwardapi-volume-170a9b7b-77d1-11e9-8275-42209be91bd2 to disappear
May 16 11:52:39.662: INFO: Pod downwardapi-volume-170a9b7b-77d1-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:52:39.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5102" for this suite.
May 16 11:52:46.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:52:46.632: INFO: namespace projected-5102 deletion completed in 6.862556649s

• [SLOW TEST:14.047 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:52:46.636: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-1f3d1550-77d1-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 11:52:46.842: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1f407f01-77d1-11e9-8275-42209be91bd2" in namespace "projected-500" to be "success or failure"
May 16 11:52:46.854: INFO: Pod "pod-projected-configmaps-1f407f01-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.074477ms
May 16 11:52:48.860: INFO: Pod "pod-projected-configmaps-1f407f01-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017645711s
May 16 11:52:50.874: INFO: Pod "pod-projected-configmaps-1f407f01-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031234949s
May 16 11:52:52.886: INFO: Pod "pod-projected-configmaps-1f407f01-77d1-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043746991s
STEP: Saw pod success
May 16 11:52:52.886: INFO: Pod "pod-projected-configmaps-1f407f01-77d1-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 11:52:52.893: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-projected-configmaps-1f407f01-77d1-11e9-8275-42209be91bd2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 16 11:52:53.190: INFO: Waiting for pod pod-projected-configmaps-1f407f01-77d1-11e9-8275-42209be91bd2 to disappear
May 16 11:52:53.198: INFO: Pod pod-projected-configmaps-1f407f01-77d1-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:52:53.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-500" for this suite.
May 16 11:52:59.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:52:59.597: INFO: namespace projected-500 deletion completed in 6.362946875s

• [SLOW TEST:12.961 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:52:59.597: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-26ef4baf-77d1-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 11:52:59.715: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-26f48b41-77d1-11e9-8275-42209be91bd2" in namespace "projected-1142" to be "success or failure"
May 16 11:52:59.731: INFO: Pod "pod-projected-secrets-26f48b41-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.419201ms
May 16 11:53:01.738: INFO: Pod "pod-projected-secrets-26f48b41-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022023991s
May 16 11:53:03.745: INFO: Pod "pod-projected-secrets-26f48b41-77d1-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029154391s
STEP: Saw pod success
May 16 11:53:03.745: INFO: Pod "pod-projected-secrets-26f48b41-77d1-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 11:53:03.752: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-projected-secrets-26f48b41-77d1-11e9-8275-42209be91bd2 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 16 11:53:03.858: INFO: Waiting for pod pod-projected-secrets-26f48b41-77d1-11e9-8275-42209be91bd2 to disappear
May 16 11:53:03.862: INFO: Pod pod-projected-secrets-26f48b41-77d1-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:53:03.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1142" for this suite.
May 16 11:53:09.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:53:10.292: INFO: namespace projected-1142 deletion completed in 6.423774186s

• [SLOW TEST:10.695 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:53:10.293: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5621
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 16 11:53:10.449: INFO: Found 0 stateful pods, waiting for 3
May 16 11:53:20.462: INFO: Found 2 stateful pods, waiting for 3
May 16 11:53:30.465: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 16 11:53:30.465: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 16 11:53:30.465: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 16 11:53:40.491: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 16 11:53:40.491: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 16 11:53:40.491: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 16 11:53:40.599: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 16 11:53:50.690: INFO: Updating stateful set ss2
May 16 11:53:50.705: INFO: Waiting for Pod statefulset-5621/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
May 16 11:54:00.847: INFO: Found 2 stateful pods, waiting for 3
May 16 11:54:10.864: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 16 11:54:10.864: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 16 11:54:10.864: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 16 11:54:10.934: INFO: Updating stateful set ss2
May 16 11:54:11.065: INFO: Waiting for Pod statefulset-5621/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 16 11:54:21.118: INFO: Updating stateful set ss2
May 16 11:54:21.147: INFO: Waiting for StatefulSet statefulset-5621/ss2 to complete update
May 16 11:54:21.147: INFO: Waiting for Pod statefulset-5621/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 16 11:54:31.161: INFO: Waiting for StatefulSet statefulset-5621/ss2 to complete update
May 16 11:54:31.161: INFO: Waiting for Pod statefulset-5621/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 16 11:54:41.168: INFO: Deleting all statefulset in ns statefulset-5621
May 16 11:54:41.175: INFO: Scaling statefulset ss2 to 0
May 16 11:55:01.243: INFO: Waiting for statefulset status.replicas updated to 0
May 16 11:55:01.253: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:55:01.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5621" for this suite.
May 16 11:55:09.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:55:09.793: INFO: namespace statefulset-5621 deletion completed in 8.440405728s

• [SLOW TEST:119.500 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:55:09.796: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:55:15.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6048" for this suite.
May 16 11:55:58.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:55:58.720: INFO: namespace kubelet-test-6048 deletion completed in 42.722895981s

• [SLOW TEST:48.925 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:55:58.722: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-91bbf246-77d1-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 11:55:58.907: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-91bdd04b-77d1-11e9-8275-42209be91bd2" in namespace "projected-6374" to be "success or failure"
May 16 11:55:58.922: INFO: Pod "pod-projected-configmaps-91bdd04b-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.24422ms
May 16 11:56:00.928: INFO: Pod "pod-projected-configmaps-91bdd04b-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020796309s
May 16 11:56:02.940: INFO: Pod "pod-projected-configmaps-91bdd04b-77d1-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032492402s
STEP: Saw pod success
May 16 11:56:02.940: INFO: Pod "pod-projected-configmaps-91bdd04b-77d1-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 11:56:02.951: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-projected-configmaps-91bdd04b-77d1-11e9-8275-42209be91bd2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 16 11:56:03.059: INFO: Waiting for pod pod-projected-configmaps-91bdd04b-77d1-11e9-8275-42209be91bd2 to disappear
May 16 11:56:03.063: INFO: Pod pod-projected-configmaps-91bdd04b-77d1-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:56:03.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6374" for this suite.
May 16 11:56:09.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:56:09.491: INFO: namespace projected-6374 deletion completed in 6.395776059s

• [SLOW TEST:10.769 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:56:09.493: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1514
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1514
STEP: Creating statefulset with conflicting port in namespace statefulset-1514
STEP: Waiting until pod test-pod will start running in namespace statefulset-1514
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1514
May 16 11:56:15.710: INFO: Observed stateful pod in namespace: statefulset-1514, name: ss-0, uid: 9ba5b774-77d1-11e9-9b2c-0a580af43164, status phase: Pending. Waiting for statefulset controller to delete.
May 16 11:56:16.087: INFO: Observed stateful pod in namespace: statefulset-1514, name: ss-0, uid: 9ba5b774-77d1-11e9-9b2c-0a580af43164, status phase: Failed. Waiting for statefulset controller to delete.
May 16 11:56:16.097: INFO: Observed stateful pod in namespace: statefulset-1514, name: ss-0, uid: 9ba5b774-77d1-11e9-9b2c-0a580af43164, status phase: Failed. Waiting for statefulset controller to delete.
May 16 11:56:16.138: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1514
STEP: Removing pod with conflicting port in namespace statefulset-1514
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1514 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 16 11:56:22.203: INFO: Deleting all statefulset in ns statefulset-1514
May 16 11:56:22.209: INFO: Scaling statefulset ss to 0
May 16 11:56:42.319: INFO: Waiting for statefulset status.replicas updated to 0
May 16 11:56:42.328: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:56:42.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1514" for this suite.
May 16 11:56:50.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:56:50.776: INFO: namespace statefulset-1514 deletion completed in 8.41223993s

• [SLOW TEST:41.283 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:56:50.776: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-b0c133d8-77d1-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 11:56:50.926: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b0c560e1-77d1-11e9-8275-42209be91bd2" in namespace "projected-1966" to be "success or failure"
May 16 11:56:50.956: INFO: Pod "pod-projected-secrets-b0c560e1-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 30.266045ms
May 16 11:56:52.964: INFO: Pod "pod-projected-secrets-b0c560e1-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037837856s
May 16 11:56:54.970: INFO: Pod "pod-projected-secrets-b0c560e1-77d1-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044008669s
STEP: Saw pod success
May 16 11:56:54.970: INFO: Pod "pod-projected-secrets-b0c560e1-77d1-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 11:56:54.978: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-projected-secrets-b0c560e1-77d1-11e9-8275-42209be91bd2 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 16 11:56:55.145: INFO: Waiting for pod pod-projected-secrets-b0c560e1-77d1-11e9-8275-42209be91bd2 to disappear
May 16 11:56:55.166: INFO: Pod pod-projected-secrets-b0c560e1-77d1-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:56:55.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1966" for this suite.
May 16 11:57:01.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:57:01.609: INFO: namespace projected-1966 deletion completed in 6.42929204s

• [SLOW TEST:10.833 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:57:01.609: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 16 11:57:01.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-258'
May 16 11:57:03.930: INFO: stderr: ""
May 16 11:57:03.930: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 16 11:57:04.937: INFO: Selector matched 1 pods for map[app:redis]
May 16 11:57:04.937: INFO: Found 0 / 1
May 16 11:57:05.941: INFO: Selector matched 1 pods for map[app:redis]
May 16 11:57:05.941: INFO: Found 0 / 1
May 16 11:57:06.937: INFO: Selector matched 1 pods for map[app:redis]
May 16 11:57:06.937: INFO: Found 0 / 1
May 16 11:57:07.939: INFO: Selector matched 1 pods for map[app:redis]
May 16 11:57:07.939: INFO: Found 0 / 1
May 16 11:57:08.936: INFO: Selector matched 1 pods for map[app:redis]
May 16 11:57:08.936: INFO: Found 0 / 1
May 16 11:57:09.938: INFO: Selector matched 1 pods for map[app:redis]
May 16 11:57:09.938: INFO: Found 1 / 1
May 16 11:57:09.938: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 16 11:57:09.944: INFO: Selector matched 1 pods for map[app:redis]
May 16 11:57:09.944: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 16 11:57:09.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 patch pod redis-master-97qgd --namespace=kubectl-258 -p {"metadata":{"annotations":{"x":"y"}}}'
May 16 11:57:10.038: INFO: stderr: ""
May 16 11:57:10.038: INFO: stdout: "pod/redis-master-97qgd patched\n"
STEP: checking annotations
May 16 11:57:10.052: INFO: Selector matched 1 pods for map[app:redis]
May 16 11:57:10.052: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:57:10.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-258" for this suite.
May 16 11:57:34.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:57:34.519: INFO: namespace kubectl-258 deletion completed in 24.441074451s

• [SLOW TEST:32.910 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:57:34.520: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
May 16 11:57:35.471: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 16 11:57:37.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 11:57:39.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 11:57:41.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 11:57:43.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 11:57:45.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 11:57:47.631: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693604655, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 11:57:52.131: INFO: Waited 2.488048439s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:57:53.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8170" for this suite.
May 16 11:57:59.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:57:59.456: INFO: namespace aggregator-8170 deletion completed in 6.376318534s

• [SLOW TEST:24.936 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:57:59.458: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 16 11:57:59.530: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:58:05.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3995" for this suite.
May 16 11:58:29.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:58:29.913: INFO: namespace init-container-3995 deletion completed in 24.428978923s

• [SLOW TEST:30.456 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:58:29.916: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 11:58:36.108: INFO: Waiting up to 5m0s for pod "client-envvars-ef741154-77d1-11e9-8275-42209be91bd2" in namespace "pods-1157" to be "success or failure"
May 16 11:58:36.114: INFO: Pod "client-envvars-ef741154-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.432983ms
May 16 11:58:38.138: INFO: Pod "client-envvars-ef741154-77d1-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029997389s
May 16 11:58:40.148: INFO: Pod "client-envvars-ef741154-77d1-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040331065s
STEP: Saw pod success
May 16 11:58:40.148: INFO: Pod "client-envvars-ef741154-77d1-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 11:58:40.154: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod client-envvars-ef741154-77d1-11e9-8275-42209be91bd2 container env3cont: <nil>
STEP: delete the pod
May 16 11:58:40.260: INFO: Waiting for pod client-envvars-ef741154-77d1-11e9-8275-42209be91bd2 to disappear
May 16 11:58:40.269: INFO: Pod client-envvars-ef741154-77d1-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:58:40.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1157" for this suite.
May 16 11:59:30.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:59:31.087: INFO: namespace pods-1157 deletion completed in 50.809690606s

• [SLOW TEST:61.173 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:59:31.089: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:59:37.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5266" for this suite.
May 16 11:59:43.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:59:44.393: INFO: namespace emptydir-wrapper-5266 deletion completed in 6.436222504s

• [SLOW TEST:13.305 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:59:44.394: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
May 16 11:59:44.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 api-versions'
May 16 11:59:44.721: INFO: stderr: ""
May 16 11:59:44.721: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvelero.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:59:44.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2945" for this suite.
May 16 11:59:50.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 11:59:51.141: INFO: namespace kubectl-2945 deletion completed in 6.413202268s

• [SLOW TEST:6.748 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 11:59:51.142: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 11:59:55.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8177" for this suite.
May 16 12:00:35.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:00:35.686: INFO: namespace kubelet-test-8177 deletion completed in 40.368860902s

• [SLOW TEST:44.545 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:00:35.686: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 16 12:00:35.800: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 16 12:00:35.814: INFO: Waiting for terminating namespaces to be deleted...
May 16 12:00:35.820: INFO: 
Logging pods the kubelet thinks is on node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh before test
May 16 12:00:35.924: INFO: tiller-deploy-796c9d7db6-hwwqm from kube-system started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.924: INFO: 	Container tiller ready: true, restart count 0
May 16 12:00:35.924: INFO: cluster-autoscaler-c8c56b5d9-2clck from kube-system started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.924: INFO: 	Container cluster-autoscaler ready: true, restart count 0
May 16 12:00:35.924: INFO: velero-5bcdd86b7c-2qd27 from velero started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.924: INFO: 	Container velero ready: true, restart count 0
May 16 12:00:35.924: INFO: sonobuoy-systemd-logs-daemon-set-8c9cd2b50a37440b-rvwgr from heptio-sonobuoy started at 2019-05-16 11:50:40 +0000 UTC (2 container statuses recorded)
May 16 12:00:35.924: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 12:00:35.925: INFO: 	Container systemd-logs ready: true, restart count 0
May 16 12:00:35.925: INFO: restic-mx74p from velero started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.925: INFO: 	Container velero ready: true, restart count 0
May 16 12:00:35.925: INFO: webterminal-85f9f78784-srg8n from webterminal started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.925: INFO: 	Container webterminal ready: true, restart count 0
May 16 12:00:35.925: INFO: coredns-6bd858f7c-qtwtk from kube-system started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.925: INFO: 	Container coredns ready: true, restart count 0
May 16 12:00:35.925: INFO: openvpn-client-5bbcf59684-hk8ll from kube-system started at 2019-05-16 11:44:48 +0000 UTC (2 container statuses recorded)
May 16 12:00:35.925: INFO: 	Container dnat-controller ready: true, restart count 0
May 16 12:00:35.925: INFO: 	Container openvpn-client ready: true, restart count 0
May 16 12:00:35.926: INFO: kubernetes-dashboard-57b5ff8798-8ts2c from kube-system started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.926: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May 16 12:00:35.926: INFO: coredns-6bd858f7c-mrns8 from kube-system started at 2019-05-16 11:44:49 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.926: INFO: 	Container coredns ready: true, restart count 0
May 16 12:00:35.926: INFO: node-exporter-7mblb from kube-system started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.926: INFO: 	Container node-exporter ready: true, restart count 0
May 16 12:00:35.926: INFO: kube-proxy-snhz7 from kube-system started at 2019-05-16 11:43:58 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.926: INFO: 	Container kube-proxy ready: true, restart count 0
May 16 12:00:35.926: INFO: canal-fvnrd from kube-system started at 2019-05-16 11:43:58 +0000 UTC (3 container statuses recorded)
May 16 12:00:35.926: INFO: 	Container calico-node ready: true, restart count 0
May 16 12:00:35.927: INFO: 	Container install-cni ready: true, restart count 0
May 16 12:00:35.927: INFO: 	Container kube-flannel ready: true, restart count 0
May 16 12:00:35.927: INFO: 
Logging pods the kubelet thinks is on node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u before test
May 16 12:00:35.982: INFO: kube-proxy-wrwbt from kube-system started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.983: INFO: 	Container kube-proxy ready: true, restart count 0
May 16 12:00:35.983: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-16 11:50:23 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.983: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 16 12:00:35.983: INFO: sonobuoy-e2e-job-d26d0e4387684e3e from heptio-sonobuoy started at 2019-05-16 11:50:39 +0000 UTC (2 container statuses recorded)
May 16 12:00:35.983: INFO: 	Container e2e ready: true, restart count 0
May 16 12:00:35.983: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 12:00:35.983: INFO: node-exporter-x2gxg from kube-system started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.983: INFO: 	Container node-exporter ready: true, restart count 0
May 16 12:00:35.983: INFO: canal-vdgj7 from kube-system started at 2019-05-16 11:43:57 +0000 UTC (3 container statuses recorded)
May 16 12:00:35.983: INFO: 	Container calico-node ready: true, restart count 0
May 16 12:00:35.984: INFO: 	Container install-cni ready: true, restart count 0
May 16 12:00:35.984: INFO: 	Container kube-flannel ready: true, restart count 0
May 16 12:00:35.984: INFO: sonobuoy-systemd-logs-daemon-set-8c9cd2b50a37440b-5xwn7 from heptio-sonobuoy started at 2019-05-16 11:50:39 +0000 UTC (2 container statuses recorded)
May 16 12:00:35.984: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 12:00:35.984: INFO: 	Container systemd-logs ready: true, restart count 0
May 16 12:00:35.984: INFO: restic-fvvxh from velero started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 12:00:35.984: INFO: 	Container velero ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-395e0828-77d2-11e9-8275-42209be91bd2 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-395e0828-77d2-11e9-8275-42209be91bd2 off the node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u
STEP: verifying the node doesn't have the label kubernetes.io/e2e-395e0828-77d2-11e9-8275-42209be91bd2
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:00:44.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-953" for this suite.
May 16 12:00:54.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:00:54.625: INFO: namespace sched-pred-953 deletion completed in 10.393173629s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:18.939 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:00:54.626: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 16 12:00:54.771: INFO: Waiting up to 5m0s for pod "pod-421b4070-77d2-11e9-8275-42209be91bd2" in namespace "emptydir-20" to be "success or failure"
May 16 12:00:54.777: INFO: Pod "pod-421b4070-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.515184ms
May 16 12:00:56.786: INFO: Pod "pod-421b4070-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013982631s
May 16 12:00:58.796: INFO: Pod "pod-421b4070-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024238073s
May 16 12:01:00.803: INFO: Pod "pod-421b4070-77d2-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031785735s
STEP: Saw pod success
May 16 12:01:00.804: INFO: Pod "pod-421b4070-77d2-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:01:00.809: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-421b4070-77d2-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 12:01:00.937: INFO: Waiting for pod pod-421b4070-77d2-11e9-8275-42209be91bd2 to disappear
May 16 12:01:00.943: INFO: Pod pod-421b4070-77d2-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:01:00.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-20" for this suite.
May 16 12:01:06.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:01:07.595: INFO: namespace emptydir-20 deletion completed in 6.644831164s

• [SLOW TEST:12.969 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:01:07.596: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
May 16 12:01:07.802: INFO: Waiting up to 5m0s for pod "client-containers-49d9fde6-77d2-11e9-8275-42209be91bd2" in namespace "containers-8234" to be "success or failure"
May 16 12:01:07.808: INFO: Pod "client-containers-49d9fde6-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.30494ms
May 16 12:01:09.813: INFO: Pod "client-containers-49d9fde6-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011128064s
May 16 12:01:11.824: INFO: Pod "client-containers-49d9fde6-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021363389s
May 16 12:01:13.834: INFO: Pod "client-containers-49d9fde6-77d2-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03216834s
STEP: Saw pod success
May 16 12:01:13.835: INFO: Pod "client-containers-49d9fde6-77d2-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:01:13.841: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod client-containers-49d9fde6-77d2-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 12:01:13.891: INFO: Waiting for pod client-containers-49d9fde6-77d2-11e9-8275-42209be91bd2 to disappear
May 16 12:01:13.901: INFO: Pod client-containers-49d9fde6-77d2-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:01:13.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8234" for this suite.
May 16 12:01:19.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:01:20.308: INFO: namespace containers-8234 deletion completed in 6.399091701s

• [SLOW TEST:12.712 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:01:20.309: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
May 16 12:01:20.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-603'
May 16 12:01:21.555: INFO: stderr: ""
May 16 12:01:21.555: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
May 16 12:01:22.564: INFO: Selector matched 1 pods for map[app:redis]
May 16 12:01:22.564: INFO: Found 0 / 1
May 16 12:01:23.564: INFO: Selector matched 1 pods for map[app:redis]
May 16 12:01:23.564: INFO: Found 0 / 1
May 16 12:01:24.569: INFO: Selector matched 1 pods for map[app:redis]
May 16 12:01:24.569: INFO: Found 1 / 1
May 16 12:01:24.569: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 16 12:01:24.575: INFO: Selector matched 1 pods for map[app:redis]
May 16 12:01:24.575: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
May 16 12:01:24.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 logs redis-master-fjm56 redis-master --namespace=kubectl-603'
May 16 12:01:24.820: INFO: stderr: ""
May 16 12:01:24.820: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 16 May 12:01:23.864 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 May 12:01:23.864 # Server started, Redis version 3.2.12\n1:M 16 May 12:01:23.864 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 May 12:01:23.864 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
May 16 12:01:24.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 log redis-master-fjm56 redis-master --namespace=kubectl-603 --tail=1'
May 16 12:01:24.931: INFO: stderr: ""
May 16 12:01:24.931: INFO: stdout: "1:M 16 May 12:01:23.864 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
May 16 12:01:24.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 log redis-master-fjm56 redis-master --namespace=kubectl-603 --limit-bytes=1'
May 16 12:01:25.049: INFO: stderr: ""
May 16 12:01:25.049: INFO: stdout: " "
STEP: exposing timestamps
May 16 12:01:25.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 log redis-master-fjm56 redis-master --namespace=kubectl-603 --tail=1 --timestamps'
May 16 12:01:25.186: INFO: stderr: ""
May 16 12:01:25.186: INFO: stdout: "2019-05-16T12:01:23.865001232Z 1:M 16 May 12:01:23.864 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
May 16 12:01:27.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 log redis-master-fjm56 redis-master --namespace=kubectl-603 --since=1s'
May 16 12:01:27.813: INFO: stderr: ""
May 16 12:01:27.813: INFO: stdout: ""
May 16 12:01:27.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 log redis-master-fjm56 redis-master --namespace=kubectl-603 --since=24h'
May 16 12:01:27.959: INFO: stderr: ""
May 16 12:01:27.959: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 16 May 12:01:23.864 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 May 12:01:23.864 # Server started, Redis version 3.2.12\n1:M 16 May 12:01:23.864 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 May 12:01:23.864 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
May 16 12:01:27.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete --grace-period=0 --force -f - --namespace=kubectl-603'
May 16 12:01:28.069: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 12:01:28.069: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
May 16 12:01:28.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get rc,svc -l name=nginx --no-headers --namespace=kubectl-603'
May 16 12:01:28.211: INFO: stderr: "No resources found.\n"
May 16 12:01:28.211: INFO: stdout: ""
May 16 12:01:28.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -l name=nginx --namespace=kubectl-603 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 16 12:01:28.296: INFO: stderr: ""
May 16 12:01:28.296: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:01:28.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-603" for this suite.
May 16 12:01:52.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:01:52.614: INFO: namespace kubectl-603 deletion completed in 24.30944554s

• [SLOW TEST:32.306 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:01:52.618: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-294/secret-test-64a76cee-77d2-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 12:01:52.737: INFO: Waiting up to 5m0s for pod "pod-configmaps-64a891a4-77d2-11e9-8275-42209be91bd2" in namespace "secrets-294" to be "success or failure"
May 16 12:01:52.744: INFO: Pod "pod-configmaps-64a891a4-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.807762ms
May 16 12:01:54.754: INFO: Pod "pod-configmaps-64a891a4-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016694112s
May 16 12:01:56.761: INFO: Pod "pod-configmaps-64a891a4-77d2-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023383254s
STEP: Saw pod success
May 16 12:01:56.761: INFO: Pod "pod-configmaps-64a891a4-77d2-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:01:56.767: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-configmaps-64a891a4-77d2-11e9-8275-42209be91bd2 container env-test: <nil>
STEP: delete the pod
May 16 12:01:56.846: INFO: Waiting for pod pod-configmaps-64a891a4-77d2-11e9-8275-42209be91bd2 to disappear
May 16 12:01:56.851: INFO: Pod pod-configmaps-64a891a4-77d2-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:01:56.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-294" for this suite.
May 16 12:02:02.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:02:03.252: INFO: namespace secrets-294 deletion completed in 6.392620909s

• [SLOW TEST:10.634 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:02:03.253: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 12:02:03.392: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6afe6eda-77d2-11e9-8275-42209be91bd2" in namespace "downward-api-5549" to be "success or failure"
May 16 12:02:03.398: INFO: Pod "downwardapi-volume-6afe6eda-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.720142ms
May 16 12:02:05.533: INFO: Pod "downwardapi-volume-6afe6eda-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.140935338s
May 16 12:02:07.553: INFO: Pod "downwardapi-volume-6afe6eda-77d2-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.16145409s
STEP: Saw pod success
May 16 12:02:07.553: INFO: Pod "downwardapi-volume-6afe6eda-77d2-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:02:07.558: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh pod downwardapi-volume-6afe6eda-77d2-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 12:02:07.899: INFO: Waiting for pod downwardapi-volume-6afe6eda-77d2-11e9-8275-42209be91bd2 to disappear
May 16 12:02:08.071: INFO: Pod downwardapi-volume-6afe6eda-77d2-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:02:08.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5549" for this suite.
May 16 12:02:14.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:02:14.555: INFO: namespace downward-api-5549 deletion completed in 6.477236656s

• [SLOW TEST:11.303 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:02:14.557: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:02:20.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5602" for this suite.
May 16 12:02:28.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:02:29.538: INFO: namespace namespaces-5602 deletion completed in 8.650511589s
STEP: Destroying namespace "nsdeletetest-2809" for this suite.
May 16 12:02:29.545: INFO: Namespace nsdeletetest-2809 was already deleted
STEP: Destroying namespace "nsdeletetest-7679" for this suite.
May 16 12:02:35.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:02:35.936: INFO: namespace nsdeletetest-7679 deletion completed in 6.391174922s

• [SLOW TEST:21.379 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:02:35.938: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:02:41.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1663" for this suite.
May 16 12:03:05.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:03:05.618: INFO: namespace replication-controller-1663 deletion completed in 24.468995851s

• [SLOW TEST:29.681 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:03:05.619: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-902b30d9-77d2-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 12:03:05.744: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-902c9bc4-77d2-11e9-8275-42209be91bd2" in namespace "projected-3659" to be "success or failure"
May 16 12:03:05.751: INFO: Pod "pod-projected-configmaps-902c9bc4-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.927043ms
May 16 12:03:07.757: INFO: Pod "pod-projected-configmaps-902c9bc4-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01348718s
May 16 12:03:09.766: INFO: Pod "pod-projected-configmaps-902c9bc4-77d2-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02254673s
STEP: Saw pod success
May 16 12:03:09.766: INFO: Pod "pod-projected-configmaps-902c9bc4-77d2-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:03:09.772: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-projected-configmaps-902c9bc4-77d2-11e9-8275-42209be91bd2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 16 12:03:09.819: INFO: Waiting for pod pod-projected-configmaps-902c9bc4-77d2-11e9-8275-42209be91bd2 to disappear
May 16 12:03:09.832: INFO: Pod pod-projected-configmaps-902c9bc4-77d2-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:03:09.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3659" for this suite.
May 16 12:03:15.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:03:16.227: INFO: namespace projected-3659 deletion completed in 6.38774152s

• [SLOW TEST:10.609 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:03:16.228: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
May 16 12:03:16.334: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-005417164 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:03:16.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2182" for this suite.
May 16 12:03:22.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:03:22.827: INFO: namespace kubectl-2182 deletion completed in 6.381570347s

• [SLOW TEST:6.599 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:03:22.828: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 16 12:03:22.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-7867'
May 16 12:03:23.079: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 16 12:03:23.079: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
May 16 12:03:27.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete deployment e2e-test-nginx-deployment --namespace=kubectl-7867'
May 16 12:03:27.248: INFO: stderr: ""
May 16 12:03:27.248: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:03:27.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7867" for this suite.
May 16 12:03:51.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:03:51.662: INFO: namespace kubectl-7867 deletion completed in 24.406137331s

• [SLOW TEST:28.835 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:03:51.665: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 16 12:03:51.824: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2911,SelfLink:/api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-watch-closed,UID:ab9b7e29-77d2-11e9-b44f-0a580af42602,ResourceVersion:6495,Generation:0,CreationTimestamp:2019-05-16 12:03:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 16 12:03:51.824: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2911,SelfLink:/api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-watch-closed,UID:ab9b7e29-77d2-11e9-b44f-0a580af42602,ResourceVersion:6496,Generation:0,CreationTimestamp:2019-05-16 12:03:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 16 12:03:51.865: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2911,SelfLink:/api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-watch-closed,UID:ab9b7e29-77d2-11e9-b44f-0a580af42602,ResourceVersion:6497,Generation:0,CreationTimestamp:2019-05-16 12:03:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 16 12:03:51.865: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2911,SelfLink:/api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-watch-closed,UID:ab9b7e29-77d2-11e9-b44f-0a580af42602,ResourceVersion:6498,Generation:0,CreationTimestamp:2019-05-16 12:03:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:03:51.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2911" for this suite.
May 16 12:03:57.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:03:58.234: INFO: namespace watch-2911 deletion completed in 6.360570128s

• [SLOW TEST:6.569 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:03:58.238: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 16 12:03:58.328: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:04:02.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6541" for this suite.
May 16 12:04:09.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:04:09.389: INFO: namespace init-container-6541 deletion completed in 6.42597832s

• [SLOW TEST:11.152 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:04:09.389: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 16 12:04:09.505: INFO: Waiting up to 5m0s for pod "pod-b62f6722-77d2-11e9-8275-42209be91bd2" in namespace "emptydir-2361" to be "success or failure"
May 16 12:04:09.511: INFO: Pod "pod-b62f6722-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.673172ms
May 16 12:04:11.518: INFO: Pod "pod-b62f6722-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012426667s
May 16 12:04:13.526: INFO: Pod "pod-b62f6722-77d2-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020484619s
STEP: Saw pod success
May 16 12:04:13.526: INFO: Pod "pod-b62f6722-77d2-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:04:13.533: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-b62f6722-77d2-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 12:04:13.598: INFO: Waiting for pod pod-b62f6722-77d2-11e9-8275-42209be91bd2 to disappear
May 16 12:04:13.607: INFO: Pod pod-b62f6722-77d2-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:04:13.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2361" for this suite.
May 16 12:04:19.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:04:19.991: INFO: namespace emptydir-2361 deletion completed in 6.377411009s

• [SLOW TEST:10.602 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:04:19.992: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
May 16 12:04:24.157: INFO: Pod pod-hostip-bc8398fe-77d2-11e9-8275-42209be91bd2 has hostIP: 192.168.1.12
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:04:24.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3122" for this suite.
May 16 12:04:48.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:04:48.658: INFO: namespace pods-3122 deletion completed in 24.494269622s

• [SLOW TEST:28.666 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:04:48.658: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-cd9b5e2f-77d2-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 12:04:48.865: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cd9ca1f6-77d2-11e9-8275-42209be91bd2" in namespace "projected-8831" to be "success or failure"
May 16 12:04:48.879: INFO: Pod "pod-projected-configmaps-cd9ca1f6-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.947659ms
May 16 12:04:50.886: INFO: Pod "pod-projected-configmaps-cd9ca1f6-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020627281s
May 16 12:04:52.898: INFO: Pod "pod-projected-configmaps-cd9ca1f6-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032855549s
May 16 12:04:54.910: INFO: Pod "pod-projected-configmaps-cd9ca1f6-77d2-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044655675s
STEP: Saw pod success
May 16 12:04:54.910: INFO: Pod "pod-projected-configmaps-cd9ca1f6-77d2-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:04:54.915: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-projected-configmaps-cd9ca1f6-77d2-11e9-8275-42209be91bd2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 16 12:04:54.977: INFO: Waiting for pod pod-projected-configmaps-cd9ca1f6-77d2-11e9-8275-42209be91bd2 to disappear
May 16 12:04:54.983: INFO: Pod pod-projected-configmaps-cd9ca1f6-77d2-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:04:54.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8831" for this suite.
May 16 12:05:01.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:05:01.355: INFO: namespace projected-8831 deletion completed in 6.365606653s

• [SLOW TEST:12.698 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:05:01.356: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 16 12:05:01.500: INFO: Waiting up to 5m0s for pod "pod-d529c0d3-77d2-11e9-8275-42209be91bd2" in namespace "emptydir-1255" to be "success or failure"
May 16 12:05:01.507: INFO: Pod "pod-d529c0d3-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.876763ms
May 16 12:05:03.513: INFO: Pod "pod-d529c0d3-77d2-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012059805s
May 16 12:05:05.520: INFO: Pod "pod-d529c0d3-77d2-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019431592s
STEP: Saw pod success
May 16 12:05:05.521: INFO: Pod "pod-d529c0d3-77d2-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:05:05.525: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-d529c0d3-77d2-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 12:05:05.641: INFO: Waiting for pod pod-d529c0d3-77d2-11e9-8275-42209be91bd2 to disappear
May 16 12:05:05.649: INFO: Pod pod-d529c0d3-77d2-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:05:05.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1255" for this suite.
May 16 12:05:11.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:05:12.109: INFO: namespace emptydir-1255 deletion completed in 6.440899255s

• [SLOW TEST:10.753 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:05:12.110: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 16 12:05:16.955: INFO: Successfully updated pod "labelsupdatedb98a46a-77d2-11e9-8275-42209be91bd2"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:05:19.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3356" for this suite.
May 16 12:05:43.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:05:43.562: INFO: namespace projected-3356 deletion completed in 24.413237682s

• [SLOW TEST:31.452 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:05:43.563: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-ee5bedff-77d2-11e9-8275-42209be91bd2
STEP: Creating secret with name s-test-opt-upd-ee5bee56-77d2-11e9-8275-42209be91bd2
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ee5bedff-77d2-11e9-8275-42209be91bd2
STEP: Updating secret s-test-opt-upd-ee5bee56-77d2-11e9-8275-42209be91bd2
STEP: Creating secret with name s-test-opt-create-ee5bee7a-77d2-11e9-8275-42209be91bd2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:05:50.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7814" for this suite.
May 16 12:06:14.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:06:14.788: INFO: namespace secrets-7814 deletion completed in 24.356474628s

• [SLOW TEST:31.226 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:06:14.789: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
May 16 12:06:14.900: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-005417164 proxy --unix-socket=/tmp/kubectl-proxy-unix035293983/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:06:14.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8288" for this suite.
May 16 12:06:21.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:06:21.396: INFO: namespace kubectl-8288 deletion completed in 6.433838039s

• [SLOW TEST:6.607 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:06:21.403: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 16 12:06:21.526: INFO: Waiting up to 5m0s for pod "pod-04dd4216-77d3-11e9-8275-42209be91bd2" in namespace "emptydir-8263" to be "success or failure"
May 16 12:06:21.530: INFO: Pod "pod-04dd4216-77d3-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008309ms
May 16 12:06:23.536: INFO: Pod "pod-04dd4216-77d3-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010090553s
May 16 12:06:25.547: INFO: Pod "pod-04dd4216-77d3-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020771787s
STEP: Saw pod success
May 16 12:06:25.547: INFO: Pod "pod-04dd4216-77d3-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:06:25.553: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-04dd4216-77d3-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 12:06:25.606: INFO: Waiting for pod pod-04dd4216-77d3-11e9-8275-42209be91bd2 to disappear
May 16 12:06:25.617: INFO: Pod pod-04dd4216-77d3-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:06:25.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8263" for this suite.
May 16 12:06:31.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:06:32.025: INFO: namespace emptydir-8263 deletion completed in 6.400603791s

• [SLOW TEST:10.623 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:06:32.026: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:06:57.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1913" for this suite.
May 16 12:07:04.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:07:04.317: INFO: namespace container-runtime-1913 deletion completed in 6.353935242s

• [SLOW TEST:32.291 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:07:04.321: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 12:07:04.420: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 16 12:07:04.435: INFO: Pod name sample-pod: Found 0 pods out of 1
May 16 12:07:09.446: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 16 12:07:09.447: INFO: Creating deployment "test-rolling-update-deployment"
May 16 12:07:09.458: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 16 12:07:09.468: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 16 12:07:11.490: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 16 12:07:11.497: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693605229, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693605229, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693605229, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693605229, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 12:07:13.504: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 16 12:07:13.524: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-1627,SelfLink:/apis/apps/v1/namespaces/deployment-1627/deployments/test-rolling-update-deployment,UID:216db328-77d3-11e9-b44f-0a580af42602,ResourceVersion:7481,Generation:1,CreationTimestamp:2019-05-16 12:07:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-16 12:07:09 +0000 UTC 2019-05-16 12:07:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-16 12:07:12 +0000 UTC 2019-05-16 12:07:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 16 12:07:13.536: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-1627,SelfLink:/apis/apps/v1/namespaces/deployment-1627/replicasets/test-rolling-update-deployment-67599b4d9,UID:2172e01e-77d3-11e9-b44f-0a580af42602,ResourceVersion:7470,Generation:1,CreationTimestamp:2019-05-16 12:07:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 216db328-77d3-11e9-b44f-0a580af42602 0xc0029b4bb0 0xc0029b4bb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 16 12:07:13.536: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 16 12:07:13.536: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-1627,SelfLink:/apis/apps/v1/namespaces/deployment-1627/replicasets/test-rolling-update-controller,UID:1e6ea1bb-77d3-11e9-b44f-0a580af42602,ResourceVersion:7480,Generation:2,CreationTimestamp:2019-05-16 12:07:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 216db328-77d3-11e9-b44f-0a580af42602 0xc0029b4adf 0xc0029b4af0}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 16 12:07:13.542: INFO: Pod "test-rolling-update-deployment-67599b4d9-bgf85" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-bgf85,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-1627,SelfLink:/api/v1/namespaces/deployment-1627/pods/test-rolling-update-deployment-67599b4d9-bgf85,UID:217580dd-77d3-11e9-b44f-0a580af42602,ResourceVersion:7469,Generation:0,CreationTimestamp:2019-05-16 12:07:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 2172e01e-77d3-11e9-b44f-0a580af42602 0xc0029b5430 0xc0029b5431}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k52br {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k52br,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-k52br true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b5490} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b54b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:07:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:07:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:07:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:07:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.0.45,StartTime:2019-05-16 12:07:09 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-16 12:07:11 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://ab3fa1b8e6a5b31239f022a94182833032935ce064be8f98b7aa10946e3af685}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:07:13.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1627" for this suite.
May 16 12:07:21.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:07:22.313: INFO: namespace deployment-1627 deletion completed in 8.76068058s

• [SLOW TEST:17.993 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:07:22.315: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-29298440-77d3-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 12:07:22.501: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-292b2ef5-77d3-11e9-8275-42209be91bd2" in namespace "projected-2325" to be "success or failure"
May 16 12:07:22.516: INFO: Pod "pod-projected-secrets-292b2ef5-77d3-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.242712ms
May 16 12:07:24.580: INFO: Pod "pod-projected-secrets-292b2ef5-77d3-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078824829s
May 16 12:07:26.588: INFO: Pod "pod-projected-secrets-292b2ef5-77d3-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.086408985s
STEP: Saw pod success
May 16 12:07:26.588: INFO: Pod "pod-projected-secrets-292b2ef5-77d3-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:07:26.592: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-projected-secrets-292b2ef5-77d3-11e9-8275-42209be91bd2 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 16 12:07:26.650: INFO: Waiting for pod pod-projected-secrets-292b2ef5-77d3-11e9-8275-42209be91bd2 to disappear
May 16 12:07:26.657: INFO: Pod pod-projected-secrets-292b2ef5-77d3-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:07:26.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2325" for this suite.
May 16 12:07:34.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:07:35.941: INFO: namespace projected-2325 deletion completed in 9.278202778s

• [SLOW TEST:13.627 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:07:35.942: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 16 12:07:36.106: INFO: PodSpec: initContainers in spec.initContainers
May 16 12:08:24.383: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-315661a8-77d3-11e9-8275-42209be91bd2", GenerateName:"", Namespace:"init-container-7573", SelfLink:"/api/v1/namespaces/init-container-7573/pods/pod-init-315661a8-77d3-11e9-8275-42209be91bd2", UID:"315267f7-77d3-11e9-b44f-0a580af42602", ResourceVersion:"7781", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63693605256, loc:(*time.Location)(0x8a060e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"106032011"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-4n48b", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002814940), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4n48b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4n48b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4n48b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001672a08), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002689da0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001672a80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001672aa0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001672aa8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001672aac)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693605256, loc:(*time.Location)(0x8a060e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693605256, loc:(*time.Location)(0x8a060e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693605256, loc:(*time.Location)(0x8a060e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693605256, loc:(*time.Location)(0x8a060e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.12", PodIP:"172.25.0.47", StartTime:(*v1.Time)(0xc002806760), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000a3f3b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000a3f490)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://e26bb0fea0be29e88914436cd01d64354d03f9a12ef60d24417a9b145b96e706"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0028067a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002806780), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:08:24.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7573" for this suite.
May 16 12:08:48.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:08:48.751: INFO: namespace init-container-7573 deletion completed in 24.355783809s

• [SLOW TEST:72.809 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:08:48.753: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0516 12:08:50.267216      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 16 12:08:50.267: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:08:50.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1914" for this suite.
May 16 12:08:56.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:08:56.618: INFO: namespace gc-1914 deletion completed in 6.345576909s

• [SLOW TEST:7.865 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:08:56.619: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-5hf7
STEP: Creating a pod to test atomic-volume-subpath
May 16 12:08:56.790: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-5hf7" in namespace "subpath-4562" to be "success or failure"
May 16 12:08:56.799: INFO: Pod "pod-subpath-test-secret-5hf7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.58845ms
May 16 12:08:58.812: INFO: Pod "pod-subpath-test-secret-5hf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022025164s
May 16 12:09:00.820: INFO: Pod "pod-subpath-test-secret-5hf7": Phase="Running", Reason="", readiness=true. Elapsed: 4.030223031s
May 16 12:09:02.828: INFO: Pod "pod-subpath-test-secret-5hf7": Phase="Running", Reason="", readiness=true. Elapsed: 6.038076015s
May 16 12:09:04.835: INFO: Pod "pod-subpath-test-secret-5hf7": Phase="Running", Reason="", readiness=true. Elapsed: 8.045779718s
May 16 12:09:06.843: INFO: Pod "pod-subpath-test-secret-5hf7": Phase="Running", Reason="", readiness=true. Elapsed: 10.052986856s
May 16 12:09:08.849: INFO: Pod "pod-subpath-test-secret-5hf7": Phase="Running", Reason="", readiness=true. Elapsed: 12.059427453s
May 16 12:09:10.858: INFO: Pod "pod-subpath-test-secret-5hf7": Phase="Running", Reason="", readiness=true. Elapsed: 14.067917082s
May 16 12:09:12.870: INFO: Pod "pod-subpath-test-secret-5hf7": Phase="Running", Reason="", readiness=true. Elapsed: 16.080189325s
May 16 12:09:14.877: INFO: Pod "pod-subpath-test-secret-5hf7": Phase="Running", Reason="", readiness=true. Elapsed: 18.087684486s
May 16 12:09:16.884: INFO: Pod "pod-subpath-test-secret-5hf7": Phase="Running", Reason="", readiness=true. Elapsed: 20.094006833s
May 16 12:09:18.898: INFO: Pod "pod-subpath-test-secret-5hf7": Phase="Running", Reason="", readiness=true. Elapsed: 22.108446711s
May 16 12:09:20.926: INFO: Pod "pod-subpath-test-secret-5hf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.136138786s
STEP: Saw pod success
May 16 12:09:20.926: INFO: Pod "pod-subpath-test-secret-5hf7" satisfied condition "success or failure"
May 16 12:09:20.930: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-subpath-test-secret-5hf7 container test-container-subpath-secret-5hf7: <nil>
STEP: delete the pod
May 16 12:09:21.009: INFO: Waiting for pod pod-subpath-test-secret-5hf7 to disappear
May 16 12:09:21.025: INFO: Pod pod-subpath-test-secret-5hf7 no longer exists
STEP: Deleting pod pod-subpath-test-secret-5hf7
May 16 12:09:21.025: INFO: Deleting pod "pod-subpath-test-secret-5hf7" in namespace "subpath-4562"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:09:21.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4562" for this suite.
May 16 12:09:27.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:09:27.425: INFO: namespace subpath-4562 deletion completed in 6.380671548s

• [SLOW TEST:30.806 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:09:27.425: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-2592
May 16 12:09:33.549: INFO: Started pod liveness-http in namespace container-probe-2592
STEP: checking the pod's current state and verifying that restartCount is present
May 16 12:09:33.555: INFO: Initial restart count of pod liveness-http is 0
May 16 12:09:51.642: INFO: Restart count of pod container-probe-2592/liveness-http is now 1 (18.085816346s elapsed)
May 16 12:10:11.741: INFO: Restart count of pod container-probe-2592/liveness-http is now 2 (38.185693301s elapsed)
May 16 12:10:31.850: INFO: Restart count of pod container-probe-2592/liveness-http is now 3 (58.294550309s elapsed)
May 16 12:10:52.090: INFO: Restart count of pod container-probe-2592/liveness-http is now 4 (1m18.534098454s elapsed)
May 16 12:12:06.557: INFO: Restart count of pod container-probe-2592/liveness-http is now 5 (2m33.001541557s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:12:06.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2592" for this suite.
May 16 12:12:12.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:12:12.933: INFO: namespace container-probe-2592 deletion completed in 6.327453012s

• [SLOW TEST:165.508 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:12:12.933: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
May 16 12:12:13.075: INFO: Waiting up to 5m0s for pod "pod-d6665c82-77d3-11e9-8275-42209be91bd2" in namespace "emptydir-648" to be "success or failure"
May 16 12:12:13.083: INFO: Pod "pod-d6665c82-77d3-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.144936ms
May 16 12:12:15.091: INFO: Pod "pod-d6665c82-77d3-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015640331s
May 16 12:12:17.097: INFO: Pod "pod-d6665c82-77d3-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021161055s
STEP: Saw pod success
May 16 12:12:17.097: INFO: Pod "pod-d6665c82-77d3-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:12:17.102: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-d6665c82-77d3-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 12:12:17.155: INFO: Waiting for pod pod-d6665c82-77d3-11e9-8275-42209be91bd2 to disappear
May 16 12:12:17.163: INFO: Pod pod-d6665c82-77d3-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:12:17.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-648" for this suite.
May 16 12:12:25.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:12:25.865: INFO: namespace emptydir-648 deletion completed in 8.693776985s

• [SLOW TEST:12.932 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:12:25.866: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 16 12:12:25.994: INFO: Pod name pod-release: Found 0 pods out of 1
May 16 12:12:31.023: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:12:32.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5247" for this suite.
May 16 12:12:38.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:12:38.478: INFO: namespace replication-controller-5247 deletion completed in 6.369737157s

• [SLOW TEST:12.612 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:12:38.478: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-e5a22a8a-77d3-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 12:12:38.629: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5a3c5dd-77d3-11e9-8275-42209be91bd2" in namespace "configmap-752" to be "success or failure"
May 16 12:12:38.635: INFO: Pod "pod-configmaps-e5a3c5dd-77d3-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.774526ms
May 16 12:12:40.646: INFO: Pod "pod-configmaps-e5a3c5dd-77d3-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017399751s
May 16 12:12:42.652: INFO: Pod "pod-configmaps-e5a3c5dd-77d3-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023371606s
May 16 12:12:44.660: INFO: Pod "pod-configmaps-e5a3c5dd-77d3-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030819556s
STEP: Saw pod success
May 16 12:12:44.660: INFO: Pod "pod-configmaps-e5a3c5dd-77d3-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:12:44.665: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-configmaps-e5a3c5dd-77d3-11e9-8275-42209be91bd2 container configmap-volume-test: <nil>
STEP: delete the pod
May 16 12:12:44.719: INFO: Waiting for pod pod-configmaps-e5a3c5dd-77d3-11e9-8275-42209be91bd2 to disappear
May 16 12:12:44.738: INFO: Pod pod-configmaps-e5a3c5dd-77d3-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:12:44.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-752" for this suite.
May 16 12:12:52.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:12:53.192: INFO: namespace configmap-752 deletion completed in 8.446138243s

• [SLOW TEST:14.714 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:12:53.193: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 12:12:53.522: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee80e7c5-77d3-11e9-8275-42209be91bd2" in namespace "projected-4234" to be "success or failure"
May 16 12:12:53.570: INFO: Pod "downwardapi-volume-ee80e7c5-77d3-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 47.681343ms
May 16 12:12:55.578: INFO: Pod "downwardapi-volume-ee80e7c5-77d3-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055541007s
May 16 12:12:57.584: INFO: Pod "downwardapi-volume-ee80e7c5-77d3-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061328697s
STEP: Saw pod success
May 16 12:12:57.584: INFO: Pod "downwardapi-volume-ee80e7c5-77d3-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:12:57.595: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-ee80e7c5-77d3-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 12:12:57.711: INFO: Waiting for pod downwardapi-volume-ee80e7c5-77d3-11e9-8275-42209be91bd2 to disappear
May 16 12:12:57.725: INFO: Pod downwardapi-volume-ee80e7c5-77d3-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:12:57.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4234" for this suite.
May 16 12:13:03.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:13:04.127: INFO: namespace projected-4234 deletion completed in 6.397075443s

• [SLOW TEST:10.934 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:13:04.128: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9339.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9339.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 177.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.177_udp@PTR;check="$$(dig +tcp +noall +answer +search 177.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.177_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9339.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9339.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 177.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.177_udp@PTR;check="$$(dig +tcp +noall +answer +search 177.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.177_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 16 12:13:34.471: INFO: Unable to read wheezy_udp@dns-test-service.dns-9339.svc.cluster.local from pod dns-9339/dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2: the server could not find the requested resource (get pods dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2)
May 16 12:13:34.522: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9339.svc.cluster.local from pod dns-9339/dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2: the server could not find the requested resource (get pods dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2)
May 16 12:13:34.540: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9339.svc.cluster.local from pod dns-9339/dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2: the server could not find the requested resource (get pods dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2)
May 16 12:13:34.549: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9339.svc.cluster.local from pod dns-9339/dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2: the server could not find the requested resource (get pods dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2)
May 16 12:13:35.047: INFO: Unable to read jessie_udp@dns-test-service.dns-9339.svc.cluster.local from pod dns-9339/dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2: the server could not find the requested resource (get pods dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2)
May 16 12:13:35.061: INFO: Unable to read jessie_tcp@dns-test-service.dns-9339.svc.cluster.local from pod dns-9339/dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2: the server could not find the requested resource (get pods dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2)
May 16 12:13:35.082: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9339.svc.cluster.local from pod dns-9339/dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2: the server could not find the requested resource (get pods dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2)
May 16 12:13:35.101: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9339.svc.cluster.local from pod dns-9339/dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2: the server could not find the requested resource (get pods dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2)
May 16 12:13:35.531: INFO: Lookups using dns-9339/dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2 failed for: [wheezy_udp@dns-test-service.dns-9339.svc.cluster.local wheezy_tcp@dns-test-service.dns-9339.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9339.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9339.svc.cluster.local jessie_udp@dns-test-service.dns-9339.svc.cluster.local jessie_tcp@dns-test-service.dns-9339.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9339.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9339.svc.cluster.local]

May 16 12:13:42.140: INFO: DNS probes using dns-9339/dns-test-f4f486ed-77d3-11e9-8275-42209be91bd2 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:13:42.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9339" for this suite.
May 16 12:13:50.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:13:51.201: INFO: namespace dns-9339 deletion completed in 8.412735871s

• [SLOW TEST:47.073 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:13:51.201: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-10f55e53-77d4-11e9-8275-42209be91bd2
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-10f55e53-77d4-11e9-8275-42209be91bd2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:15:07.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6698" for this suite.
May 16 12:15:31.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:15:31.697: INFO: namespace projected-6698 deletion completed in 24.465904306s

• [SLOW TEST:100.496 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:15:31.698: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-1622
May 16 12:15:35.835: INFO: Started pod liveness-exec in namespace container-probe-1622
STEP: checking the pod's current state and verifying that restartCount is present
May 16 12:15:35.849: INFO: Initial restart count of pod liveness-exec is 0
May 16 12:16:24.111: INFO: Restart count of pod container-probe-1622/liveness-exec is now 1 (48.261778118s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:16:24.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1622" for this suite.
May 16 12:16:30.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:16:30.553: INFO: namespace container-probe-1622 deletion completed in 6.401297365s

• [SLOW TEST:58.855 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:16:30.554: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-15
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 16 12:16:30.688: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 16 12:16:59.032: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.58 8081 | grep -v '^\s*$'] Namespace:pod-network-test-15 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:16:59.032: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:17:00.572: INFO: Found all expected endpoints: [netserver-0]
May 16 12:17:00.582: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.18 8081 | grep -v '^\s*$'] Namespace:pod-network-test-15 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:17:00.582: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:17:02.216: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:17:02.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-15" for this suite.
May 16 12:17:26.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:17:26.640: INFO: namespace pod-network-test-15 deletion completed in 24.415760108s

• [SLOW TEST:56.087 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:17:26.643: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 16 12:17:26.748: INFO: Waiting up to 5m0s for pod "pod-915ec563-77d4-11e9-8275-42209be91bd2" in namespace "emptydir-6342" to be "success or failure"
May 16 12:17:26.752: INFO: Pod "pod-915ec563-77d4-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.728524ms
May 16 12:17:28.760: INFO: Pod "pod-915ec563-77d4-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0123231s
May 16 12:17:30.767: INFO: Pod "pod-915ec563-77d4-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019133057s
STEP: Saw pod success
May 16 12:17:30.767: INFO: Pod "pod-915ec563-77d4-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:17:30.773: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-915ec563-77d4-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 12:17:30.903: INFO: Waiting for pod pod-915ec563-77d4-11e9-8275-42209be91bd2 to disappear
May 16 12:17:30.911: INFO: Pod pod-915ec563-77d4-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:17:30.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6342" for this suite.
May 16 12:17:36.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:17:37.336: INFO: namespace emptydir-6342 deletion completed in 6.417940096s

• [SLOW TEST:10.694 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:17:37.338: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-97c18f56-77d4-11e9-8275-42209be91bd2
STEP: Creating secret with name secret-projected-all-test-volume-97c18f38-77d4-11e9-8275-42209be91bd2
STEP: Creating a pod to test Check all projections for projected volume plugin
May 16 12:17:37.516: INFO: Waiting up to 5m0s for pod "projected-volume-97c18ee5-77d4-11e9-8275-42209be91bd2" in namespace "projected-605" to be "success or failure"
May 16 12:17:37.532: INFO: Pod "projected-volume-97c18ee5-77d4-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.066941ms
May 16 12:17:39.571: INFO: Pod "projected-volume-97c18ee5-77d4-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054330345s
May 16 12:17:41.804: INFO: Pod "projected-volume-97c18ee5-77d4-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.287558654s
STEP: Saw pod success
May 16 12:17:41.804: INFO: Pod "projected-volume-97c18ee5-77d4-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:17:41.812: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod projected-volume-97c18ee5-77d4-11e9-8275-42209be91bd2 container projected-all-volume-test: <nil>
STEP: delete the pod
May 16 12:17:41.903: INFO: Waiting for pod projected-volume-97c18ee5-77d4-11e9-8275-42209be91bd2 to disappear
May 16 12:17:41.920: INFO: Pod projected-volume-97c18ee5-77d4-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:17:41.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-605" for this suite.
May 16 12:17:47.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:17:48.299: INFO: namespace projected-605 deletion completed in 6.37059342s

• [SLOW TEST:10.961 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:17:48.300: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 12:18:10.443: INFO: Container started at 2019-05-16 12:17:51 +0000 UTC, pod became ready at 2019-05-16 12:18:08 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:18:10.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2555" for this suite.
May 16 12:18:34.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:18:34.875: INFO: namespace container-probe-2555 deletion completed in 24.425483414s

• [SLOW TEST:46.576 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:18:34.875: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
May 16 12:18:34.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-3644'
May 16 12:18:36.968: INFO: stderr: ""
May 16 12:18:36.969: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 16 12:18:36.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3644'
May 16 12:18:37.076: INFO: stderr: ""
May 16 12:18:37.076: INFO: stdout: "update-demo-nautilus-xlwpg update-demo-nautilus-z4kqw "
May 16 12:18:37.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-xlwpg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:18:37.173: INFO: stderr: ""
May 16 12:18:37.173: INFO: stdout: ""
May 16 12:18:37.173: INFO: update-demo-nautilus-xlwpg is created but not running
May 16 12:18:42.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3644'
May 16 12:18:42.288: INFO: stderr: ""
May 16 12:18:42.288: INFO: stdout: "update-demo-nautilus-xlwpg update-demo-nautilus-z4kqw "
May 16 12:18:42.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-xlwpg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:18:42.389: INFO: stderr: ""
May 16 12:18:42.389: INFO: stdout: "true"
May 16 12:18:42.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-xlwpg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:18:42.475: INFO: stderr: ""
May 16 12:18:42.475: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 16 12:18:42.475: INFO: validating pod update-demo-nautilus-xlwpg
May 16 12:18:42.576: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 12:18:42.576: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 12:18:42.576: INFO: update-demo-nautilus-xlwpg is verified up and running
May 16 12:18:42.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-z4kqw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:18:42.701: INFO: stderr: ""
May 16 12:18:42.701: INFO: stdout: ""
May 16 12:18:42.701: INFO: update-demo-nautilus-z4kqw is created but not running
May 16 12:18:47.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3644'
May 16 12:18:47.815: INFO: stderr: ""
May 16 12:18:47.815: INFO: stdout: "update-demo-nautilus-xlwpg update-demo-nautilus-z4kqw "
May 16 12:18:47.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-xlwpg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:18:47.896: INFO: stderr: ""
May 16 12:18:47.896: INFO: stdout: "true"
May 16 12:18:47.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-xlwpg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:18:47.989: INFO: stderr: ""
May 16 12:18:47.989: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 16 12:18:47.989: INFO: validating pod update-demo-nautilus-xlwpg
May 16 12:18:48.002: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 12:18:48.002: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 12:18:48.002: INFO: update-demo-nautilus-xlwpg is verified up and running
May 16 12:18:48.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-z4kqw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:18:48.120: INFO: stderr: ""
May 16 12:18:48.121: INFO: stdout: "true"
May 16 12:18:48.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-z4kqw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:18:48.223: INFO: stderr: ""
May 16 12:18:48.223: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 16 12:18:48.223: INFO: validating pod update-demo-nautilus-z4kqw
May 16 12:18:48.327: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 12:18:48.327: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 12:18:48.327: INFO: update-demo-nautilus-z4kqw is verified up and running
STEP: rolling-update to new replication controller
May 16 12:18:48.331: INFO: scanned /root for discovery docs: <nil>
May 16 12:18:48.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-3644'
May 16 12:19:21.006: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 16 12:19:21.006: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 16 12:19:21.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3644'
May 16 12:19:21.147: INFO: stderr: ""
May 16 12:19:21.147: INFO: stdout: "update-demo-kitten-9hbsk update-demo-kitten-qpq2c "
May 16 12:19:21.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-kitten-9hbsk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:19:21.252: INFO: stderr: ""
May 16 12:19:21.252: INFO: stdout: "true"
May 16 12:19:21.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-kitten-9hbsk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:19:21.347: INFO: stderr: ""
May 16 12:19:21.347: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 16 12:19:21.347: INFO: validating pod update-demo-kitten-9hbsk
May 16 12:19:51.375: INFO: update-demo-kitten-9hbsk is running right image but validator function failed: the server is currently unable to handle the request (get pods update-demo-kitten-9hbsk)
May 16 12:19:56.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3644'
May 16 12:19:56.486: INFO: stderr: ""
May 16 12:19:56.486: INFO: stdout: "update-demo-kitten-9hbsk update-demo-kitten-qpq2c "
May 16 12:19:56.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-kitten-9hbsk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:19:56.588: INFO: stderr: ""
May 16 12:19:56.588: INFO: stdout: "true"
May 16 12:19:56.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-kitten-9hbsk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:19:56.699: INFO: stderr: ""
May 16 12:19:56.699: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 16 12:19:56.699: INFO: validating pod update-demo-kitten-9hbsk
May 16 12:20:26.708: INFO: update-demo-kitten-9hbsk is running right image but validator function failed: the server is currently unable to handle the request (get pods update-demo-kitten-9hbsk)
May 16 12:20:31.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3644'
May 16 12:20:31.832: INFO: stderr: ""
May 16 12:20:31.832: INFO: stdout: "update-demo-kitten-9hbsk update-demo-kitten-qpq2c "
May 16 12:20:31.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-kitten-9hbsk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:20:31.937: INFO: stderr: ""
May 16 12:20:31.937: INFO: stdout: "true"
May 16 12:20:31.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-kitten-9hbsk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:20:32.027: INFO: stderr: ""
May 16 12:20:32.027: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 16 12:20:32.027: INFO: validating pod update-demo-kitten-9hbsk
May 16 12:20:32.125: INFO: got data: {
  "image": "kitten.jpg"
}

May 16 12:20:32.125: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 16 12:20:32.125: INFO: update-demo-kitten-9hbsk is verified up and running
May 16 12:20:32.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-kitten-qpq2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:20:32.222: INFO: stderr: ""
May 16 12:20:32.222: INFO: stdout: "true"
May 16 12:20:32.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-kitten-qpq2c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3644'
May 16 12:20:32.320: INFO: stderr: ""
May 16 12:20:32.320: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 16 12:20:32.320: INFO: validating pod update-demo-kitten-qpq2c
May 16 12:20:32.421: INFO: got data: {
  "image": "kitten.jpg"
}

May 16 12:20:32.421: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 16 12:20:32.421: INFO: update-demo-kitten-qpq2c is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:20:32.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3644" for this suite.
May 16 12:20:56.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:20:56.781: INFO: namespace kubectl-3644 deletion completed in 24.351124082s

• [SLOW TEST:141.906 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:20:56.782: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 12:20:56.882: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:21:01.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5702" for this suite.
May 16 12:21:41.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:21:41.365: INFO: namespace pods-5702 deletion completed in 40.329519713s

• [SLOW TEST:44.583 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:21:41.366: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 16 12:21:41.473: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:21:45.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6344" for this suite.
May 16 12:21:51.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:21:51.777: INFO: namespace init-container-6344 deletion completed in 6.431417847s

• [SLOW TEST:10.411 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:21:51.777: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 12:21:52.261: INFO: (0) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 115.714871ms)
May 16 12:21:52.270: INFO: (1) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.935709ms)
May 16 12:21:52.311: INFO: (2) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 40.520881ms)
May 16 12:21:52.325: INFO: (3) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.490075ms)
May 16 12:21:52.335: INFO: (4) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.418771ms)
May 16 12:21:52.349: INFO: (5) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 14.645061ms)
May 16 12:21:52.364: INFO: (6) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 14.983794ms)
May 16 12:21:52.375: INFO: (7) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.631447ms)
May 16 12:21:52.383: INFO: (8) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.011419ms)
May 16 12:21:52.394: INFO: (9) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.235305ms)
May 16 12:21:52.403: INFO: (10) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.932833ms)
May 16 12:21:52.419: INFO: (11) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 16.527438ms)
May 16 12:21:52.434: INFO: (12) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 14.921303ms)
May 16 12:21:52.448: INFO: (13) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.913173ms)
May 16 12:21:52.459: INFO: (14) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.881032ms)
May 16 12:21:52.486: INFO: (15) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 26.375077ms)
May 16 12:21:52.496: INFO: (16) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.572092ms)
May 16 12:21:52.506: INFO: (17) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.8441ms)
May 16 12:21:52.516: INFO: (18) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.469843ms)
May 16 12:21:52.525: INFO: (19) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.510837ms)
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:21:52.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9866" for this suite.
May 16 12:21:58.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:21:58.902: INFO: namespace proxy-9866 deletion completed in 6.371051623s

• [SLOW TEST:7.125 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:21:58.902: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 16 12:21:59.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-6963'
May 16 12:21:59.311: INFO: stderr: ""
May 16 12:21:59.311: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 16 12:21:59.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6963'
May 16 12:21:59.475: INFO: stderr: ""
May 16 12:21:59.475: INFO: stdout: "update-demo-nautilus-84sv2 update-demo-nautilus-bk2tg "
May 16 12:21:59.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-84sv2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:21:59.586: INFO: stderr: ""
May 16 12:21:59.586: INFO: stdout: ""
May 16 12:21:59.586: INFO: update-demo-nautilus-84sv2 is created but not running
May 16 12:22:04.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6963'
May 16 12:22:04.735: INFO: stderr: ""
May 16 12:22:04.735: INFO: stdout: "update-demo-nautilus-84sv2 update-demo-nautilus-bk2tg "
May 16 12:22:04.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-84sv2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:22:04.824: INFO: stderr: ""
May 16 12:22:04.824: INFO: stdout: "true"
May 16 12:22:04.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-84sv2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:22:05.049: INFO: stderr: ""
May 16 12:22:05.049: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 16 12:22:05.049: INFO: validating pod update-demo-nautilus-84sv2
May 16 12:22:05.172: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 12:22:05.172: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 12:22:05.172: INFO: update-demo-nautilus-84sv2 is verified up and running
May 16 12:22:05.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-bk2tg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:22:05.274: INFO: stderr: ""
May 16 12:22:05.274: INFO: stdout: "true"
May 16 12:22:05.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-bk2tg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:22:05.376: INFO: stderr: ""
May 16 12:22:05.376: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 16 12:22:05.376: INFO: validating pod update-demo-nautilus-bk2tg
May 16 12:22:05.472: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 12:22:05.472: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 12:22:05.472: INFO: update-demo-nautilus-bk2tg is verified up and running
STEP: scaling down the replication controller
May 16 12:22:05.474: INFO: scanned /root for discovery docs: <nil>
May 16 12:22:05.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6963'
May 16 12:22:06.647: INFO: stderr: ""
May 16 12:22:06.647: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 16 12:22:06.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6963'
May 16 12:22:06.732: INFO: stderr: ""
May 16 12:22:06.732: INFO: stdout: "update-demo-nautilus-84sv2 update-demo-nautilus-bk2tg "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 16 12:22:11.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6963'
May 16 12:22:11.848: INFO: stderr: ""
May 16 12:22:11.848: INFO: stdout: "update-demo-nautilus-bk2tg "
May 16 12:22:11.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-bk2tg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:22:11.970: INFO: stderr: ""
May 16 12:22:11.970: INFO: stdout: "true"
May 16 12:22:11.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-bk2tg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:22:12.081: INFO: stderr: ""
May 16 12:22:12.081: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 16 12:22:12.081: INFO: validating pod update-demo-nautilus-bk2tg
May 16 12:22:12.092: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 12:22:12.092: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 12:22:12.092: INFO: update-demo-nautilus-bk2tg is verified up and running
STEP: scaling up the replication controller
May 16 12:22:12.094: INFO: scanned /root for discovery docs: <nil>
May 16 12:22:12.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6963'
May 16 12:22:13.255: INFO: stderr: ""
May 16 12:22:13.255: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 16 12:22:13.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6963'
May 16 12:22:13.372: INFO: stderr: ""
May 16 12:22:13.372: INFO: stdout: "update-demo-nautilus-bk2tg update-demo-nautilus-rtncn "
May 16 12:22:13.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-bk2tg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:22:13.489: INFO: stderr: ""
May 16 12:22:13.489: INFO: stdout: "true"
May 16 12:22:13.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-bk2tg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:22:13.607: INFO: stderr: ""
May 16 12:22:13.607: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 16 12:22:13.607: INFO: validating pod update-demo-nautilus-bk2tg
May 16 12:22:13.616: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 12:22:13.616: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 12:22:13.616: INFO: update-demo-nautilus-bk2tg is verified up and running
May 16 12:22:13.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-rtncn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:22:13.720: INFO: stderr: ""
May 16 12:22:13.720: INFO: stdout: ""
May 16 12:22:13.720: INFO: update-demo-nautilus-rtncn is created but not running
May 16 12:22:18.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6963'
May 16 12:22:18.812: INFO: stderr: ""
May 16 12:22:18.812: INFO: stdout: "update-demo-nautilus-bk2tg update-demo-nautilus-rtncn "
May 16 12:22:18.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-bk2tg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:22:18.915: INFO: stderr: ""
May 16 12:22:18.915: INFO: stdout: "true"
May 16 12:22:18.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-bk2tg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:22:19.024: INFO: stderr: ""
May 16 12:22:19.024: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 16 12:22:19.024: INFO: validating pod update-demo-nautilus-bk2tg
May 16 12:22:19.080: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 12:22:19.080: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 12:22:19.080: INFO: update-demo-nautilus-bk2tg is verified up and running
May 16 12:22:19.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-rtncn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:22:19.174: INFO: stderr: ""
May 16 12:22:19.174: INFO: stdout: "true"
May 16 12:22:19.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-rtncn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6963'
May 16 12:22:19.258: INFO: stderr: ""
May 16 12:22:19.258: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 16 12:22:19.258: INFO: validating pod update-demo-nautilus-rtncn
May 16 12:22:19.356: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 12:22:19.356: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 12:22:19.356: INFO: update-demo-nautilus-rtncn is verified up and running
STEP: using delete to clean up resources
May 16 12:22:19.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete --grace-period=0 --force -f - --namespace=kubectl-6963'
May 16 12:22:19.497: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 12:22:19.498: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 16 12:22:19.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6963'
May 16 12:22:19.620: INFO: stderr: "No resources found.\n"
May 16 12:22:19.620: INFO: stdout: ""
May 16 12:22:19.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -l name=update-demo --namespace=kubectl-6963 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 16 12:22:19.728: INFO: stderr: ""
May 16 12:22:19.728: INFO: stdout: "update-demo-nautilus-bk2tg\nupdate-demo-nautilus-rtncn\n"
May 16 12:22:20.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6963'
May 16 12:22:20.360: INFO: stderr: "No resources found.\n"
May 16 12:22:20.360: INFO: stdout: ""
May 16 12:22:20.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -l name=update-demo --namespace=kubectl-6963 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 16 12:22:20.468: INFO: stderr: ""
May 16 12:22:20.468: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:22:20.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6963" for this suite.
May 16 12:22:44.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:22:44.948: INFO: namespace kubectl-6963 deletion completed in 24.472536242s

• [SLOW TEST:46.046 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:22:44.949: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-4f1f5bf7-77d5-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 12:22:45.108: INFO: Waiting up to 5m0s for pod "pod-secrets-4f218ed4-77d5-11e9-8275-42209be91bd2" in namespace "secrets-8312" to be "success or failure"
May 16 12:22:45.113: INFO: Pod "pod-secrets-4f218ed4-77d5-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.621342ms
May 16 12:22:47.121: INFO: Pod "pod-secrets-4f218ed4-77d5-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012752303s
May 16 12:22:49.127: INFO: Pod "pod-secrets-4f218ed4-77d5-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019122515s
STEP: Saw pod success
May 16 12:22:49.128: INFO: Pod "pod-secrets-4f218ed4-77d5-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:22:49.135: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-secrets-4f218ed4-77d5-11e9-8275-42209be91bd2 container secret-volume-test: <nil>
STEP: delete the pod
May 16 12:22:49.406: INFO: Waiting for pod pod-secrets-4f218ed4-77d5-11e9-8275-42209be91bd2 to disappear
May 16 12:22:49.412: INFO: Pod pod-secrets-4f218ed4-77d5-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:22:49.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8312" for this suite.
May 16 12:22:55.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:22:55.840: INFO: namespace secrets-8312 deletion completed in 6.422112485s

• [SLOW TEST:10.891 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:22:55.840: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 12:22:55.946: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55976817-77d5-11e9-8275-42209be91bd2" in namespace "downward-api-2976" to be "success or failure"
May 16 12:22:55.958: INFO: Pod "downwardapi-volume-55976817-77d5-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.411383ms
May 16 12:22:57.966: INFO: Pod "downwardapi-volume-55976817-77d5-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020294377s
May 16 12:22:59.974: INFO: Pod "downwardapi-volume-55976817-77d5-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027819684s
May 16 12:23:01.982: INFO: Pod "downwardapi-volume-55976817-77d5-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035730098s
STEP: Saw pod success
May 16 12:23:01.982: INFO: Pod "downwardapi-volume-55976817-77d5-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:23:01.988: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-55976817-77d5-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 12:23:02.165: INFO: Waiting for pod downwardapi-volume-55976817-77d5-11e9-8275-42209be91bd2 to disappear
May 16 12:23:02.170: INFO: Pod downwardapi-volume-55976817-77d5-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:23:02.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2976" for this suite.
May 16 12:23:08.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:23:08.541: INFO: namespace downward-api-2976 deletion completed in 6.353590331s

• [SLOW TEST:12.701 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:23:08.542: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-6090
May 16 12:23:12.688: INFO: Started pod liveness-exec in namespace container-probe-6090
STEP: checking the pod's current state and verifying that restartCount is present
May 16 12:23:12.694: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:27:14.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6090" for this suite.
May 16 12:27:20.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:27:20.447: INFO: namespace container-probe-6090 deletion completed in 6.3472293s

• [SLOW TEST:251.906 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:27:20.449: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-f353c2d8-77d5-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 12:27:20.596: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3550492-77d5-11e9-8275-42209be91bd2" in namespace "configmap-2339" to be "success or failure"
May 16 12:27:20.602: INFO: Pod "pod-configmaps-f3550492-77d5-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.268409ms
May 16 12:27:22.615: INFO: Pod "pod-configmaps-f3550492-77d5-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019541059s
May 16 12:27:24.624: INFO: Pod "pod-configmaps-f3550492-77d5-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027902286s
STEP: Saw pod success
May 16 12:27:24.624: INFO: Pod "pod-configmaps-f3550492-77d5-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:27:24.642: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-configmaps-f3550492-77d5-11e9-8275-42209be91bd2 container configmap-volume-test: <nil>
STEP: delete the pod
May 16 12:27:24.798: INFO: Waiting for pod pod-configmaps-f3550492-77d5-11e9-8275-42209be91bd2 to disappear
May 16 12:27:24.803: INFO: Pod pod-configmaps-f3550492-77d5-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:27:24.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2339" for this suite.
May 16 12:27:30.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:27:31.193: INFO: namespace configmap-2339 deletion completed in 6.374403163s

• [SLOW TEST:10.744 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:27:31.195: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-f9b750ab-77d5-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 12:27:31.342: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f9b98dc6-77d5-11e9-8275-42209be91bd2" in namespace "projected-9157" to be "success or failure"
May 16 12:27:31.350: INFO: Pod "pod-projected-secrets-f9b98dc6-77d5-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.420786ms
May 16 12:27:33.362: INFO: Pod "pod-projected-secrets-f9b98dc6-77d5-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020215091s
May 16 12:27:35.372: INFO: Pod "pod-projected-secrets-f9b98dc6-77d5-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029278917s
STEP: Saw pod success
May 16 12:27:35.372: INFO: Pod "pod-projected-secrets-f9b98dc6-77d5-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:27:35.378: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-projected-secrets-f9b98dc6-77d5-11e9-8275-42209be91bd2 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 16 12:27:35.436: INFO: Waiting for pod pod-projected-secrets-f9b98dc6-77d5-11e9-8275-42209be91bd2 to disappear
May 16 12:27:35.440: INFO: Pod pod-projected-secrets-f9b98dc6-77d5-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:27:35.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9157" for this suite.
May 16 12:27:41.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:27:41.813: INFO: namespace projected-9157 deletion completed in 6.365056267s

• [SLOW TEST:10.618 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:27:41.813: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 12:27:41.907: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:27:46.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6863" for this suite.
May 16 12:28:28.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:28:28.673: INFO: namespace pods-6863 deletion completed in 42.339546182s

• [SLOW TEST:46.860 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:28:28.683: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:29:28.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3591" for this suite.
May 16 12:29:52.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:29:53.233: INFO: namespace container-probe-3591 deletion completed in 24.385459642s

• [SLOW TEST:84.550 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:29:53.234: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:29:53.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-523" for this suite.
May 16 12:29:59.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:29:59.720: INFO: namespace services-523 deletion completed in 6.319276153s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.487 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:29:59.723: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 16 12:30:04.459: INFO: Successfully updated pod "annotationupdate523f1971-77d6-11e9-8275-42209be91bd2"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:30:06.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9024" for this suite.
May 16 12:30:30.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:30:30.987: INFO: namespace projected-9024 deletion completed in 24.417128386s

• [SLOW TEST:31.264 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:30:30.987: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:30:58.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1244" for this suite.
May 16 12:31:04.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:31:04.832: INFO: namespace namespaces-1244 deletion completed in 6.322046287s
STEP: Destroying namespace "nsdeletetest-8839" for this suite.
May 16 12:31:04.838: INFO: Namespace nsdeletetest-8839 was already deleted
STEP: Destroying namespace "nsdeletetest-4294" for this suite.
May 16 12:31:10.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:31:11.233: INFO: namespace nsdeletetest-4294 deletion completed in 6.395595456s

• [SLOW TEST:40.246 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:31:11.234: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0516 12:31:17.433655      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 16 12:31:17.433: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:31:17.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9985" for this suite.
May 16 12:31:25.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:31:25.827: INFO: namespace gc-9985 deletion completed in 8.389359957s

• [SLOW TEST:14.593 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:31:25.828: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2938
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 16 12:31:25.937: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 16 12:31:54.353: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.84:8080/dial?request=hostName&protocol=http&host=172.25.0.83&port=8080&tries=1'] Namespace:pod-network-test-2938 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:31:54.353: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:31:54.940: INFO: Waiting for endpoints: map[]
May 16 12:31:54.946: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.84:8080/dial?request=hostName&protocol=http&host=172.25.1.27&port=8080&tries=1'] Namespace:pod-network-test-2938 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:31:54.946: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:31:55.552: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:31:55.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2938" for this suite.
May 16 12:32:29.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:32:29.922: INFO: namespace pod-network-test-2938 deletion completed in 34.354423649s

• [SLOW TEST:64.094 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:32:29.922: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 12:32:30.085: INFO: Waiting up to 5m0s for pod "downwardapi-volume-abcd97bd-77d6-11e9-8275-42209be91bd2" in namespace "downward-api-4898" to be "success or failure"
May 16 12:32:30.102: INFO: Pod "downwardapi-volume-abcd97bd-77d6-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.052821ms
May 16 12:32:32.108: INFO: Pod "downwardapi-volume-abcd97bd-77d6-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023211728s
May 16 12:32:34.129: INFO: Pod "downwardapi-volume-abcd97bd-77d6-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043922091s
May 16 12:32:36.138: INFO: Pod "downwardapi-volume-abcd97bd-77d6-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053414982s
May 16 12:32:38.146: INFO: Pod "downwardapi-volume-abcd97bd-77d6-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.061263919s
STEP: Saw pod success
May 16 12:32:38.146: INFO: Pod "downwardapi-volume-abcd97bd-77d6-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:32:38.159: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-abcd97bd-77d6-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 12:32:38.314: INFO: Waiting for pod downwardapi-volume-abcd97bd-77d6-11e9-8275-42209be91bd2 to disappear
May 16 12:32:38.321: INFO: Pod downwardapi-volume-abcd97bd-77d6-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:32:38.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4898" for this suite.
May 16 12:32:44.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:32:44.665: INFO: namespace downward-api-4898 deletion completed in 6.338009598s

• [SLOW TEST:14.744 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:32:44.667: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0516 12:32:54.856665      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 16 12:32:54.856: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:32:54.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2932" for this suite.
May 16 12:33:00.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:33:01.251: INFO: namespace gc-2932 deletion completed in 6.388008412s

• [SLOW TEST:16.585 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:33:01.255: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 12:33:01.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 version --client'
May 16 12:33:01.414: INFO: stderr: ""
May 16 12:33:01.414: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:11:31Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
May 16 12:33:01.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-6325'
May 16 12:33:03.429: INFO: stderr: ""
May 16 12:33:03.429: INFO: stdout: "replicationcontroller/redis-master created\n"
May 16 12:33:03.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-6325'
May 16 12:33:03.844: INFO: stderr: ""
May 16 12:33:03.844: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 16 12:33:04.851: INFO: Selector matched 1 pods for map[app:redis]
May 16 12:33:04.851: INFO: Found 0 / 1
May 16 12:33:05.852: INFO: Selector matched 1 pods for map[app:redis]
May 16 12:33:05.852: INFO: Found 0 / 1
May 16 12:33:06.852: INFO: Selector matched 1 pods for map[app:redis]
May 16 12:33:06.852: INFO: Found 1 / 1
May 16 12:33:06.852: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 16 12:33:06.859: INFO: Selector matched 1 pods for map[app:redis]
May 16 12:33:06.859: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 16 12:33:06.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 describe pod redis-master-prbln --namespace=kubectl-6325'
May 16 12:33:06.970: INFO: stderr: ""
May 16 12:33:06.970: INFO: stdout: "Name:               redis-master-prbln\nNamespace:          kubectl-6325\nPriority:           0\nPriorityClassName:  <none>\nNode:               machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u/192.168.1.12\nStart Time:         Thu, 16 May 2019 12:33:03 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 172.25.0.87\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://03e44f2c2241822247e5eb771e1eb5de9fce544b89d458d03832064de1f62d43\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 16 May 2019 12:33:05 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-xjldz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-xjldz:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-xjldz\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                             Message\n  ----    ------     ----  ----                                                             -------\n  Normal  Scheduled  3s    default-scheduler                                                Successfully assigned kubectl-6325/redis-master-prbln to machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u\n  Normal  Pulled     1s    kubelet, machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Created container redis-master\n  Normal  Started    1s    kubelet, machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Started container redis-master\n"
May 16 12:33:06.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 describe rc redis-master --namespace=kubectl-6325'
May 16 12:33:07.116: INFO: stderr: ""
May 16 12:33:07.116: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-6325\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-prbln\n"
May 16 12:33:07.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 describe service redis-master --namespace=kubectl-6325'
May 16 12:33:07.240: INFO: stderr: ""
May 16 12:33:07.240: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-6325\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.10.10.124\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.25.0.87:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 16 12:33:07.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 describe node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh'
May 16 12:33:07.441: INFO: stderr: ""
May 16 12:33:07.441: INFO: stdout: "Name:               machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=216cc975-806e-44dd-93a1-c0b27513a975\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=dbl\n                    failure-domain.beta.kubernetes.io/zone=dbl1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=47694d4b-77cf-11e9-9fa2-0a580af42f88\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"7e:8b:9f:f8:55:cc\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.1.10\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 16 May 2019 11:43:57 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 16 May 2019 12:32:27 +0000   Thu, 16 May 2019 11:43:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 16 May 2019 12:32:27 +0000   Thu, 16 May 2019 11:43:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 16 May 2019 12:32:27 +0000   Thu, 16 May 2019 11:43:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 16 May 2019 12:32:27 +0000   Thu, 16 May 2019 11:44:47 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.1.10\n  ExternalIP:  195.192.130.195\n  Hostname:    machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh\nCapacity:\n attachable-volumes-cinder:  256\n cpu:                        2\n ephemeral-storage:          50633164Ki\n hugepages-2Mi:              0\n memory:                     8168228Ki\n pods:                       110\nAllocatable:\n attachable-volumes-cinder:  256\n cpu:                        1800m\n ephemeral-storage:          44516040218\n hugepages-2Mi:              0\n memory:                     7861028Ki\n pods:                       110\nSystem Info:\n Machine ID:                 ab5058ebcb19401689618faec6f2d059\n System UUID:                AB5058EB-CB19-4016-8961-8FAEC6F2D059\n Boot ID:                    b445ea3b-774a-4c64-9475-10d2d16bbb7b\n Kernel Version:             4.15.0-46-generic\n OS Image:                   Ubuntu 18.04.2 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.2\n Kubelet Version:            v1.14.1\n Kube-Proxy Version:         v1.14.1\nPodCIDR:                     172.25.1.0/24\nProviderID:                  openstack:///ab5058eb-cb19-4016-8961-8faec6f2d059\nNon-terminated Pods:         (13 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-8c9cd2b50a37440b-rvwgr    0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  kube-system                canal-fvnrd                                                350m (19%)    100m (5%)   50Mi (0%)        50Mi (0%)      49m\n  kube-system                cluster-autoscaler-c8c56b5d9-2clck                         10m (0%)      100m (5%)   40Mi (0%)        300Mi (3%)     53m\n  kube-system                coredns-6bd858f7c-mrns8                                    100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     53m\n  kube-system                coredns-6bd858f7c-qtwtk                                    100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     53m\n  kube-system                kube-proxy-snhz7                                           75m (4%)      250m (13%)  50Mi (0%)        250Mi (3%)     49m\n  kube-system                kubernetes-dashboard-57b5ff8798-8ts2c                      50m (2%)      100m (5%)   100Mi (1%)       300Mi (3%)     53m\n  kube-system                node-exporter-7mblb                                        3m (0%)       200m (11%)  16Mi (0%)        50Mi (0%)      49m\n  kube-system                openvpn-client-5bbcf59684-hk8ll                            30m (1%)      200m (11%)  30Mi (0%)        82Mi (1%)      53m\n  kube-system                tiller-deploy-796c9d7db6-hwwqm                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\n  velero                     restic-mx74p                                               5m (0%)       100m (5%)   32Mi (0%)        300Mi (3%)     49m\n  velero                     velero-5bcdd86b7c-2qd27                                    10m (0%)      100m (5%)   64Mi (0%)        300Mi (3%)     53m\n  webterminal                webterminal-85f9f78784-srg8n                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests    Limits\n  --------                   --------    ------\n  cpu                        733m (40%)  1150m (63%)\n  memory                     522Mi (6%)  1972Mi (25%)\n  ephemeral-storage          0 (0%)      0 (0%)\n  attachable-volumes-cinder  0           0\nEvents:\n  Type    Reason                   Age                From                                                                Message\n  ----    ------                   ----               ----                                                                -------\n  Normal  NodeHasSufficientMemory  49m (x6 over 49m)  kubelet, machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh     Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    49m (x6 over 49m)  kubelet, machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh     Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     49m (x6 over 49m)  kubelet, machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh     Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh status is now: NodeHasSufficientPID\n  Normal  Starting                 48m                kube-proxy, machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh  Starting kube-proxy.\n  Normal  NodeReady                48m                kubelet, machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh     Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh status is now: NodeReady\n"
May 16 12:33:07.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 describe namespace kubectl-6325'
May 16 12:33:07.581: INFO: stderr: ""
May 16 12:33:07.581: INFO: stdout: "Name:         kubectl-6325\nLabels:       e2e-framework=kubectl\n              e2e-run=15e3616e-77d1-11e9-8275-42209be91bd2\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:33:07.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6325" for this suite.
May 16 12:33:31.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:33:31.963: INFO: namespace kubectl-6325 deletion completed in 24.376267993s

• [SLOW TEST:30.709 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:33:31.965: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 16 12:33:32.183: INFO: Waiting up to 5m0s for pod "downward-api-d0d26178-77d6-11e9-8275-42209be91bd2" in namespace "downward-api-3020" to be "success or failure"
May 16 12:33:32.190: INFO: Pod "downward-api-d0d26178-77d6-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.307968ms
May 16 12:33:34.197: INFO: Pod "downward-api-d0d26178-77d6-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013382697s
May 16 12:33:36.205: INFO: Pod "downward-api-d0d26178-77d6-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022183125s
May 16 12:33:38.236: INFO: Pod "downward-api-d0d26178-77d6-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052906982s
STEP: Saw pod success
May 16 12:33:38.236: INFO: Pod "downward-api-d0d26178-77d6-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:33:38.242: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh pod downward-api-d0d26178-77d6-11e9-8275-42209be91bd2 container dapi-container: <nil>
STEP: delete the pod
May 16 12:33:38.395: INFO: Waiting for pod downward-api-d0d26178-77d6-11e9-8275-42209be91bd2 to disappear
May 16 12:33:38.410: INFO: Pod downward-api-d0d26178-77d6-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:33:38.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3020" for this suite.
May 16 12:33:44.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:33:44.793: INFO: namespace downward-api-3020 deletion completed in 6.377590467s

• [SLOW TEST:12.828 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:33:44.796: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-hv4f
STEP: Creating a pod to test atomic-volume-subpath
May 16 12:33:45.015: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-hv4f" in namespace "subpath-8250" to be "success or failure"
May 16 12:33:45.021: INFO: Pod "pod-subpath-test-downwardapi-hv4f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.878598ms
May 16 12:33:47.062: INFO: Pod "pod-subpath-test-downwardapi-hv4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046984551s
May 16 12:33:49.072: INFO: Pod "pod-subpath-test-downwardapi-hv4f": Phase="Running", Reason="", readiness=true. Elapsed: 4.056804032s
May 16 12:33:51.079: INFO: Pod "pod-subpath-test-downwardapi-hv4f": Phase="Running", Reason="", readiness=true. Elapsed: 6.064273242s
May 16 12:33:53.087: INFO: Pod "pod-subpath-test-downwardapi-hv4f": Phase="Running", Reason="", readiness=true. Elapsed: 8.071964383s
May 16 12:33:55.095: INFO: Pod "pod-subpath-test-downwardapi-hv4f": Phase="Running", Reason="", readiness=true. Elapsed: 10.079518502s
May 16 12:33:57.101: INFO: Pod "pod-subpath-test-downwardapi-hv4f": Phase="Running", Reason="", readiness=true. Elapsed: 12.0860076s
May 16 12:33:59.110: INFO: Pod "pod-subpath-test-downwardapi-hv4f": Phase="Running", Reason="", readiness=true. Elapsed: 14.094581532s
May 16 12:34:01.149: INFO: Pod "pod-subpath-test-downwardapi-hv4f": Phase="Running", Reason="", readiness=true. Elapsed: 16.133806143s
May 16 12:34:03.159: INFO: Pod "pod-subpath-test-downwardapi-hv4f": Phase="Running", Reason="", readiness=true. Elapsed: 18.144201771s
May 16 12:34:05.171: INFO: Pod "pod-subpath-test-downwardapi-hv4f": Phase="Running", Reason="", readiness=true. Elapsed: 20.155487952s
May 16 12:34:07.179: INFO: Pod "pod-subpath-test-downwardapi-hv4f": Phase="Running", Reason="", readiness=true. Elapsed: 22.163488002s
May 16 12:34:09.201: INFO: Pod "pod-subpath-test-downwardapi-hv4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.185676896s
STEP: Saw pod success
May 16 12:34:09.201: INFO: Pod "pod-subpath-test-downwardapi-hv4f" satisfied condition "success or failure"
May 16 12:34:09.212: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-subpath-test-downwardapi-hv4f container test-container-subpath-downwardapi-hv4f: <nil>
STEP: delete the pod
May 16 12:34:09.276: INFO: Waiting for pod pod-subpath-test-downwardapi-hv4f to disappear
May 16 12:34:09.282: INFO: Pod pod-subpath-test-downwardapi-hv4f no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-hv4f
May 16 12:34:09.288: INFO: Deleting pod "pod-subpath-test-downwardapi-hv4f" in namespace "subpath-8250"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:34:09.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8250" for this suite.
May 16 12:34:15.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:34:15.652: INFO: namespace subpath-8250 deletion completed in 6.346449491s

• [SLOW TEST:30.857 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:34:15.655: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 16 12:34:15.821: INFO: Number of nodes with available pods: 0
May 16 12:34:15.821: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:16.839: INFO: Number of nodes with available pods: 0
May 16 12:34:16.839: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:17.836: INFO: Number of nodes with available pods: 0
May 16 12:34:17.836: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:18.841: INFO: Number of nodes with available pods: 1
May 16 12:34:18.841: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:19.837: INFO: Number of nodes with available pods: 2
May 16 12:34:19.837: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 16 12:34:19.892: INFO: Number of nodes with available pods: 1
May 16 12:34:19.892: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:20.914: INFO: Number of nodes with available pods: 1
May 16 12:34:20.914: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:21.909: INFO: Number of nodes with available pods: 1
May 16 12:34:21.909: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:22.912: INFO: Number of nodes with available pods: 1
May 16 12:34:22.912: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:23.908: INFO: Number of nodes with available pods: 1
May 16 12:34:23.908: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:24.904: INFO: Number of nodes with available pods: 1
May 16 12:34:24.904: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:25.928: INFO: Number of nodes with available pods: 1
May 16 12:34:25.928: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:27.012: INFO: Number of nodes with available pods: 1
May 16 12:34:27.013: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:27.908: INFO: Number of nodes with available pods: 1
May 16 12:34:27.908: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:28.905: INFO: Number of nodes with available pods: 1
May 16 12:34:28.905: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:29.909: INFO: Number of nodes with available pods: 1
May 16 12:34:29.909: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:34:30.904: INFO: Number of nodes with available pods: 2
May 16 12:34:30.904: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7084, will wait for the garbage collector to delete the pods
May 16 12:34:30.979: INFO: Deleting DaemonSet.extensions daemon-set took: 15.369474ms
May 16 12:34:31.479: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.218118ms
May 16 12:34:46.787: INFO: Number of nodes with available pods: 0
May 16 12:34:46.787: INFO: Number of running nodes: 0, number of available pods: 0
May 16 12:34:46.794: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7084/daemonsets","resourceVersion":"14163"},"items":null}

May 16 12:34:46.800: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7084/pods","resourceVersion":"14163"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:34:46.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7084" for this suite.
May 16 12:34:54.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:34:55.195: INFO: namespace daemonsets-7084 deletion completed in 8.347917527s

• [SLOW TEST:39.540 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:34:55.196: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 16 12:35:05.424: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9232 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:35:05.424: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:35:05.999: INFO: Exec stderr: ""
May 16 12:35:05.999: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9232 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:35:05.999: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:35:06.678: INFO: Exec stderr: ""
May 16 12:35:06.678: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9232 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:35:06.679: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:35:07.294: INFO: Exec stderr: ""
May 16 12:35:07.294: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9232 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:35:07.294: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:35:08.055: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 16 12:35:08.055: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9232 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:35:08.055: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:35:08.673: INFO: Exec stderr: ""
May 16 12:35:08.673: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9232 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:35:08.673: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:35:09.260: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 16 12:35:09.260: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9232 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:35:09.260: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:35:09.837: INFO: Exec stderr: ""
May 16 12:35:09.837: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9232 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:35:09.837: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:35:10.505: INFO: Exec stderr: ""
May 16 12:35:10.505: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9232 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:35:10.505: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:35:11.245: INFO: Exec stderr: ""
May 16 12:35:11.245: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9232 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:35:11.245: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:35:11.782: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:35:11.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9232" for this suite.
May 16 12:35:51.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:35:52.192: INFO: namespace e2e-kubelet-etc-hosts-9232 deletion completed in 40.399768922s

• [SLOW TEST:56.996 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:35:52.193: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-2456cba2-77d7-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 12:35:52.335: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-245802f9-77d7-11e9-8275-42209be91bd2" in namespace "projected-8423" to be "success or failure"
May 16 12:35:52.342: INFO: Pod "pod-projected-secrets-245802f9-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.98566ms
May 16 12:35:54.353: INFO: Pod "pod-projected-secrets-245802f9-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017635686s
May 16 12:35:56.360: INFO: Pod "pod-projected-secrets-245802f9-77d7-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024530055s
STEP: Saw pod success
May 16 12:35:56.360: INFO: Pod "pod-projected-secrets-245802f9-77d7-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:35:56.365: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-projected-secrets-245802f9-77d7-11e9-8275-42209be91bd2 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 16 12:35:56.411: INFO: Waiting for pod pod-projected-secrets-245802f9-77d7-11e9-8275-42209be91bd2 to disappear
May 16 12:35:56.416: INFO: Pod pod-projected-secrets-245802f9-77d7-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:35:56.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8423" for this suite.
May 16 12:36:02.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:36:02.763: INFO: namespace projected-8423 deletion completed in 6.339361286s

• [SLOW TEST:10.570 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:36:02.765: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
May 16 12:36:02.945: INFO: Waiting up to 5m0s for pod "client-containers-2aa67d4c-77d7-11e9-8275-42209be91bd2" in namespace "containers-5757" to be "success or failure"
May 16 12:36:02.954: INFO: Pod "client-containers-2aa67d4c-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.850696ms
May 16 12:36:04.962: INFO: Pod "client-containers-2aa67d4c-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016687801s
May 16 12:36:06.970: INFO: Pod "client-containers-2aa67d4c-77d7-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025055566s
STEP: Saw pod success
May 16 12:36:06.970: INFO: Pod "client-containers-2aa67d4c-77d7-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:36:06.976: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod client-containers-2aa67d4c-77d7-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 12:36:07.033: INFO: Waiting for pod client-containers-2aa67d4c-77d7-11e9-8275-42209be91bd2 to disappear
May 16 12:36:07.037: INFO: Pod client-containers-2aa67d4c-77d7-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:36:07.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5757" for this suite.
May 16 12:36:13.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:36:13.510: INFO: namespace containers-5757 deletion completed in 6.463726799s

• [SLOW TEST:10.745 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:36:13.511: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 16 12:36:13.653: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:36:26.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3674" for this suite.
May 16 12:36:32.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:36:32.905: INFO: namespace pods-3674 deletion completed in 6.346388796s

• [SLOW TEST:19.394 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:36:32.905: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 12:36:33.060: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ca215aa-77d7-11e9-8275-42209be91bd2" in namespace "projected-3853" to be "success or failure"
May 16 12:36:33.065: INFO: Pod "downwardapi-volume-3ca215aa-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.66383ms
May 16 12:36:35.076: INFO: Pod "downwardapi-volume-3ca215aa-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016488254s
May 16 12:36:37.083: INFO: Pod "downwardapi-volume-3ca215aa-77d7-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023344255s
STEP: Saw pod success
May 16 12:36:37.083: INFO: Pod "downwardapi-volume-3ca215aa-77d7-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:36:37.090: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-3ca215aa-77d7-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 12:36:37.235: INFO: Waiting for pod downwardapi-volume-3ca215aa-77d7-11e9-8275-42209be91bd2 to disappear
May 16 12:36:37.242: INFO: Pod downwardapi-volume-3ca215aa-77d7-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:36:37.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3853" for this suite.
May 16 12:36:43.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:36:43.611: INFO: namespace projected-3853 deletion completed in 6.358883624s

• [SLOW TEST:10.706 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:36:43.612: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 16 12:36:43.770: INFO: Waiting up to 5m0s for pod "downward-api-43015ee7-77d7-11e9-8275-42209be91bd2" in namespace "downward-api-5025" to be "success or failure"
May 16 12:36:43.776: INFO: Pod "downward-api-43015ee7-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.773913ms
May 16 12:36:45.784: INFO: Pod "downward-api-43015ee7-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014032633s
May 16 12:36:47.792: INFO: Pod "downward-api-43015ee7-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022125078s
May 16 12:36:49.801: INFO: Pod "downward-api-43015ee7-77d7-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031638645s
STEP: Saw pod success
May 16 12:36:49.801: INFO: Pod "downward-api-43015ee7-77d7-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:36:49.807: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh pod downward-api-43015ee7-77d7-11e9-8275-42209be91bd2 container dapi-container: <nil>
STEP: delete the pod
May 16 12:36:49.912: INFO: Waiting for pod downward-api-43015ee7-77d7-11e9-8275-42209be91bd2 to disappear
May 16 12:36:49.916: INFO: Pod downward-api-43015ee7-77d7-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:36:49.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5025" for this suite.
May 16 12:36:55.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:36:56.284: INFO: namespace downward-api-5025 deletion completed in 6.361506173s

• [SLOW TEST:12.672 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:36:56.288: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 12:36:56.433: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a8f16a7-77d7-11e9-8275-42209be91bd2" in namespace "downward-api-3267" to be "success or failure"
May 16 12:36:56.439: INFO: Pod "downwardapi-volume-4a8f16a7-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.373615ms
May 16 12:36:58.452: INFO: Pod "downwardapi-volume-4a8f16a7-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018566292s
May 16 12:37:00.459: INFO: Pod "downwardapi-volume-4a8f16a7-77d7-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02579974s
STEP: Saw pod success
May 16 12:37:00.459: INFO: Pod "downwardapi-volume-4a8f16a7-77d7-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:37:00.465: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-4a8f16a7-77d7-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 12:37:00.511: INFO: Waiting for pod downwardapi-volume-4a8f16a7-77d7-11e9-8275-42209be91bd2 to disappear
May 16 12:37:00.525: INFO: Pod downwardapi-volume-4a8f16a7-77d7-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:37:00.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3267" for this suite.
May 16 12:37:06.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:37:06.918: INFO: namespace downward-api-3267 deletion completed in 6.380765882s

• [SLOW TEST:10.631 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:37:06.922: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1328.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1328.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1328.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1328.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1328.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1328.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 16 12:37:13.767: INFO: DNS probes using dns-1328/dns-test-50e507f4-77d7-11e9-8275-42209be91bd2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:37:13.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1328" for this suite.
May 16 12:37:21.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:37:22.193: INFO: namespace dns-1328 deletion completed in 8.364266544s

• [SLOW TEST:15.272 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:37:22.195: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-5a067bbc-77d7-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 12:37:22.393: INFO: Waiting up to 5m0s for pod "pod-secrets-5a092842-77d7-11e9-8275-42209be91bd2" in namespace "secrets-2331" to be "success or failure"
May 16 12:37:22.404: INFO: Pod "pod-secrets-5a092842-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.370913ms
May 16 12:37:24.414: INFO: Pod "pod-secrets-5a092842-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020347633s
May 16 12:37:26.429: INFO: Pod "pod-secrets-5a092842-77d7-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035120672s
STEP: Saw pod success
May 16 12:37:26.429: INFO: Pod "pod-secrets-5a092842-77d7-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:37:26.435: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-secrets-5a092842-77d7-11e9-8275-42209be91bd2 container secret-env-test: <nil>
STEP: delete the pod
May 16 12:37:26.524: INFO: Waiting for pod pod-secrets-5a092842-77d7-11e9-8275-42209be91bd2 to disappear
May 16 12:37:26.529: INFO: Pod pod-secrets-5a092842-77d7-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:37:26.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2331" for this suite.
May 16 12:37:32.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:37:32.905: INFO: namespace secrets-2331 deletion completed in 6.36762499s

• [SLOW TEST:10.710 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:37:32.908: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 16 12:37:43.180: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 16 12:37:43.187: INFO: Pod pod-with-poststart-exec-hook still exists
May 16 12:37:45.187: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 16 12:37:45.194: INFO: Pod pod-with-poststart-exec-hook still exists
May 16 12:37:47.187: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 16 12:37:47.194: INFO: Pod pod-with-poststart-exec-hook still exists
May 16 12:37:49.187: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 16 12:37:49.194: INFO: Pod pod-with-poststart-exec-hook still exists
May 16 12:37:51.187: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 16 12:37:51.196: INFO: Pod pod-with-poststart-exec-hook still exists
May 16 12:37:53.187: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 16 12:37:53.195: INFO: Pod pod-with-poststart-exec-hook still exists
May 16 12:37:55.187: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 16 12:37:55.198: INFO: Pod pod-with-poststart-exec-hook still exists
May 16 12:37:57.187: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 16 12:37:57.195: INFO: Pod pod-with-poststart-exec-hook still exists
May 16 12:37:59.187: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 16 12:37:59.198: INFO: Pod pod-with-poststart-exec-hook still exists
May 16 12:38:01.189: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 16 12:38:01.195: INFO: Pod pod-with-poststart-exec-hook still exists
May 16 12:38:03.187: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 16 12:38:03.194: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:38:03.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9293" for this suite.
May 16 12:38:27.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:38:27.726: INFO: namespace container-lifecycle-hook-9293 deletion completed in 24.521966336s

• [SLOW TEST:54.818 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:38:27.726: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 12:38:27.897: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8114f3fc-77d7-11e9-8275-42209be91bd2" in namespace "projected-4488" to be "success or failure"
May 16 12:38:27.903: INFO: Pod "downwardapi-volume-8114f3fc-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.605833ms
May 16 12:38:29.910: INFO: Pod "downwardapi-volume-8114f3fc-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012135775s
May 16 12:38:31.917: INFO: Pod "downwardapi-volume-8114f3fc-77d7-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01899114s
STEP: Saw pod success
May 16 12:38:31.917: INFO: Pod "downwardapi-volume-8114f3fc-77d7-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:38:31.922: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-8114f3fc-77d7-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 12:38:31.990: INFO: Waiting for pod downwardapi-volume-8114f3fc-77d7-11e9-8275-42209be91bd2 to disappear
May 16 12:38:32.002: INFO: Pod downwardapi-volume-8114f3fc-77d7-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:38:32.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4488" for this suite.
May 16 12:38:38.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:38:38.390: INFO: namespace projected-4488 deletion completed in 6.380152802s

• [SLOW TEST:10.663 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:38:38.391: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 16 12:38:42.664: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-87730108-77d7-11e9-8275-42209be91bd2,GenerateName:,Namespace:events-24,SelfLink:/api/v1/namespaces/events-24/pods/send-events-87730108-77d7-11e9-8275-42209be91bd2,UID:876e3539-77d7-11e9-b44f-0a580af42602,ResourceVersion:15260,Generation:0,CreationTimestamp:2019-05-16 12:38:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 564588934,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5pkjp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5pkjp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-5pkjp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001968550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001968570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:38:38 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:38:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:38:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:38:38 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.0.101,StartTime:2019-05-16 12:38:38 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-16 12:38:40 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://5badf1c61badb2a6a933eadccef027057d379537d33ec3a14de66b799834cc3e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
May 16 12:38:44.679: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 16 12:38:46.686: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:38:46.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-24" for this suite.
May 16 12:39:28.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:39:29.167: INFO: namespace events-24 deletion completed in 42.436465494s

• [SLOW TEST:50.776 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:39:29.169: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 12:39:29.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5af3130-77d7-11e9-8275-42209be91bd2" in namespace "downward-api-4158" to be "success or failure"
May 16 12:39:29.336: INFO: Pod "downwardapi-volume-a5af3130-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.350036ms
May 16 12:39:31.346: INFO: Pod "downwardapi-volume-a5af3130-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015697958s
May 16 12:39:33.352: INFO: Pod "downwardapi-volume-a5af3130-77d7-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022013124s
STEP: Saw pod success
May 16 12:39:33.352: INFO: Pod "downwardapi-volume-a5af3130-77d7-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:39:33.357: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-a5af3130-77d7-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 12:39:33.400: INFO: Waiting for pod downwardapi-volume-a5af3130-77d7-11e9-8275-42209be91bd2 to disappear
May 16 12:39:33.413: INFO: Pod downwardapi-volume-a5af3130-77d7-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:39:33.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4158" for this suite.
May 16 12:39:39.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:39:39.827: INFO: namespace downward-api-4158 deletion completed in 6.407933406s

• [SLOW TEST:10.657 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:39:39.829: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 16 12:39:44.561: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ac06bde8-77d7-11e9-8275-42209be91bd2"
May 16 12:39:44.561: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ac06bde8-77d7-11e9-8275-42209be91bd2" in namespace "pods-4396" to be "terminated due to deadline exceeded"
May 16 12:39:44.618: INFO: Pod "pod-update-activedeadlineseconds-ac06bde8-77d7-11e9-8275-42209be91bd2": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 57.566416ms
May 16 12:39:44.618: INFO: Pod "pod-update-activedeadlineseconds-ac06bde8-77d7-11e9-8275-42209be91bd2" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:39:44.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4396" for this suite.
May 16 12:39:50.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:39:50.969: INFO: namespace pods-4396 deletion completed in 6.342539153s

• [SLOW TEST:11.140 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:39:50.969: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-b2abcdf5-77d7-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 12:39:51.124: INFO: Waiting up to 5m0s for pod "pod-configmaps-b2ae0a43-77d7-11e9-8275-42209be91bd2" in namespace "configmap-3541" to be "success or failure"
May 16 12:39:51.145: INFO: Pod "pod-configmaps-b2ae0a43-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.97883ms
May 16 12:39:53.153: INFO: Pod "pod-configmaps-b2ae0a43-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028582992s
May 16 12:39:55.166: INFO: Pod "pod-configmaps-b2ae0a43-77d7-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041371502s
STEP: Saw pod success
May 16 12:39:55.166: INFO: Pod "pod-configmaps-b2ae0a43-77d7-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:39:55.174: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-configmaps-b2ae0a43-77d7-11e9-8275-42209be91bd2 container configmap-volume-test: <nil>
STEP: delete the pod
May 16 12:39:55.395: INFO: Waiting for pod pod-configmaps-b2ae0a43-77d7-11e9-8275-42209be91bd2 to disappear
May 16 12:39:55.405: INFO: Pod pod-configmaps-b2ae0a43-77d7-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:39:55.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3541" for this suite.
May 16 12:40:01.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:40:01.817: INFO: namespace configmap-3541 deletion completed in 6.406560644s

• [SLOW TEST:10.848 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:40:01.818: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 12:40:02.017: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 16 12:40:02.121: INFO: Number of nodes with available pods: 0
May 16 12:40:02.121: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 16 12:40:02.428: INFO: Number of nodes with available pods: 0
May 16 12:40:02.428: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:03.455: INFO: Number of nodes with available pods: 0
May 16 12:40:03.456: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:04.435: INFO: Number of nodes with available pods: 0
May 16 12:40:04.435: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:05.444: INFO: Number of nodes with available pods: 1
May 16 12:40:05.444: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 16 12:40:05.536: INFO: Number of nodes with available pods: 0
May 16 12:40:05.536: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 16 12:40:05.579: INFO: Number of nodes with available pods: 0
May 16 12:40:05.580: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:06.587: INFO: Number of nodes with available pods: 0
May 16 12:40:06.587: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:07.587: INFO: Number of nodes with available pods: 0
May 16 12:40:07.587: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:08.588: INFO: Number of nodes with available pods: 0
May 16 12:40:08.589: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:09.587: INFO: Number of nodes with available pods: 0
May 16 12:40:09.587: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:10.586: INFO: Number of nodes with available pods: 0
May 16 12:40:10.586: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:11.587: INFO: Number of nodes with available pods: 0
May 16 12:40:11.587: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:12.593: INFO: Number of nodes with available pods: 0
May 16 12:40:12.593: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:13.593: INFO: Number of nodes with available pods: 0
May 16 12:40:13.593: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:14.586: INFO: Number of nodes with available pods: 0
May 16 12:40:14.586: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:15.598: INFO: Number of nodes with available pods: 0
May 16 12:40:15.599: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:16.587: INFO: Number of nodes with available pods: 0
May 16 12:40:16.587: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:17.597: INFO: Number of nodes with available pods: 0
May 16 12:40:17.597: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:18.587: INFO: Number of nodes with available pods: 0
May 16 12:40:18.587: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:19.586: INFO: Number of nodes with available pods: 0
May 16 12:40:19.586: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:40:20.587: INFO: Number of nodes with available pods: 1
May 16 12:40:20.587: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4185, will wait for the garbage collector to delete the pods
May 16 12:40:20.672: INFO: Deleting DaemonSet.extensions daemon-set took: 17.657692ms
May 16 12:40:21.173: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.343357ms
May 16 12:40:26.879: INFO: Number of nodes with available pods: 0
May 16 12:40:26.879: INFO: Number of running nodes: 0, number of available pods: 0
May 16 12:40:26.884: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4185/daemonsets","resourceVersion":"15725"},"items":null}

May 16 12:40:26.893: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4185/pods","resourceVersion":"15725"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:40:26.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4185" for this suite.
May 16 12:40:34.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:40:35.242: INFO: namespace daemonsets-4185 deletion completed in 8.290914263s

• [SLOW TEST:33.425 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:40:35.243: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-cd1111e6-77d7-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 12:40:35.503: INFO: Waiting up to 5m0s for pod "pod-secrets-cd241eb9-77d7-11e9-8275-42209be91bd2" in namespace "secrets-2926" to be "success or failure"
May 16 12:40:35.508: INFO: Pod "pod-secrets-cd241eb9-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.623165ms
May 16 12:40:37.535: INFO: Pod "pod-secrets-cd241eb9-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031713235s
May 16 12:40:39.542: INFO: Pod "pod-secrets-cd241eb9-77d7-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03884268s
STEP: Saw pod success
May 16 12:40:39.542: INFO: Pod "pod-secrets-cd241eb9-77d7-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:40:39.548: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-secrets-cd241eb9-77d7-11e9-8275-42209be91bd2 container secret-volume-test: <nil>
STEP: delete the pod
May 16 12:40:39.609: INFO: Waiting for pod pod-secrets-cd241eb9-77d7-11e9-8275-42209be91bd2 to disappear
May 16 12:40:39.627: INFO: Pod pod-secrets-cd241eb9-77d7-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:40:39.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2926" for this suite.
May 16 12:40:45.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:40:46.085: INFO: namespace secrets-2926 deletion completed in 6.4512911s
STEP: Destroying namespace "secret-namespace-7705" for this suite.
May 16 12:40:52.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:40:52.463: INFO: namespace secret-namespace-7705 deletion completed in 6.37739015s

• [SLOW TEST:17.220 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:40:52.464: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4360
I0516 12:40:52.580861      16 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4360, replica count: 1
I0516 12:40:53.631274      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0516 12:40:54.632830      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0516 12:40:55.633045      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 16 12:40:55.757: INFO: Created: latency-svc-ltj5h
May 16 12:40:55.782: INFO: Got endpoints: latency-svc-ltj5h [48.871295ms]
May 16 12:40:55.807: INFO: Created: latency-svc-vtl8q
May 16 12:40:55.816: INFO: Created: latency-svc-m95vd
May 16 12:40:55.831: INFO: Created: latency-svc-pc8m9
May 16 12:40:55.832: INFO: Got endpoints: latency-svc-m95vd [49.815648ms]
May 16 12:40:55.832: INFO: Got endpoints: latency-svc-vtl8q [50.231544ms]
May 16 12:40:55.850: INFO: Created: latency-svc-gzb9c
May 16 12:40:55.853: INFO: Got endpoints: latency-svc-pc8m9 [71.375802ms]
May 16 12:40:55.880: INFO: Created: latency-svc-t45g5
May 16 12:40:55.890: INFO: Got endpoints: latency-svc-gzb9c [107.970389ms]
May 16 12:40:55.895: INFO: Got endpoints: latency-svc-t45g5 [113.240705ms]
May 16 12:40:55.899: INFO: Created: latency-svc-vxh2n
May 16 12:40:55.911: INFO: Got endpoints: latency-svc-vxh2n [129.115893ms]
May 16 12:40:55.926: INFO: Created: latency-svc-5vb7j
May 16 12:40:55.936: INFO: Created: latency-svc-l7m7s
May 16 12:40:55.943: INFO: Got endpoints: latency-svc-5vb7j [160.573469ms]
May 16 12:40:55.959: INFO: Created: latency-svc-g76md
May 16 12:40:55.962: INFO: Got endpoints: latency-svc-l7m7s [179.774043ms]
May 16 12:40:55.983: INFO: Got endpoints: latency-svc-g76md [200.460379ms]
May 16 12:40:55.993: INFO: Created: latency-svc-n2gvd
May 16 12:40:55.994: INFO: Created: latency-svc-vzqqh
May 16 12:40:56.004: INFO: Got endpoints: latency-svc-n2gvd [222.220925ms]
May 16 12:40:56.008: INFO: Got endpoints: latency-svc-vzqqh [226.011983ms]
May 16 12:40:56.023: INFO: Created: latency-svc-lzbtq
May 16 12:40:56.053: INFO: Got endpoints: latency-svc-lzbtq [270.651297ms]
May 16 12:40:56.062: INFO: Created: latency-svc-6tckf
May 16 12:40:56.079: INFO: Created: latency-svc-slhl9
May 16 12:40:56.084: INFO: Got endpoints: latency-svc-6tckf [301.74629ms]
May 16 12:40:56.095: INFO: Got endpoints: latency-svc-slhl9 [313.09347ms]
May 16 12:40:56.104: INFO: Created: latency-svc-ld9lp
May 16 12:40:56.139: INFO: Created: latency-svc-26bbv
May 16 12:40:56.149: INFO: Got endpoints: latency-svc-ld9lp [366.826507ms]
May 16 12:40:56.156: INFO: Got endpoints: latency-svc-26bbv [324.506346ms]
May 16 12:40:56.164: INFO: Created: latency-svc-bklf9
May 16 12:40:56.191: INFO: Created: latency-svc-qwhlh
May 16 12:40:56.198: INFO: Got endpoints: latency-svc-bklf9 [365.897941ms]
May 16 12:40:56.206: INFO: Created: latency-svc-qvfjt
May 16 12:40:56.208: INFO: Got endpoints: latency-svc-qwhlh [354.531928ms]
May 16 12:40:56.220: INFO: Created: latency-svc-9wft2
May 16 12:40:56.222: INFO: Got endpoints: latency-svc-qvfjt [331.593554ms]
May 16 12:40:56.250: INFO: Created: latency-svc-sqsvk
May 16 12:40:56.251: INFO: Got endpoints: latency-svc-9wft2 [355.677302ms]
May 16 12:40:56.269: INFO: Created: latency-svc-x9wjl
May 16 12:40:56.280: INFO: Got endpoints: latency-svc-sqsvk [368.495514ms]
May 16 12:40:56.291: INFO: Got endpoints: latency-svc-x9wjl [348.124419ms]
May 16 12:40:56.296: INFO: Created: latency-svc-tgfc8
May 16 12:40:56.334: INFO: Created: latency-svc-4cvkk
May 16 12:40:56.335: INFO: Got endpoints: latency-svc-tgfc8 [372.937689ms]
May 16 12:40:56.351: INFO: Got endpoints: latency-svc-4cvkk [368.445697ms]
May 16 12:40:56.368: INFO: Created: latency-svc-jjf66
May 16 12:40:56.377: INFO: Got endpoints: latency-svc-jjf66 [372.356485ms]
May 16 12:40:56.388: INFO: Created: latency-svc-2mbj2
May 16 12:40:56.407: INFO: Got endpoints: latency-svc-2mbj2 [398.753394ms]
May 16 12:40:56.427: INFO: Created: latency-svc-pbnjr
May 16 12:40:56.443: INFO: Got endpoints: latency-svc-pbnjr [390.294779ms]
May 16 12:40:56.444: INFO: Created: latency-svc-ps5tv
May 16 12:40:56.455: INFO: Got endpoints: latency-svc-ps5tv [370.916335ms]
May 16 12:40:56.476: INFO: Created: latency-svc-tmtkd
May 16 12:40:56.499: INFO: Got endpoints: latency-svc-tmtkd [403.415463ms]
May 16 12:40:56.511: INFO: Created: latency-svc-dmp2t
May 16 12:40:56.536: INFO: Got endpoints: latency-svc-dmp2t [386.512393ms]
May 16 12:40:56.536: INFO: Created: latency-svc-7vxmz
May 16 12:40:56.544: INFO: Got endpoints: latency-svc-7vxmz [387.762999ms]
May 16 12:40:56.544: INFO: Created: latency-svc-fqzqb
May 16 12:40:56.556: INFO: Got endpoints: latency-svc-fqzqb [357.899343ms]
May 16 12:40:56.557: INFO: Created: latency-svc-4wb7b
May 16 12:40:56.571: INFO: Got endpoints: latency-svc-4wb7b [361.229849ms]
May 16 12:40:56.572: INFO: Created: latency-svc-db4dj
May 16 12:40:56.593: INFO: Got endpoints: latency-svc-db4dj [370.360914ms]
May 16 12:40:56.602: INFO: Created: latency-svc-vn8fc
May 16 12:40:56.623: INFO: Got endpoints: latency-svc-vn8fc [371.758534ms]
May 16 12:40:56.628: INFO: Created: latency-svc-bmsc7
May 16 12:40:56.636: INFO: Got endpoints: latency-svc-bmsc7 [356.475402ms]
May 16 12:40:56.643: INFO: Created: latency-svc-7zkz2
May 16 12:40:56.647: INFO: Got endpoints: latency-svc-7zkz2 [355.74713ms]
May 16 12:40:56.654: INFO: Created: latency-svc-dnvmn
May 16 12:40:56.671: INFO: Got endpoints: latency-svc-dnvmn [336.466518ms]
May 16 12:40:56.672: INFO: Created: latency-svc-xczst
May 16 12:40:56.676: INFO: Got endpoints: latency-svc-xczst [324.374263ms]
May 16 12:40:56.679: INFO: Created: latency-svc-gx7kw
May 16 12:40:56.689: INFO: Got endpoints: latency-svc-gx7kw [311.615681ms]
May 16 12:40:56.692: INFO: Created: latency-svc-b4db6
May 16 12:40:56.711: INFO: Got endpoints: latency-svc-b4db6 [303.055137ms]
May 16 12:40:56.733: INFO: Created: latency-svc-xf6bj
May 16 12:40:56.755: INFO: Got endpoints: latency-svc-xf6bj [311.639127ms]
May 16 12:40:56.756: INFO: Created: latency-svc-6sn5d
May 16 12:40:56.775: INFO: Got endpoints: latency-svc-6sn5d [320.138075ms]
May 16 12:40:56.776: INFO: Created: latency-svc-tbkgp
May 16 12:40:56.794: INFO: Got endpoints: latency-svc-tbkgp [294.528747ms]
May 16 12:40:56.797: INFO: Created: latency-svc-xg8hd
May 16 12:40:56.833: INFO: Got endpoints: latency-svc-xg8hd [296.89825ms]
May 16 12:40:56.846: INFO: Created: latency-svc-9n77b
May 16 12:40:56.855: INFO: Got endpoints: latency-svc-9n77b [311.327091ms]
May 16 12:40:56.864: INFO: Created: latency-svc-284tc
May 16 12:40:56.877: INFO: Got endpoints: latency-svc-284tc [321.089985ms]
May 16 12:40:56.878: INFO: Created: latency-svc-96sqb
May 16 12:40:56.895: INFO: Got endpoints: latency-svc-96sqb [324.07414ms]
May 16 12:40:56.917: INFO: Created: latency-svc-8jxls
May 16 12:40:56.932: INFO: Created: latency-svc-vh7cj
May 16 12:40:56.934: INFO: Got endpoints: latency-svc-8jxls [341.195379ms]
May 16 12:40:56.939: INFO: Created: latency-svc-6svdz
May 16 12:40:56.948: INFO: Got endpoints: latency-svc-vh7cj [324.854371ms]
May 16 12:40:56.968: INFO: Got endpoints: latency-svc-6svdz [331.242532ms]
May 16 12:40:56.971: INFO: Created: latency-svc-tqnpv
May 16 12:40:57.002: INFO: Got endpoints: latency-svc-tqnpv [354.43816ms]
May 16 12:40:57.005: INFO: Created: latency-svc-skjrj
May 16 12:40:57.033: INFO: Got endpoints: latency-svc-skjrj [361.056424ms]
May 16 12:40:57.073: INFO: Created: latency-svc-dnpsh
May 16 12:40:57.092: INFO: Got endpoints: latency-svc-dnpsh [416.71665ms]
May 16 12:40:57.096: INFO: Created: latency-svc-kftj7
May 16 12:40:57.116: INFO: Got endpoints: latency-svc-kftj7 [426.958977ms]
May 16 12:40:57.118: INFO: Created: latency-svc-r8dhq
May 16 12:40:57.141: INFO: Got endpoints: latency-svc-r8dhq [430.356079ms]
May 16 12:40:57.142: INFO: Created: latency-svc-dd2th
May 16 12:40:57.164: INFO: Created: latency-svc-kcdkx
May 16 12:40:57.175: INFO: Got endpoints: latency-svc-dd2th [420.217913ms]
May 16 12:40:57.178: INFO: Created: latency-svc-vh5dx
May 16 12:40:57.192: INFO: Created: latency-svc-wg6md
May 16 12:40:57.204: INFO: Created: latency-svc-4xqsx
May 16 12:40:57.225: INFO: Created: latency-svc-w5hss
May 16 12:40:57.231: INFO: Got endpoints: latency-svc-kcdkx [455.539626ms]
May 16 12:40:57.246: INFO: Created: latency-svc-c5d4j
May 16 12:40:57.272: INFO: Created: latency-svc-rcfpr
May 16 12:40:57.283: INFO: Got endpoints: latency-svc-vh5dx [489.134938ms]
May 16 12:40:57.291: INFO: Created: latency-svc-g2tsb
May 16 12:40:57.314: INFO: Created: latency-svc-jjlbq
May 16 12:40:57.325: INFO: Got endpoints: latency-svc-wg6md [492.772432ms]
May 16 12:40:57.329: INFO: Created: latency-svc-w92w6
May 16 12:40:57.341: INFO: Created: latency-svc-h2v2j
May 16 12:40:57.365: INFO: Created: latency-svc-wpgpg
May 16 12:40:57.406: INFO: Created: latency-svc-s2dc6
May 16 12:40:57.406: INFO: Got endpoints: latency-svc-4xqsx [550.243422ms]
May 16 12:40:57.424: INFO: Created: latency-svc-cbx4w
May 16 12:40:57.429: INFO: Got endpoints: latency-svc-w5hss [552.047794ms]
May 16 12:40:57.454: INFO: Created: latency-svc-2lgjr
May 16 12:40:57.486: INFO: Created: latency-svc-bnktb
May 16 12:40:57.490: INFO: Got endpoints: latency-svc-c5d4j [594.442965ms]
May 16 12:40:57.508: INFO: Created: latency-svc-cmjpg
May 16 12:40:57.525: INFO: Created: latency-svc-4tbv4
May 16 12:40:57.532: INFO: Got endpoints: latency-svc-rcfpr [597.761678ms]
May 16 12:40:57.537: INFO: Created: latency-svc-m5wx5
May 16 12:40:57.552: INFO: Created: latency-svc-bkd58
May 16 12:40:57.586: INFO: Got endpoints: latency-svc-g2tsb [637.959699ms]
May 16 12:40:57.590: INFO: Created: latency-svc-mkpk4
May 16 12:40:57.605: INFO: Created: latency-svc-cqlvj
May 16 12:40:57.654: INFO: Got endpoints: latency-svc-jjlbq [685.874288ms]
May 16 12:40:57.656: INFO: Created: latency-svc-vnvmw
May 16 12:40:57.683: INFO: Created: latency-svc-rm7mw
May 16 12:40:57.689: INFO: Got endpoints: latency-svc-w92w6 [687.296942ms]
May 16 12:40:57.719: INFO: Created: latency-svc-lqfbl
May 16 12:40:57.749: INFO: Got endpoints: latency-svc-h2v2j [715.952002ms]
May 16 12:40:57.804: INFO: Created: latency-svc-th85g
May 16 12:40:57.804: INFO: Got endpoints: latency-svc-wpgpg [711.34006ms]
May 16 12:40:57.840: INFO: Created: latency-svc-l9r9v
May 16 12:40:57.841: INFO: Got endpoints: latency-svc-s2dc6 [725.831673ms]
May 16 12:40:57.872: INFO: Created: latency-svc-wdfq5
May 16 12:40:57.874: INFO: Got endpoints: latency-svc-cbx4w [733.016486ms]
May 16 12:40:57.893: INFO: Created: latency-svc-gwfvc
May 16 12:40:57.928: INFO: Got endpoints: latency-svc-2lgjr [752.858964ms]
May 16 12:40:57.947: INFO: Created: latency-svc-z7qch
May 16 12:40:57.984: INFO: Got endpoints: latency-svc-bnktb [753.062185ms]
May 16 12:40:58.030: INFO: Created: latency-svc-9t7vq
May 16 12:40:58.036: INFO: Got endpoints: latency-svc-cmjpg [752.659237ms]
May 16 12:40:58.052: INFO: Created: latency-svc-shkp6
May 16 12:40:58.090: INFO: Got endpoints: latency-svc-4tbv4 [764.354593ms]
May 16 12:40:58.120: INFO: Created: latency-svc-94xqk
May 16 12:40:58.131: INFO: Got endpoints: latency-svc-m5wx5 [725.290719ms]
May 16 12:40:58.166: INFO: Created: latency-svc-5khfc
May 16 12:40:58.181: INFO: Got endpoints: latency-svc-bkd58 [751.954192ms]
May 16 12:40:58.201: INFO: Created: latency-svc-f7zdb
May 16 12:40:58.226: INFO: Got endpoints: latency-svc-mkpk4 [736.000647ms]
May 16 12:40:58.244: INFO: Created: latency-svc-jjqqn
May 16 12:40:58.280: INFO: Got endpoints: latency-svc-cqlvj [748.01018ms]
May 16 12:40:58.301: INFO: Created: latency-svc-ccsl2
May 16 12:40:58.359: INFO: Got endpoints: latency-svc-vnvmw [772.516759ms]
May 16 12:40:58.392: INFO: Got endpoints: latency-svc-rm7mw [738.069136ms]
May 16 12:40:58.394: INFO: Created: latency-svc-2c85j
May 16 12:40:58.410: INFO: Created: latency-svc-t75hx
May 16 12:40:58.425: INFO: Got endpoints: latency-svc-lqfbl [736.048686ms]
May 16 12:40:58.464: INFO: Created: latency-svc-kdqpp
May 16 12:40:58.473: INFO: Got endpoints: latency-svc-th85g [724.228692ms]
May 16 12:40:58.500: INFO: Created: latency-svc-fnjzw
May 16 12:40:58.524: INFO: Got endpoints: latency-svc-l9r9v [719.854781ms]
May 16 12:40:58.544: INFO: Created: latency-svc-kpth9
May 16 12:40:58.582: INFO: Got endpoints: latency-svc-wdfq5 [740.627679ms]
May 16 12:40:58.612: INFO: Created: latency-svc-vrjxf
May 16 12:40:58.626: INFO: Got endpoints: latency-svc-gwfvc [751.432223ms]
May 16 12:40:58.650: INFO: Created: latency-svc-hzjg7
May 16 12:40:58.674: INFO: Got endpoints: latency-svc-z7qch [745.142672ms]
May 16 12:40:58.707: INFO: Created: latency-svc-vhq86
May 16 12:40:58.733: INFO: Got endpoints: latency-svc-9t7vq [748.967165ms]
May 16 12:40:58.756: INFO: Created: latency-svc-slmgz
May 16 12:40:58.771: INFO: Got endpoints: latency-svc-shkp6 [735.36987ms]
May 16 12:40:58.785: INFO: Created: latency-svc-rk8cp
May 16 12:40:58.824: INFO: Got endpoints: latency-svc-94xqk [734.129191ms]
May 16 12:40:58.851: INFO: Created: latency-svc-rgzvw
May 16 12:40:58.891: INFO: Got endpoints: latency-svc-5khfc [759.499206ms]
May 16 12:40:58.907: INFO: Created: latency-svc-qq9n8
May 16 12:40:58.926: INFO: Got endpoints: latency-svc-f7zdb [744.702959ms]
May 16 12:40:58.969: INFO: Created: latency-svc-8mwh9
May 16 12:40:58.976: INFO: Got endpoints: latency-svc-jjqqn [750.413184ms]
May 16 12:40:59.029: INFO: Created: latency-svc-hz2w9
May 16 12:40:59.036: INFO: Got endpoints: latency-svc-ccsl2 [755.61718ms]
May 16 12:40:59.057: INFO: Created: latency-svc-x5fdm
May 16 12:40:59.092: INFO: Got endpoints: latency-svc-2c85j [732.260195ms]
May 16 12:40:59.119: INFO: Created: latency-svc-m2459
May 16 12:40:59.135: INFO: Got endpoints: latency-svc-t75hx [742.950616ms]
May 16 12:40:59.166: INFO: Created: latency-svc-8mvbz
May 16 12:40:59.180: INFO: Got endpoints: latency-svc-kdqpp [754.655699ms]
May 16 12:40:59.204: INFO: Created: latency-svc-svwd8
May 16 12:40:59.223: INFO: Got endpoints: latency-svc-fnjzw [750.266645ms]
May 16 12:40:59.262: INFO: Created: latency-svc-d2s4z
May 16 12:40:59.279: INFO: Got endpoints: latency-svc-kpth9 [754.651941ms]
May 16 12:40:59.296: INFO: Created: latency-svc-8shn4
May 16 12:40:59.346: INFO: Got endpoints: latency-svc-vrjxf [763.409572ms]
May 16 12:40:59.380: INFO: Created: latency-svc-v5klf
May 16 12:40:59.387: INFO: Got endpoints: latency-svc-hzjg7 [760.652212ms]
May 16 12:40:59.406: INFO: Created: latency-svc-bgcsr
May 16 12:40:59.427: INFO: Got endpoints: latency-svc-vhq86 [752.893552ms]
May 16 12:40:59.462: INFO: Created: latency-svc-c8rc5
May 16 12:40:59.475: INFO: Got endpoints: latency-svc-slmgz [742.010061ms]
May 16 12:40:59.492: INFO: Created: latency-svc-6m26c
May 16 12:40:59.526: INFO: Got endpoints: latency-svc-rk8cp [754.2665ms]
May 16 12:40:59.543: INFO: Created: latency-svc-5rqgr
May 16 12:40:59.573: INFO: Got endpoints: latency-svc-rgzvw [749.055071ms]
May 16 12:40:59.630: INFO: Created: latency-svc-b7474
May 16 12:40:59.635: INFO: Got endpoints: latency-svc-qq9n8 [744.458054ms]
May 16 12:40:59.664: INFO: Created: latency-svc-qh29g
May 16 12:40:59.686: INFO: Got endpoints: latency-svc-8mwh9 [758.913028ms]
May 16 12:40:59.719: INFO: Created: latency-svc-t4b54
May 16 12:40:59.731: INFO: Got endpoints: latency-svc-hz2w9 [754.792583ms]
May 16 12:40:59.759: INFO: Created: latency-svc-cxszx
May 16 12:40:59.777: INFO: Got endpoints: latency-svc-x5fdm [741.576095ms]
May 16 12:40:59.792: INFO: Created: latency-svc-8fftw
May 16 12:40:59.824: INFO: Got endpoints: latency-svc-m2459 [732.715081ms]
May 16 12:40:59.844: INFO: Created: latency-svc-xw8zt
May 16 12:40:59.879: INFO: Got endpoints: latency-svc-8mvbz [744.200297ms]
May 16 12:40:59.945: INFO: Created: latency-svc-xmb82
May 16 12:40:59.957: INFO: Got endpoints: latency-svc-svwd8 [776.481975ms]
May 16 12:40:59.976: INFO: Created: latency-svc-cg5zz
May 16 12:40:59.979: INFO: Got endpoints: latency-svc-d2s4z [754.927191ms]
May 16 12:41:00.008: INFO: Created: latency-svc-q7qb2
May 16 12:41:00.026: INFO: Got endpoints: latency-svc-8shn4 [747.38671ms]
May 16 12:41:00.103: INFO: Got endpoints: latency-svc-v5klf [756.133921ms]
May 16 12:41:00.103: INFO: Created: latency-svc-qvcdr
May 16 12:41:00.117: INFO: Created: latency-svc-ls65n
May 16 12:41:00.133: INFO: Got endpoints: latency-svc-bgcsr [745.877302ms]
May 16 12:41:00.164: INFO: Created: latency-svc-jwfdd
May 16 12:41:00.175: INFO: Got endpoints: latency-svc-c8rc5 [747.264413ms]
May 16 12:41:00.209: INFO: Created: latency-svc-mfd5d
May 16 12:41:00.251: INFO: Got endpoints: latency-svc-6m26c [775.97789ms]
May 16 12:41:00.284: INFO: Created: latency-svc-8zdpl
May 16 12:41:00.300: INFO: Got endpoints: latency-svc-5rqgr [773.901759ms]
May 16 12:41:00.351: INFO: Created: latency-svc-6dlp6
May 16 12:41:00.364: INFO: Got endpoints: latency-svc-b7474 [790.693408ms]
May 16 12:41:00.374: INFO: Got endpoints: latency-svc-qh29g [738.33217ms]
May 16 12:41:00.388: INFO: Created: latency-svc-hptmc
May 16 12:41:00.403: INFO: Created: latency-svc-99rm9
May 16 12:41:00.425: INFO: Got endpoints: latency-svc-t4b54 [739.597023ms]
May 16 12:41:00.455: INFO: Created: latency-svc-srp85
May 16 12:41:00.483: INFO: Got endpoints: latency-svc-cxszx [751.603942ms]
May 16 12:41:00.515: INFO: Created: latency-svc-vrjj4
May 16 12:41:00.526: INFO: Got endpoints: latency-svc-8fftw [748.462243ms]
May 16 12:41:00.564: INFO: Created: latency-svc-4ztx7
May 16 12:41:00.587: INFO: Got endpoints: latency-svc-xw8zt [762.44485ms]
May 16 12:41:00.617: INFO: Created: latency-svc-df7jz
May 16 12:41:00.625: INFO: Got endpoints: latency-svc-xmb82 [745.729814ms]
May 16 12:41:00.651: INFO: Created: latency-svc-9j7b9
May 16 12:41:00.680: INFO: Got endpoints: latency-svc-cg5zz [722.700994ms]
May 16 12:41:00.708: INFO: Created: latency-svc-5twg4
May 16 12:41:00.725: INFO: Got endpoints: latency-svc-q7qb2 [746.260183ms]
May 16 12:41:00.772: INFO: Created: latency-svc-sjcw4
May 16 12:41:00.780: INFO: Got endpoints: latency-svc-qvcdr [753.378095ms]
May 16 12:41:00.814: INFO: Created: latency-svc-7x98q
May 16 12:41:00.823: INFO: Got endpoints: latency-svc-ls65n [720.190619ms]
May 16 12:41:00.843: INFO: Created: latency-svc-xz62f
May 16 12:41:00.892: INFO: Got endpoints: latency-svc-jwfdd [759.345379ms]
May 16 12:41:00.912: INFO: Created: latency-svc-n5hwj
May 16 12:41:00.927: INFO: Got endpoints: latency-svc-mfd5d [751.556049ms]
May 16 12:41:00.950: INFO: Created: latency-svc-z4wtb
May 16 12:41:00.980: INFO: Got endpoints: latency-svc-8zdpl [728.252516ms]
May 16 12:41:01.029: INFO: Got endpoints: latency-svc-6dlp6 [729.319616ms]
May 16 12:41:01.033: INFO: Created: latency-svc-t46jp
May 16 12:41:01.049: INFO: Created: latency-svc-kbx57
May 16 12:41:01.090: INFO: Got endpoints: latency-svc-hptmc [726.02775ms]
May 16 12:41:01.154: INFO: Created: latency-svc-r6kpf
May 16 12:41:01.200: INFO: Got endpoints: latency-svc-srp85 [774.303472ms]
May 16 12:41:01.200: INFO: Got endpoints: latency-svc-99rm9 [826.566043ms]
May 16 12:41:01.228: INFO: Created: latency-svc-x7bc8
May 16 12:41:01.243: INFO: Got endpoints: latency-svc-vrjj4 [759.331402ms]
May 16 12:41:01.280: INFO: Created: latency-svc-hvb7x
May 16 12:41:01.295: INFO: Created: latency-svc-sm7sf
May 16 12:41:01.308: INFO: Got endpoints: latency-svc-4ztx7 [781.51053ms]
May 16 12:41:01.345: INFO: Created: latency-svc-k9ddr
May 16 12:41:01.351: INFO: Got endpoints: latency-svc-df7jz [763.884831ms]
May 16 12:41:01.370: INFO: Created: latency-svc-mzvw5
May 16 12:41:01.392: INFO: Got endpoints: latency-svc-9j7b9 [766.50465ms]
May 16 12:41:01.420: INFO: Created: latency-svc-x4bt2
May 16 12:41:01.436: INFO: Got endpoints: latency-svc-5twg4 [755.631231ms]
May 16 12:41:01.458: INFO: Created: latency-svc-5lv2f
May 16 12:41:01.473: INFO: Got endpoints: latency-svc-sjcw4 [747.909432ms]
May 16 12:41:01.524: INFO: Created: latency-svc-2c2th
May 16 12:41:01.573: INFO: Got endpoints: latency-svc-7x98q [792.971814ms]
May 16 12:41:01.591: INFO: Got endpoints: latency-svc-xz62f [768.282383ms]
May 16 12:41:01.609: INFO: Created: latency-svc-w5trq
May 16 12:41:01.626: INFO: Created: latency-svc-pqksl
May 16 12:41:01.635: INFO: Got endpoints: latency-svc-n5hwj [742.859926ms]
May 16 12:41:01.669: INFO: Created: latency-svc-2jqgr
May 16 12:41:01.706: INFO: Got endpoints: latency-svc-z4wtb [778.895302ms]
May 16 12:41:01.722: INFO: Created: latency-svc-l8xc7
May 16 12:41:01.737: INFO: Got endpoints: latency-svc-t46jp [757.351018ms]
May 16 12:41:01.763: INFO: Created: latency-svc-jx8jt
May 16 12:41:01.787: INFO: Got endpoints: latency-svc-kbx57 [757.030173ms]
May 16 12:41:01.834: INFO: Created: latency-svc-sxpxv
May 16 12:41:01.842: INFO: Got endpoints: latency-svc-r6kpf [752.041985ms]
May 16 12:41:01.863: INFO: Created: latency-svc-th6t8
May 16 12:41:01.883: INFO: Got endpoints: latency-svc-x7bc8 [683.441386ms]
May 16 12:41:01.925: INFO: Created: latency-svc-6tjxc
May 16 12:41:01.935: INFO: Got endpoints: latency-svc-hvb7x [734.286714ms]
May 16 12:41:01.989: INFO: Created: latency-svc-dt4q9
May 16 12:41:01.990: INFO: Got endpoints: latency-svc-sm7sf [747.261191ms]
May 16 12:41:02.017: INFO: Created: latency-svc-s8zjs
May 16 12:41:02.027: INFO: Got endpoints: latency-svc-k9ddr [719.298983ms]
May 16 12:41:02.041: INFO: Created: latency-svc-d4khb
May 16 12:41:02.095: INFO: Got endpoints: latency-svc-mzvw5 [744.070841ms]
May 16 12:41:02.146: INFO: Created: latency-svc-5jc7t
May 16 12:41:02.153: INFO: Got endpoints: latency-svc-x4bt2 [760.975781ms]
May 16 12:41:02.173: INFO: Created: latency-svc-4s74w
May 16 12:41:02.174: INFO: Got endpoints: latency-svc-5lv2f [738.445963ms]
May 16 12:41:02.195: INFO: Created: latency-svc-hc98q
May 16 12:41:02.235: INFO: Got endpoints: latency-svc-2c2th [762.578159ms]
May 16 12:41:02.261: INFO: Created: latency-svc-mdxtq
May 16 12:41:02.279: INFO: Got endpoints: latency-svc-w5trq [705.880253ms]
May 16 12:41:02.297: INFO: Created: latency-svc-l7k42
May 16 12:41:02.333: INFO: Got endpoints: latency-svc-pqksl [741.75604ms]
May 16 12:41:02.385: INFO: Created: latency-svc-tq56x
May 16 12:41:02.386: INFO: Got endpoints: latency-svc-2jqgr [750.272326ms]
May 16 12:41:02.400: INFO: Created: latency-svc-x7r9j
May 16 12:41:02.433: INFO: Got endpoints: latency-svc-l8xc7 [726.953303ms]
May 16 12:41:02.453: INFO: Created: latency-svc-qqdlg
May 16 12:41:02.482: INFO: Got endpoints: latency-svc-jx8jt [744.607729ms]
May 16 12:41:02.537: INFO: Created: latency-svc-kdvtw
May 16 12:41:02.547: INFO: Got endpoints: latency-svc-sxpxv [760.530579ms]
May 16 12:41:02.561: INFO: Created: latency-svc-8q989
May 16 12:41:02.573: INFO: Got endpoints: latency-svc-th6t8 [730.630252ms]
May 16 12:41:02.592: INFO: Created: latency-svc-wwqrl
May 16 12:41:02.730: INFO: Got endpoints: latency-svc-6tjxc [846.560189ms]
May 16 12:41:02.731: INFO: Got endpoints: latency-svc-dt4q9 [796.451623ms]
May 16 12:41:02.734: INFO: Got endpoints: latency-svc-s8zjs [743.740965ms]
May 16 12:41:02.767: INFO: Created: latency-svc-zm6v4
May 16 12:41:02.781: INFO: Got endpoints: latency-svc-d4khb [753.413564ms]
May 16 12:41:02.792: INFO: Created: latency-svc-bkm75
May 16 12:41:02.819: INFO: Created: latency-svc-5lsmb
May 16 12:41:02.831: INFO: Got endpoints: latency-svc-5jc7t [735.841294ms]
May 16 12:41:02.841: INFO: Created: latency-svc-f9nv7
May 16 12:41:02.858: INFO: Created: latency-svc-g8tlx
May 16 12:41:02.888: INFO: Got endpoints: latency-svc-4s74w [735.232414ms]
May 16 12:41:02.908: INFO: Created: latency-svc-l8lwp
May 16 12:41:02.937: INFO: Got endpoints: latency-svc-hc98q [762.737703ms]
May 16 12:41:02.985: INFO: Created: latency-svc-c5dcf
May 16 12:41:02.986: INFO: Got endpoints: latency-svc-mdxtq [750.094979ms]
May 16 12:41:03.026: INFO: Created: latency-svc-k2bkb
May 16 12:41:03.031: INFO: Got endpoints: latency-svc-l7k42 [751.858767ms]
May 16 12:41:03.086: INFO: Created: latency-svc-rpmkt
May 16 12:41:03.086: INFO: Got endpoints: latency-svc-tq56x [752.603652ms]
May 16 12:41:03.115: INFO: Created: latency-svc-bvrj5
May 16 12:41:03.142: INFO: Got endpoints: latency-svc-x7r9j [755.879482ms]
May 16 12:41:03.160: INFO: Created: latency-svc-878jq
May 16 12:41:03.175: INFO: Got endpoints: latency-svc-qqdlg [742.181781ms]
May 16 12:41:03.229: INFO: Created: latency-svc-fmm4k
May 16 12:41:03.235: INFO: Got endpoints: latency-svc-kdvtw [753.254774ms]
May 16 12:41:03.280: INFO: Created: latency-svc-zqmxt
May 16 12:41:03.296: INFO: Got endpoints: latency-svc-8q989 [748.545113ms]
May 16 12:41:03.323: INFO: Created: latency-svc-6ggk9
May 16 12:41:03.329: INFO: Got endpoints: latency-svc-wwqrl [755.952786ms]
May 16 12:41:03.368: INFO: Created: latency-svc-sq6n7
May 16 12:41:03.400: INFO: Got endpoints: latency-svc-zm6v4 [670.37032ms]
May 16 12:41:03.420: INFO: Created: latency-svc-5twjh
May 16 12:41:03.424: INFO: Got endpoints: latency-svc-bkm75 [692.675439ms]
May 16 12:41:03.444: INFO: Created: latency-svc-2zdb8
May 16 12:41:03.488: INFO: Got endpoints: latency-svc-5lsmb [754.646771ms]
May 16 12:41:03.507: INFO: Created: latency-svc-8h8cn
May 16 12:41:03.542: INFO: Got endpoints: latency-svc-f9nv7 [761.256944ms]
May 16 12:41:03.569: INFO: Created: latency-svc-vzbtn
May 16 12:41:03.588: INFO: Got endpoints: latency-svc-g8tlx [756.326854ms]
May 16 12:41:03.603: INFO: Created: latency-svc-n5rgz
May 16 12:41:03.625: INFO: Got endpoints: latency-svc-l8lwp [736.29745ms]
May 16 12:41:03.674: INFO: Got endpoints: latency-svc-c5dcf [736.353128ms]
May 16 12:41:03.731: INFO: Got endpoints: latency-svc-k2bkb [745.419095ms]
May 16 12:41:03.783: INFO: Got endpoints: latency-svc-rpmkt [752.580798ms]
May 16 12:41:03.843: INFO: Got endpoints: latency-svc-bvrj5 [756.702479ms]
May 16 12:41:03.892: INFO: Got endpoints: latency-svc-878jq [750.271048ms]
May 16 12:41:03.942: INFO: Got endpoints: latency-svc-fmm4k [765.953492ms]
May 16 12:41:03.982: INFO: Got endpoints: latency-svc-zqmxt [746.321405ms]
May 16 12:41:04.035: INFO: Got endpoints: latency-svc-6ggk9 [738.653407ms]
May 16 12:41:04.085: INFO: Got endpoints: latency-svc-sq6n7 [755.207266ms]
May 16 12:41:04.122: INFO: Got endpoints: latency-svc-5twjh [722.038441ms]
May 16 12:41:04.179: INFO: Got endpoints: latency-svc-2zdb8 [754.638432ms]
May 16 12:41:04.223: INFO: Got endpoints: latency-svc-8h8cn [734.86043ms]
May 16 12:41:04.294: INFO: Got endpoints: latency-svc-vzbtn [751.799061ms]
May 16 12:41:04.329: INFO: Got endpoints: latency-svc-n5rgz [741.406915ms]
May 16 12:41:04.329: INFO: Latencies: [49.815648ms 50.231544ms 71.375802ms 107.970389ms 113.240705ms 129.115893ms 160.573469ms 179.774043ms 200.460379ms 222.220925ms 226.011983ms 270.651297ms 294.528747ms 296.89825ms 301.74629ms 303.055137ms 311.327091ms 311.615681ms 311.639127ms 313.09347ms 320.138075ms 321.089985ms 324.07414ms 324.374263ms 324.506346ms 324.854371ms 331.242532ms 331.593554ms 336.466518ms 341.195379ms 348.124419ms 354.43816ms 354.531928ms 355.677302ms 355.74713ms 356.475402ms 357.899343ms 361.056424ms 361.229849ms 365.897941ms 366.826507ms 368.445697ms 368.495514ms 370.360914ms 370.916335ms 371.758534ms 372.356485ms 372.937689ms 386.512393ms 387.762999ms 390.294779ms 398.753394ms 403.415463ms 416.71665ms 420.217913ms 426.958977ms 430.356079ms 455.539626ms 489.134938ms 492.772432ms 550.243422ms 552.047794ms 594.442965ms 597.761678ms 637.959699ms 670.37032ms 683.441386ms 685.874288ms 687.296942ms 692.675439ms 705.880253ms 711.34006ms 715.952002ms 719.298983ms 719.854781ms 720.190619ms 722.038441ms 722.700994ms 724.228692ms 725.290719ms 725.831673ms 726.02775ms 726.953303ms 728.252516ms 729.319616ms 730.630252ms 732.260195ms 732.715081ms 733.016486ms 734.129191ms 734.286714ms 734.86043ms 735.232414ms 735.36987ms 735.841294ms 736.000647ms 736.048686ms 736.29745ms 736.353128ms 738.069136ms 738.33217ms 738.445963ms 738.653407ms 739.597023ms 740.627679ms 741.406915ms 741.576095ms 741.75604ms 742.010061ms 742.181781ms 742.859926ms 742.950616ms 743.740965ms 744.070841ms 744.200297ms 744.458054ms 744.607729ms 744.702959ms 745.142672ms 745.419095ms 745.729814ms 745.877302ms 746.260183ms 746.321405ms 747.261191ms 747.264413ms 747.38671ms 747.909432ms 748.01018ms 748.462243ms 748.545113ms 748.967165ms 749.055071ms 750.094979ms 750.266645ms 750.271048ms 750.272326ms 750.413184ms 751.432223ms 751.556049ms 751.603942ms 751.799061ms 751.858767ms 751.954192ms 752.041985ms 752.580798ms 752.603652ms 752.659237ms 752.858964ms 752.893552ms 753.062185ms 753.254774ms 753.378095ms 753.413564ms 754.2665ms 754.638432ms 754.646771ms 754.651941ms 754.655699ms 754.792583ms 754.927191ms 755.207266ms 755.61718ms 755.631231ms 755.879482ms 755.952786ms 756.133921ms 756.326854ms 756.702479ms 757.030173ms 757.351018ms 758.913028ms 759.331402ms 759.345379ms 759.499206ms 760.530579ms 760.652212ms 760.975781ms 761.256944ms 762.44485ms 762.578159ms 762.737703ms 763.409572ms 763.884831ms 764.354593ms 765.953492ms 766.50465ms 768.282383ms 772.516759ms 773.901759ms 774.303472ms 775.97789ms 776.481975ms 778.895302ms 781.51053ms 790.693408ms 792.971814ms 796.451623ms 826.566043ms 846.560189ms]
May 16 12:41:04.330: INFO: 50 %ile: 738.33217ms
May 16 12:41:04.330: INFO: 90 %ile: 762.578159ms
May 16 12:41:04.330: INFO: 99 %ile: 826.566043ms
May 16 12:41:04.330: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:41:04.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4360" for this suite.
May 16 12:41:22.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:41:22.708: INFO: namespace svc-latency-4360 deletion completed in 18.370151095s

• [SLOW TEST:30.244 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:41:22.708: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-e95b352a-77d7-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 12:41:22.864: INFO: Waiting up to 5m0s for pod "pod-secrets-e95c8093-77d7-11e9-8275-42209be91bd2" in namespace "secrets-339" to be "success or failure"
May 16 12:41:22.869: INFO: Pod "pod-secrets-e95c8093-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.123532ms
May 16 12:41:24.877: INFO: Pod "pod-secrets-e95c8093-77d7-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012526173s
May 16 12:41:26.885: INFO: Pod "pod-secrets-e95c8093-77d7-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020382155s
STEP: Saw pod success
May 16 12:41:26.885: INFO: Pod "pod-secrets-e95c8093-77d7-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:41:26.892: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-secrets-e95c8093-77d7-11e9-8275-42209be91bd2 container secret-volume-test: <nil>
STEP: delete the pod
May 16 12:41:26.933: INFO: Waiting for pod pod-secrets-e95c8093-77d7-11e9-8275-42209be91bd2 to disappear
May 16 12:41:26.940: INFO: Pod pod-secrets-e95c8093-77d7-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:41:26.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-339" for this suite.
May 16 12:41:32.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:41:33.289: INFO: namespace secrets-339 deletion completed in 6.330333053s

• [SLOW TEST:10.581 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:41:33.289: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 16 12:41:34.193: INFO: Pod name wrapped-volume-race-f01a0f26-77d7-11e9-8275-42209be91bd2: Found 0 pods out of 5
May 16 12:41:39.204: INFO: Pod name wrapped-volume-race-f01a0f26-77d7-11e9-8275-42209be91bd2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f01a0f26-77d7-11e9-8275-42209be91bd2 in namespace emptydir-wrapper-89, will wait for the garbage collector to delete the pods
May 16 12:41:53.336: INFO: Deleting ReplicationController wrapped-volume-race-f01a0f26-77d7-11e9-8275-42209be91bd2 took: 19.940056ms
May 16 12:41:53.936: INFO: Terminating ReplicationController wrapped-volume-race-f01a0f26-77d7-11e9-8275-42209be91bd2 pods took: 600.252741ms
STEP: Creating RC which spawns configmap-volume pods
May 16 12:42:38.222: INFO: Pod name wrapped-volume-race-163edd0e-77d8-11e9-8275-42209be91bd2: Found 0 pods out of 5
May 16 12:42:43.238: INFO: Pod name wrapped-volume-race-163edd0e-77d8-11e9-8275-42209be91bd2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-163edd0e-77d8-11e9-8275-42209be91bd2 in namespace emptydir-wrapper-89, will wait for the garbage collector to delete the pods
May 16 12:42:57.428: INFO: Deleting ReplicationController wrapped-volume-race-163edd0e-77d8-11e9-8275-42209be91bd2 took: 19.445118ms
May 16 12:42:58.029: INFO: Terminating ReplicationController wrapped-volume-race-163edd0e-77d8-11e9-8275-42209be91bd2 pods took: 600.461941ms
STEP: Creating RC which spawns configmap-volume pods
May 16 12:43:38.042: INFO: Pod name wrapped-volume-race-39e27b0c-77d8-11e9-8275-42209be91bd2: Found 0 pods out of 5
May 16 12:43:43.136: INFO: Pod name wrapped-volume-race-39e27b0c-77d8-11e9-8275-42209be91bd2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-39e27b0c-77d8-11e9-8275-42209be91bd2 in namespace emptydir-wrapper-89, will wait for the garbage collector to delete the pods
May 16 12:43:59.286: INFO: Deleting ReplicationController wrapped-volume-race-39e27b0c-77d8-11e9-8275-42209be91bd2 took: 16.985574ms
May 16 12:43:59.987: INFO: Terminating ReplicationController wrapped-volume-race-39e27b0c-77d8-11e9-8275-42209be91bd2 pods took: 700.527299ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:44:39.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-89" for this suite.
May 16 12:44:51.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:44:51.592: INFO: namespace emptydir-wrapper-89 deletion completed in 12.332194212s

• [SLOW TEST:198.303 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:44:51.593: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-4527
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4527 to expose endpoints map[]
May 16 12:44:51.745: INFO: successfully validated that service multi-endpoint-test in namespace services-4527 exposes endpoints map[] (16.27645ms elapsed)
STEP: Creating pod pod1 in namespace services-4527
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4527 to expose endpoints map[pod1:[100]]
May 16 12:44:54.834: INFO: successfully validated that service multi-endpoint-test in namespace services-4527 exposes endpoints map[pod1:[100]] (3.052009107s elapsed)
STEP: Creating pod pod2 in namespace services-4527
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4527 to expose endpoints map[pod1:[100] pod2:[101]]
May 16 12:44:59.005: INFO: successfully validated that service multi-endpoint-test in namespace services-4527 exposes endpoints map[pod1:[100] pod2:[101]] (4.16016948s elapsed)
STEP: Deleting pod pod1 in namespace services-4527
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4527 to expose endpoints map[pod2:[101]]
May 16 12:44:59.082: INFO: successfully validated that service multi-endpoint-test in namespace services-4527 exposes endpoints map[pod2:[101]] (57.92198ms elapsed)
STEP: Deleting pod pod2 in namespace services-4527
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4527 to expose endpoints map[]
May 16 12:45:00.121: INFO: successfully validated that service multi-endpoint-test in namespace services-4527 exposes endpoints map[] (1.024143527s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:45:00.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4527" for this suite.
May 16 12:45:24.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:45:24.606: INFO: namespace services-4527 deletion completed in 24.376041474s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:33.015 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:45:24.614: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-798a0531-77d8-11e9-8275-42209be91bd2
STEP: Creating configMap with name cm-test-opt-upd-798a0573-77d8-11e9-8275-42209be91bd2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-798a0531-77d8-11e9-8275-42209be91bd2
STEP: Updating configmap cm-test-opt-upd-798a0573-77d8-11e9-8275-42209be91bd2
STEP: Creating configMap with name cm-test-opt-create-798a0593-77d8-11e9-8275-42209be91bd2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:45:31.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5219" for this suite.
May 16 12:45:55.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:45:55.823: INFO: namespace projected-5219 deletion completed in 24.351794914s

• [SLOW TEST:31.210 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:45:55.825: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 16 12:45:55.966: INFO: Waiting up to 5m0s for pod "downward-api-8c24d55f-77d8-11e9-8275-42209be91bd2" in namespace "downward-api-804" to be "success or failure"
May 16 12:45:55.972: INFO: Pod "downward-api-8c24d55f-77d8-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.486872ms
May 16 12:45:57.979: INFO: Pod "downward-api-8c24d55f-77d8-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012318766s
May 16 12:45:59.996: INFO: Pod "downward-api-8c24d55f-77d8-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029800791s
STEP: Saw pod success
May 16 12:45:59.996: INFO: Pod "downward-api-8c24d55f-77d8-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:46:00.009: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downward-api-8c24d55f-77d8-11e9-8275-42209be91bd2 container dapi-container: <nil>
STEP: delete the pod
May 16 12:46:00.149: INFO: Waiting for pod downward-api-8c24d55f-77d8-11e9-8275-42209be91bd2 to disappear
May 16 12:46:00.157: INFO: Pod downward-api-8c24d55f-77d8-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:46:00.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-804" for this suite.
May 16 12:46:06.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:46:06.616: INFO: namespace downward-api-804 deletion completed in 6.450392685s

• [SLOW TEST:10.791 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:46:06.618: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 12:46:06.802: INFO: Waiting up to 5m0s for pod "downwardapi-volume-929b39ce-77d8-11e9-8275-42209be91bd2" in namespace "projected-3021" to be "success or failure"
May 16 12:46:06.840: INFO: Pod "downwardapi-volume-929b39ce-77d8-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 37.642839ms
May 16 12:46:08.848: INFO: Pod "downwardapi-volume-929b39ce-77d8-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045635117s
May 16 12:46:10.855: INFO: Pod "downwardapi-volume-929b39ce-77d8-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053058874s
STEP: Saw pod success
May 16 12:46:10.855: INFO: Pod "downwardapi-volume-929b39ce-77d8-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:46:10.861: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-929b39ce-77d8-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 12:46:10.925: INFO: Waiting for pod downwardapi-volume-929b39ce-77d8-11e9-8275-42209be91bd2 to disappear
May 16 12:46:10.943: INFO: Pod downwardapi-volume-929b39ce-77d8-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:46:10.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3021" for this suite.
May 16 12:46:16.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:46:17.319: INFO: namespace projected-3021 deletion completed in 6.367668653s

• [SLOW TEST:10.701 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:46:17.328: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:46:17.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1290" for this suite.
May 16 12:46:23.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:46:23.908: INFO: namespace kubelet-test-1290 deletion completed in 6.335701652s

• [SLOW TEST:6.580 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:46:23.908: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-9ceacf87-77d8-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 12:46:24.126: INFO: Waiting up to 5m0s for pod "pod-configmaps-9cec32ae-77d8-11e9-8275-42209be91bd2" in namespace "configmap-5108" to be "success or failure"
May 16 12:46:24.131: INFO: Pod "pod-configmaps-9cec32ae-77d8-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.000569ms
May 16 12:46:26.142: INFO: Pod "pod-configmaps-9cec32ae-77d8-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016259491s
May 16 12:46:28.150: INFO: Pod "pod-configmaps-9cec32ae-77d8-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023653229s
STEP: Saw pod success
May 16 12:46:28.150: INFO: Pod "pod-configmaps-9cec32ae-77d8-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:46:28.157: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-configmaps-9cec32ae-77d8-11e9-8275-42209be91bd2 container configmap-volume-test: <nil>
STEP: delete the pod
May 16 12:46:28.240: INFO: Waiting for pod pod-configmaps-9cec32ae-77d8-11e9-8275-42209be91bd2 to disappear
May 16 12:46:28.246: INFO: Pod pod-configmaps-9cec32ae-77d8-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:46:28.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5108" for this suite.
May 16 12:46:34.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:46:34.740: INFO: namespace configmap-5108 deletion completed in 6.487497862s

• [SLOW TEST:10.832 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:46:34.741: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 12:46:34.870: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 16 12:46:39.880: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 16 12:46:39.880: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 16 12:46:39.961: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-3795,SelfLink:/apis/apps/v1/namespaces/deployment-3795/deployments/test-cleanup-deployment,UID:a64d1bc9-77d8-11e9-b44f-0a580af42602,ResourceVersion:19390,Generation:1,CreationTimestamp:2019-05-16 12:46:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

May 16 12:46:39.968: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
May 16 12:46:39.968: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 16 12:46:39.968: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-3795,SelfLink:/apis/apps/v1/namespaces/deployment-3795/replicasets/test-cleanup-controller,UID:a34b09be-77d8-11e9-b44f-0a580af42602,ResourceVersion:19391,Generation:1,CreationTimestamp:2019-05-16 12:46:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment a64d1bc9-77d8-11e9-b44f-0a580af42602 0xc002b9a667 0xc002b9a668}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 16 12:46:40.138: INFO: Pod "test-cleanup-controller-t6sxj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-t6sxj,GenerateName:test-cleanup-controller-,Namespace:deployment-3795,SelfLink:/api/v1/namespaces/deployment-3795/pods/test-cleanup-controller-t6sxj,UID:a34ef60d-77d8-11e9-b44f-0a580af42602,ResourceVersion:19380,Generation:0,CreationTimestamp:2019-05-16 12:46:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller a34b09be-77d8-11e9-b44f-0a580af42602 0xc002b9ad17 0xc002b9ad18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-66mlm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-66mlm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-66mlm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b9ad80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b9ada0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:46:34 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:46:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:46:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:46:34 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.0.113,StartTime:2019-05-16 12:46:34 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-16 12:46:36 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0408dac7302847cc9400d1e49907e2f044475934574ef6f8aad532648206c2f9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:46:40.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3795" for this suite.
May 16 12:46:48.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:46:48.644: INFO: namespace deployment-3795 deletion completed in 8.454752792s

• [SLOW TEST:13.903 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:46:48.645: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-9qg7
STEP: Creating a pod to test atomic-volume-subpath
May 16 12:46:48.834: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9qg7" in namespace "subpath-4428" to be "success or failure"
May 16 12:46:48.842: INFO: Pod "pod-subpath-test-configmap-9qg7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.167096ms
May 16 12:46:50.849: INFO: Pod "pod-subpath-test-configmap-9qg7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015126836s
May 16 12:46:52.857: INFO: Pod "pod-subpath-test-configmap-9qg7": Phase="Running", Reason="", readiness=true. Elapsed: 4.022582892s
May 16 12:46:54.865: INFO: Pod "pod-subpath-test-configmap-9qg7": Phase="Running", Reason="", readiness=true. Elapsed: 6.030703284s
May 16 12:46:56.874: INFO: Pod "pod-subpath-test-configmap-9qg7": Phase="Running", Reason="", readiness=true. Elapsed: 8.039761235s
May 16 12:46:58.880: INFO: Pod "pod-subpath-test-configmap-9qg7": Phase="Running", Reason="", readiness=true. Elapsed: 10.046066978s
May 16 12:47:00.886: INFO: Pod "pod-subpath-test-configmap-9qg7": Phase="Running", Reason="", readiness=true. Elapsed: 12.052037908s
May 16 12:47:02.894: INFO: Pod "pod-subpath-test-configmap-9qg7": Phase="Running", Reason="", readiness=true. Elapsed: 14.060477861s
May 16 12:47:04.911: INFO: Pod "pod-subpath-test-configmap-9qg7": Phase="Running", Reason="", readiness=true. Elapsed: 16.076868976s
May 16 12:47:07.142: INFO: Pod "pod-subpath-test-configmap-9qg7": Phase="Running", Reason="", readiness=true. Elapsed: 18.307713299s
May 16 12:47:09.149: INFO: Pod "pod-subpath-test-configmap-9qg7": Phase="Running", Reason="", readiness=true. Elapsed: 20.31525757s
May 16 12:47:11.178: INFO: Pod "pod-subpath-test-configmap-9qg7": Phase="Running", Reason="", readiness=true. Elapsed: 22.344206417s
May 16 12:47:13.186: INFO: Pod "pod-subpath-test-configmap-9qg7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.352201978s
STEP: Saw pod success
May 16 12:47:13.186: INFO: Pod "pod-subpath-test-configmap-9qg7" satisfied condition "success or failure"
May 16 12:47:13.193: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-subpath-test-configmap-9qg7 container test-container-subpath-configmap-9qg7: <nil>
STEP: delete the pod
May 16 12:47:13.278: INFO: Waiting for pod pod-subpath-test-configmap-9qg7 to disappear
May 16 12:47:13.286: INFO: Pod pod-subpath-test-configmap-9qg7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9qg7
May 16 12:47:13.286: INFO: Deleting pod "pod-subpath-test-configmap-9qg7" in namespace "subpath-4428"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:47:13.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4428" for this suite.
May 16 12:47:19.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:47:19.774: INFO: namespace subpath-4428 deletion completed in 6.468112124s

• [SLOW TEST:31.129 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:47:19.775: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 16 12:47:20.082: INFO: Waiting up to 5m0s for pod "downward-api-be3492e6-77d8-11e9-8275-42209be91bd2" in namespace "downward-api-7377" to be "success or failure"
May 16 12:47:20.088: INFO: Pod "downward-api-be3492e6-77d8-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.593844ms
May 16 12:47:22.103: INFO: Pod "downward-api-be3492e6-77d8-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02062876s
May 16 12:47:24.123: INFO: Pod "downward-api-be3492e6-77d8-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040363197s
May 16 12:47:26.132: INFO: Pod "downward-api-be3492e6-77d8-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050087752s
STEP: Saw pod success
May 16 12:47:26.133: INFO: Pod "downward-api-be3492e6-77d8-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:47:26.139: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh pod downward-api-be3492e6-77d8-11e9-8275-42209be91bd2 container dapi-container: <nil>
STEP: delete the pod
May 16 12:47:26.207: INFO: Waiting for pod downward-api-be3492e6-77d8-11e9-8275-42209be91bd2 to disappear
May 16 12:47:26.211: INFO: Pod downward-api-be3492e6-77d8-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:47:26.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7377" for this suite.
May 16 12:47:32.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:47:32.638: INFO: namespace downward-api-7377 deletion completed in 6.419654121s

• [SLOW TEST:12.863 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:47:32.638: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-c5dcb286-77d8-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 12:47:32.801: INFO: Waiting up to 5m0s for pod "pod-configmaps-c5de1f1f-77d8-11e9-8275-42209be91bd2" in namespace "configmap-1284" to be "success or failure"
May 16 12:47:32.820: INFO: Pod "pod-configmaps-c5de1f1f-77d8-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.437429ms
May 16 12:47:34.846: INFO: Pod "pod-configmaps-c5de1f1f-77d8-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044908304s
May 16 12:47:36.855: INFO: Pod "pod-configmaps-c5de1f1f-77d8-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054691315s
STEP: Saw pod success
May 16 12:47:36.855: INFO: Pod "pod-configmaps-c5de1f1f-77d8-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:47:36.865: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-configmaps-c5de1f1f-77d8-11e9-8275-42209be91bd2 container configmap-volume-test: <nil>
STEP: delete the pod
May 16 12:47:37.021: INFO: Waiting for pod pod-configmaps-c5de1f1f-77d8-11e9-8275-42209be91bd2 to disappear
May 16 12:47:37.026: INFO: Pod pod-configmaps-c5de1f1f-77d8-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:47:37.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1284" for this suite.
May 16 12:47:43.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:47:43.405: INFO: namespace configmap-1284 deletion completed in 6.368769686s

• [SLOW TEST:10.767 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:47:43.405: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-f5g4j in namespace proxy-4699
I0516 12:47:43.579491      16 runners.go:184] Created replication controller with name: proxy-service-f5g4j, namespace: proxy-4699, replica count: 1
I0516 12:47:44.632607      16 runners.go:184] proxy-service-f5g4j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0516 12:47:45.632837      16 runners.go:184] proxy-service-f5g4j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0516 12:47:46.633015      16 runners.go:184] proxy-service-f5g4j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0516 12:47:47.633301      16 runners.go:184] proxy-service-f5g4j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0516 12:47:48.633527      16 runners.go:184] proxy-service-f5g4j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0516 12:47:49.633714      16 runners.go:184] proxy-service-f5g4j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0516 12:47:50.633894      16 runners.go:184] proxy-service-f5g4j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0516 12:47:51.634091      16 runners.go:184] proxy-service-f5g4j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0516 12:47:52.634347      16 runners.go:184] proxy-service-f5g4j Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 16 12:47:52.640: INFO: setup took 9.10251672s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 16 12:47:52.705: INFO: (0) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 64.496703ms)
May 16 12:47:52.708: INFO: (0) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 67.933325ms)
May 16 12:47:52.708: INFO: (0) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 68.121381ms)
May 16 12:47:52.708: INFO: (0) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 67.530345ms)
May 16 12:47:52.709: INFO: (0) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 68.431267ms)
May 16 12:47:52.709: INFO: (0) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 68.68641ms)
May 16 12:47:52.709: INFO: (0) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 69.406465ms)
May 16 12:47:52.713: INFO: (0) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 72.832914ms)
May 16 12:47:52.715: INFO: (0) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 74.567507ms)
May 16 12:47:52.716: INFO: (0) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 74.832535ms)
May 16 12:47:52.717: INFO: (0) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 76.469964ms)
May 16 12:47:52.723: INFO: (0) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 83.416781ms)
May 16 12:47:52.733: INFO: (0) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 92.381783ms)
May 16 12:47:52.733: INFO: (0) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 92.761637ms)
May 16 12:47:52.734: INFO: (0) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 92.944821ms)
May 16 12:47:52.734: INFO: (0) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 93.162367ms)
May 16 12:47:52.749: INFO: (1) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 15.55266ms)
May 16 12:47:52.750: INFO: (1) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 15.515921ms)
May 16 12:47:52.750: INFO: (1) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 16.337345ms)
May 16 12:47:52.752: INFO: (1) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 17.520747ms)
May 16 12:47:52.753: INFO: (1) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 19.101938ms)
May 16 12:47:52.753: INFO: (1) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 19.004256ms)
May 16 12:47:52.754: INFO: (1) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 19.573518ms)
May 16 12:47:52.754: INFO: (1) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 19.819ms)
May 16 12:47:52.754: INFO: (1) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 19.517531ms)
May 16 12:47:52.755: INFO: (1) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 19.845395ms)
May 16 12:47:52.755: INFO: (1) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 20.236818ms)
May 16 12:47:52.755: INFO: (1) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 21.091747ms)
May 16 12:47:52.756: INFO: (1) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 21.069824ms)
May 16 12:47:52.756: INFO: (1) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 21.302002ms)
May 16 12:47:52.757: INFO: (1) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 21.956609ms)
May 16 12:47:52.763: INFO: (1) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 28.526789ms)
May 16 12:47:52.788: INFO: (2) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 24.58015ms)
May 16 12:47:52.789: INFO: (2) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 25.716203ms)
May 16 12:47:52.789: INFO: (2) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 25.349695ms)
May 16 12:47:52.791: INFO: (2) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 27.376166ms)
May 16 12:47:52.792: INFO: (2) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 27.431804ms)
May 16 12:47:52.792: INFO: (2) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 27.641511ms)
May 16 12:47:52.792: INFO: (2) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 27.660139ms)
May 16 12:47:52.792: INFO: (2) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 27.638346ms)
May 16 12:47:52.794: INFO: (2) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 29.432084ms)
May 16 12:47:52.803: INFO: (2) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 38.923678ms)
May 16 12:47:52.805: INFO: (2) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 40.927107ms)
May 16 12:47:52.805: INFO: (2) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 40.907839ms)
May 16 12:47:52.806: INFO: (2) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 41.44486ms)
May 16 12:47:52.806: INFO: (2) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 42.309113ms)
May 16 12:47:52.806: INFO: (2) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 41.538992ms)
May 16 12:47:52.807: INFO: (2) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 42.265769ms)
May 16 12:47:52.824: INFO: (3) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 17.035151ms)
May 16 12:47:52.824: INFO: (3) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 17.026625ms)
May 16 12:47:52.824: INFO: (3) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 17.255442ms)
May 16 12:47:52.825: INFO: (3) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 17.162542ms)
May 16 12:47:52.825: INFO: (3) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 17.56478ms)
May 16 12:47:52.826: INFO: (3) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 18.424229ms)
May 16 12:47:52.826: INFO: (3) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 18.764786ms)
May 16 12:47:52.826: INFO: (3) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 18.996338ms)
May 16 12:47:52.826: INFO: (3) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 19.047471ms)
May 16 12:47:52.827: INFO: (3) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 19.604061ms)
May 16 12:47:52.827: INFO: (3) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 19.718941ms)
May 16 12:47:52.828: INFO: (3) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 20.804171ms)
May 16 12:47:52.829: INFO: (3) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 21.444852ms)
May 16 12:47:52.829: INFO: (3) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 21.972791ms)
May 16 12:47:52.868: INFO: (3) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 60.797472ms)
May 16 12:47:52.868: INFO: (3) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 61.136457ms)
May 16 12:47:52.885: INFO: (4) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 16.071022ms)
May 16 12:47:52.886: INFO: (4) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 17.023523ms)
May 16 12:47:52.887: INFO: (4) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 17.828519ms)
May 16 12:47:52.893: INFO: (4) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 23.770756ms)
May 16 12:47:52.893: INFO: (4) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 24.142667ms)
May 16 12:47:52.893: INFO: (4) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 24.327852ms)
May 16 12:47:52.895: INFO: (4) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 25.664509ms)
May 16 12:47:52.895: INFO: (4) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 25.881459ms)
May 16 12:47:52.895: INFO: (4) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 25.637696ms)
May 16 12:47:52.895: INFO: (4) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 26.581403ms)
May 16 12:47:52.896: INFO: (4) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 26.864623ms)
May 16 12:47:52.896: INFO: (4) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 26.586898ms)
May 16 12:47:52.896: INFO: (4) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 27.238524ms)
May 16 12:47:52.940: INFO: (4) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 71.229948ms)
May 16 12:47:52.940: INFO: (4) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 71.182634ms)
May 16 12:47:52.940: INFO: (4) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 71.486917ms)
May 16 12:47:52.957: INFO: (5) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 15.972569ms)
May 16 12:47:52.957: INFO: (5) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 15.542827ms)
May 16 12:47:52.957: INFO: (5) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 15.84809ms)
May 16 12:47:52.957: INFO: (5) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 15.695273ms)
May 16 12:47:52.958: INFO: (5) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 16.403585ms)
May 16 12:47:52.958: INFO: (5) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 16.5379ms)
May 16 12:47:52.958: INFO: (5) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 17.111993ms)
May 16 12:47:52.958: INFO: (5) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 16.173507ms)
May 16 12:47:52.958: INFO: (5) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 16.473062ms)
May 16 12:47:52.958: INFO: (5) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 17.22521ms)
May 16 12:47:52.961: INFO: (5) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 19.732447ms)
May 16 12:47:52.961: INFO: (5) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 19.781567ms)
May 16 12:47:52.961: INFO: (5) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 20.120681ms)
May 16 12:47:52.962: INFO: (5) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 20.881086ms)
May 16 12:47:52.962: INFO: (5) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 21.05724ms)
May 16 12:47:53.014: INFO: (5) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 72.811077ms)
May 16 12:47:53.039: INFO: (6) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 24.803991ms)
May 16 12:47:53.039: INFO: (6) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 24.629525ms)
May 16 12:47:53.040: INFO: (6) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 24.853875ms)
May 16 12:47:53.040: INFO: (6) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 26.54132ms)
May 16 12:47:53.040: INFO: (6) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 25.738334ms)
May 16 12:47:53.041: INFO: (6) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 27.007135ms)
May 16 12:47:53.041: INFO: (6) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 26.614883ms)
May 16 12:47:53.042: INFO: (6) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 27.355697ms)
May 16 12:47:53.042: INFO: (6) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 27.070379ms)
May 16 12:47:53.042: INFO: (6) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 27.338446ms)
May 16 12:47:53.044: INFO: (6) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 30.243405ms)
May 16 12:47:53.088: INFO: (6) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 73.784224ms)
May 16 12:47:53.089: INFO: (6) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 74.517401ms)
May 16 12:47:53.089: INFO: (6) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 73.926222ms)
May 16 12:47:53.089: INFO: (6) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 74.417603ms)
May 16 12:47:53.091: INFO: (6) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 76.534642ms)
May 16 12:47:53.109: INFO: (7) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 16.634366ms)
May 16 12:47:53.109: INFO: (7) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 17.565248ms)
May 16 12:47:53.110: INFO: (7) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 17.105444ms)
May 16 12:47:53.110: INFO: (7) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 17.922126ms)
May 16 12:47:53.110: INFO: (7) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 17.571043ms)
May 16 12:47:53.110: INFO: (7) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 18.967174ms)
May 16 12:47:53.112: INFO: (7) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 20.638468ms)
May 16 12:47:53.113: INFO: (7) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 20.823397ms)
May 16 12:47:53.113: INFO: (7) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 21.998709ms)
May 16 12:47:53.114: INFO: (7) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 20.879636ms)
May 16 12:47:53.114: INFO: (7) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 21.252753ms)
May 16 12:47:53.114: INFO: (7) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 22.156226ms)
May 16 12:47:53.163: INFO: (7) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 71.544466ms)
May 16 12:47:53.163: INFO: (7) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 70.668058ms)
May 16 12:47:53.163: INFO: (7) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 70.8969ms)
May 16 12:47:53.164: INFO: (7) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 71.946643ms)
May 16 12:47:53.188: INFO: (8) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 23.774399ms)
May 16 12:47:53.188: INFO: (8) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 24.6104ms)
May 16 12:47:53.189: INFO: (8) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 24.924188ms)
May 16 12:47:53.189: INFO: (8) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 24.476686ms)
May 16 12:47:53.189: INFO: (8) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 24.992299ms)
May 16 12:47:53.190: INFO: (8) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 25.707335ms)
May 16 12:47:53.190: INFO: (8) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 26.090666ms)
May 16 12:47:53.191: INFO: (8) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 26.284331ms)
May 16 12:47:53.191: INFO: (8) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 26.529377ms)
May 16 12:47:53.191: INFO: (8) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 27.082319ms)
May 16 12:47:53.192: INFO: (8) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 27.476219ms)
May 16 12:47:53.192: INFO: (8) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 27.855407ms)
May 16 12:47:53.192: INFO: (8) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 27.764956ms)
May 16 12:47:53.192: INFO: (8) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 28.006555ms)
May 16 12:47:53.192: INFO: (8) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 28.295011ms)
May 16 12:47:53.193: INFO: (8) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 28.839922ms)
May 16 12:47:53.227: INFO: (9) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 33.767926ms)
May 16 12:47:53.227: INFO: (9) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 33.514896ms)
May 16 12:47:53.228: INFO: (9) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 34.783237ms)
May 16 12:47:53.228: INFO: (9) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 34.178046ms)
May 16 12:47:53.229: INFO: (9) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 34.891445ms)
May 16 12:47:53.230: INFO: (9) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 35.899867ms)
May 16 12:47:53.232: INFO: (9) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 38.247195ms)
May 16 12:47:53.232: INFO: (9) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 38.479071ms)
May 16 12:47:53.232: INFO: (9) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 38.474395ms)
May 16 12:47:53.232: INFO: (9) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 38.250536ms)
May 16 12:47:53.232: INFO: (9) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 38.430489ms)
May 16 12:47:53.232: INFO: (9) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 38.740935ms)
May 16 12:47:53.235: INFO: (9) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 41.298162ms)
May 16 12:47:53.235: INFO: (9) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 40.755289ms)
May 16 12:47:53.235: INFO: (9) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 41.226049ms)
May 16 12:47:53.235: INFO: (9) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 41.877821ms)
May 16 12:47:53.260: INFO: (10) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 24.290572ms)
May 16 12:47:53.260: INFO: (10) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 24.069843ms)
May 16 12:47:53.260: INFO: (10) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 24.213271ms)
May 16 12:47:53.260: INFO: (10) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 24.951901ms)
May 16 12:47:53.260: INFO: (10) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 24.748938ms)
May 16 12:47:53.260: INFO: (10) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 24.716738ms)
May 16 12:47:53.261: INFO: (10) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 25.072792ms)
May 16 12:47:53.261: INFO: (10) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 25.240263ms)
May 16 12:47:53.261: INFO: (10) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 25.731435ms)
May 16 12:47:53.261: INFO: (10) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 25.671657ms)
May 16 12:47:53.262: INFO: (10) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 26.603051ms)
May 16 12:47:53.263: INFO: (10) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 27.505169ms)
May 16 12:47:53.265: INFO: (10) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 29.259813ms)
May 16 12:47:53.265: INFO: (10) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 30.082886ms)
May 16 12:47:53.266: INFO: (10) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 30.357479ms)
May 16 12:47:53.267: INFO: (10) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 30.833615ms)
May 16 12:47:53.293: INFO: (11) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 25.559862ms)
May 16 12:47:53.293: INFO: (11) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 25.938885ms)
May 16 12:47:53.293: INFO: (11) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 25.156692ms)
May 16 12:47:53.293: INFO: (11) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 26.068107ms)
May 16 12:47:53.293: INFO: (11) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 26.289205ms)
May 16 12:47:53.294: INFO: (11) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 26.207616ms)
May 16 12:47:53.294: INFO: (11) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 26.797318ms)
May 16 12:47:53.294: INFO: (11) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 27.069347ms)
May 16 12:47:53.295: INFO: (11) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 27.022478ms)
May 16 12:47:53.295: INFO: (11) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 26.969801ms)
May 16 12:47:53.295: INFO: (11) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 27.447888ms)
May 16 12:47:53.295: INFO: (11) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 27.214483ms)
May 16 12:47:53.297: INFO: (11) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 29.389065ms)
May 16 12:47:53.302: INFO: (11) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 34.673866ms)
May 16 12:47:53.303: INFO: (11) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 35.052089ms)
May 16 12:47:53.303: INFO: (11) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 35.554703ms)
May 16 12:47:53.320: INFO: (12) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 16.562088ms)
May 16 12:47:53.321: INFO: (12) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 17.566176ms)
May 16 12:47:53.321: INFO: (12) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 17.440455ms)
May 16 12:47:53.322: INFO: (12) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 18.410002ms)
May 16 12:47:53.324: INFO: (12) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 20.338382ms)
May 16 12:47:53.324: INFO: (12) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 20.619075ms)
May 16 12:47:53.326: INFO: (12) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 22.185178ms)
May 16 12:47:53.326: INFO: (12) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 22.466814ms)
May 16 12:47:53.327: INFO: (12) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 22.951192ms)
May 16 12:47:53.327: INFO: (12) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 23.264708ms)
May 16 12:47:53.327: INFO: (12) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 23.50622ms)
May 16 12:47:53.327: INFO: (12) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 23.686794ms)
May 16 12:47:53.329: INFO: (12) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 25.363202ms)
May 16 12:47:53.335: INFO: (12) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 31.491087ms)
May 16 12:47:53.335: INFO: (12) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 31.704776ms)
May 16 12:47:53.335: INFO: (12) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 31.865419ms)
May 16 12:47:53.353: INFO: (13) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 16.906ms)
May 16 12:47:53.354: INFO: (13) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 17.902586ms)
May 16 12:47:53.354: INFO: (13) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 18.791033ms)
May 16 12:47:53.355: INFO: (13) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 18.738989ms)
May 16 12:47:53.355: INFO: (13) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 19.249523ms)
May 16 12:47:53.355: INFO: (13) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 18.703942ms)
May 16 12:47:53.355: INFO: (13) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 19.508749ms)
May 16 12:47:53.355: INFO: (13) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 19.416087ms)
May 16 12:47:53.355: INFO: (13) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 19.897827ms)
May 16 12:47:53.356: INFO: (13) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 20.510738ms)
May 16 12:47:53.356: INFO: (13) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 20.564775ms)
May 16 12:47:53.357: INFO: (13) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 21.131581ms)
May 16 12:47:53.357: INFO: (13) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 21.093494ms)
May 16 12:47:53.359: INFO: (13) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 23.118019ms)
May 16 12:47:53.359: INFO: (13) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 23.070928ms)
May 16 12:47:53.359: INFO: (13) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 23.446936ms)
May 16 12:47:53.375: INFO: (14) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 15.193081ms)
May 16 12:47:53.375: INFO: (14) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 15.864575ms)
May 16 12:47:53.375: INFO: (14) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 15.394182ms)
May 16 12:47:53.376: INFO: (14) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 15.543079ms)
May 16 12:47:53.376: INFO: (14) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 16.534453ms)
May 16 12:47:53.377: INFO: (14) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 17.293541ms)
May 16 12:47:53.378: INFO: (14) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 18.168629ms)
May 16 12:47:53.378: INFO: (14) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 18.18537ms)
May 16 12:47:53.379: INFO: (14) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 18.36156ms)
May 16 12:47:53.381: INFO: (14) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 20.724014ms)
May 16 12:47:53.382: INFO: (14) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 22.048014ms)
May 16 12:47:53.388: INFO: (14) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 28.062028ms)
May 16 12:47:53.388: INFO: (14) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 28.497823ms)
May 16 12:47:53.388: INFO: (14) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 28.022309ms)
May 16 12:47:53.420: INFO: (14) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 60.317315ms)
May 16 12:47:53.420: INFO: (14) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 60.185564ms)
May 16 12:47:53.448: INFO: (15) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 27.039806ms)
May 16 12:47:53.448: INFO: (15) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 27.329466ms)
May 16 12:47:53.448: INFO: (15) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 27.076615ms)
May 16 12:47:53.448: INFO: (15) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 27.239787ms)
May 16 12:47:53.448: INFO: (15) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 27.588023ms)
May 16 12:47:53.452: INFO: (15) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 31.366461ms)
May 16 12:47:53.452: INFO: (15) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 31.321396ms)
May 16 12:47:53.453: INFO: (15) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 31.615192ms)
May 16 12:47:53.453: INFO: (15) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 31.835782ms)
May 16 12:47:53.453: INFO: (15) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 31.599386ms)
May 16 12:47:53.453: INFO: (15) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 32.199878ms)
May 16 12:47:53.453: INFO: (15) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 32.330606ms)
May 16 12:47:53.453: INFO: (15) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 32.521178ms)
May 16 12:47:53.454: INFO: (15) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 32.978888ms)
May 16 12:47:53.457: INFO: (15) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 35.835143ms)
May 16 12:47:53.458: INFO: (15) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 37.117274ms)
May 16 12:47:53.490: INFO: (16) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 31.802846ms)
May 16 12:47:53.490: INFO: (16) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 31.355054ms)
May 16 12:47:53.490: INFO: (16) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 31.689622ms)
May 16 12:47:53.491: INFO: (16) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 32.228595ms)
May 16 12:47:53.491: INFO: (16) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 32.411945ms)
May 16 12:47:53.491: INFO: (16) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 32.370897ms)
May 16 12:47:53.491: INFO: (16) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 32.636266ms)
May 16 12:47:53.491: INFO: (16) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 33.18051ms)
May 16 12:47:53.492: INFO: (16) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 32.69107ms)
May 16 12:47:53.492: INFO: (16) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 32.94736ms)
May 16 12:47:53.567: INFO: (16) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 108.513324ms)
May 16 12:47:53.567: INFO: (16) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 108.23026ms)
May 16 12:47:53.567: INFO: (16) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 108.609852ms)
May 16 12:47:53.567: INFO: (16) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 108.838723ms)
May 16 12:47:53.568: INFO: (16) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 108.96968ms)
May 16 12:47:53.569: INFO: (16) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 110.081454ms)
May 16 12:47:53.580: INFO: (17) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 11.474422ms)
May 16 12:47:53.580: INFO: (17) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 11.072128ms)
May 16 12:47:53.582: INFO: (17) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 12.157082ms)
May 16 12:47:53.582: INFO: (17) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 12.882918ms)
May 16 12:47:53.582: INFO: (17) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 13.065556ms)
May 16 12:47:53.583: INFO: (17) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 14.159121ms)
May 16 12:47:53.583: INFO: (17) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 13.791385ms)
May 16 12:47:53.584: INFO: (17) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 14.27798ms)
May 16 12:47:53.584: INFO: (17) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 14.854601ms)
May 16 12:47:53.586: INFO: (17) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 17.642482ms)
May 16 12:47:53.588: INFO: (17) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 19.455594ms)
May 16 12:47:53.589: INFO: (17) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 19.48154ms)
May 16 12:47:53.632: INFO: (17) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 62.149616ms)
May 16 12:47:53.632: INFO: (17) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 62.583358ms)
May 16 12:47:53.632: INFO: (17) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 62.974381ms)
May 16 12:47:53.632: INFO: (17) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 63.182072ms)
May 16 12:47:53.653: INFO: (18) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 19.830113ms)
May 16 12:47:53.653: INFO: (18) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 19.630412ms)
May 16 12:47:53.653: INFO: (18) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 19.930766ms)
May 16 12:47:53.653: INFO: (18) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 20.174204ms)
May 16 12:47:53.654: INFO: (18) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 21.423727ms)
May 16 12:47:53.653: INFO: (18) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 20.122542ms)
May 16 12:47:53.656: INFO: (18) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 23.4781ms)
May 16 12:47:53.656: INFO: (18) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 23.804144ms)
May 16 12:47:53.658: INFO: (18) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 25.344807ms)
May 16 12:47:53.658: INFO: (18) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 25.174382ms)
May 16 12:47:53.658: INFO: (18) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 25.088816ms)
May 16 12:47:53.658: INFO: (18) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 25.576963ms)
May 16 12:47:53.658: INFO: (18) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 25.058978ms)
May 16 12:47:53.658: INFO: (18) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 25.015456ms)
May 16 12:47:53.658: INFO: (18) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 25.437874ms)
May 16 12:47:53.659: INFO: (18) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 25.782729ms)
May 16 12:47:53.773: INFO: (19) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 113.923164ms)
May 16 12:47:53.773: INFO: (19) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:443/proxy/tlsrewritem... (200; 114.230446ms)
May 16 12:47:53.774: INFO: (19) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">test<... (200; 114.528989ms)
May 16 12:47:53.775: INFO: (19) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 115.037504ms)
May 16 12:47:53.775: INFO: (19) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:162/proxy/: bar (200; 115.290667ms)
May 16 12:47:53.775: INFO: (19) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:460/proxy/: tls baz (200; 115.811278ms)
May 16 12:47:53.775: INFO: (19) /api/v1/namespaces/proxy-4699/pods/https:proxy-service-f5g4j-zcjcx:462/proxy/: tls qux (200; 115.678058ms)
May 16 12:47:53.776: INFO: (19) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx/proxy/rewriteme">test</a> (200; 116.847106ms)
May 16 12:47:53.776: INFO: (19) /api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/: <a href="/api/v1/namespaces/proxy-4699/pods/http:proxy-service-f5g4j-zcjcx:1080/proxy/rewriteme">... (200; 116.126002ms)
May 16 12:47:53.776: INFO: (19) /api/v1/namespaces/proxy-4699/pods/proxy-service-f5g4j-zcjcx:160/proxy/: foo (200; 116.147615ms)
May 16 12:47:53.776: INFO: (19) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname1/proxy/: tls baz (200; 116.655987ms)
May 16 12:47:53.776: INFO: (19) /api/v1/namespaces/proxy-4699/services/https:proxy-service-f5g4j:tlsportname2/proxy/: tls qux (200; 116.511857ms)
May 16 12:47:53.823: INFO: (19) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname1/proxy/: foo (200; 162.839265ms)
May 16 12:47:53.823: INFO: (19) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname2/proxy/: bar (200; 164.195047ms)
May 16 12:47:53.823: INFO: (19) /api/v1/namespaces/proxy-4699/services/proxy-service-f5g4j:portname2/proxy/: bar (200; 164.357145ms)
May 16 12:47:53.823: INFO: (19) /api/v1/namespaces/proxy-4699/services/http:proxy-service-f5g4j:portname1/proxy/: foo (200; 163.737336ms)
STEP: deleting ReplicationController proxy-service-f5g4j in namespace proxy-4699, will wait for the garbage collector to delete the pods
May 16 12:47:53.896: INFO: Deleting ReplicationController proxy-service-f5g4j took: 16.330009ms
May 16 12:47:54.396: INFO: Terminating ReplicationController proxy-service-f5g4j pods took: 500.451998ms
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:47:57.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4699" for this suite.
May 16 12:48:03.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:48:03.740: INFO: namespace proxy-4699 deletion completed in 6.334806248s

• [SLOW TEST:20.335 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:48:03.741: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:48:07.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3209" for this suite.
May 16 12:48:13.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:48:14.303: INFO: namespace kubelet-test-3209 deletion completed in 6.427944895s

• [SLOW TEST:10.563 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:48:14.311: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 16 12:48:14.452: INFO: Waiting up to 5m0s for pod "pod-deb1b259-77d8-11e9-8275-42209be91bd2" in namespace "emptydir-9485" to be "success or failure"
May 16 12:48:14.460: INFO: Pod "pod-deb1b259-77d8-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.76597ms
May 16 12:48:16.485: INFO: Pod "pod-deb1b259-77d8-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032460231s
May 16 12:48:18.499: INFO: Pod "pod-deb1b259-77d8-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045927052s
STEP: Saw pod success
May 16 12:48:18.499: INFO: Pod "pod-deb1b259-77d8-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:48:18.552: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-deb1b259-77d8-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 12:48:18.657: INFO: Waiting for pod pod-deb1b259-77d8-11e9-8275-42209be91bd2 to disappear
May 16 12:48:18.669: INFO: Pod pod-deb1b259-77d8-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:48:18.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9485" for this suite.
May 16 12:48:24.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:48:25.065: INFO: namespace emptydir-9485 deletion completed in 6.389978844s

• [SLOW TEST:10.755 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:48:25.069: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 16 12:48:33.408: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 16 12:48:33.415: INFO: Pod pod-with-poststart-http-hook still exists
May 16 12:48:35.415: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 16 12:48:35.423: INFO: Pod pod-with-poststart-http-hook still exists
May 16 12:48:37.415: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 16 12:48:37.427: INFO: Pod pod-with-poststart-http-hook still exists
May 16 12:48:39.415: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 16 12:48:39.424: INFO: Pod pod-with-poststart-http-hook still exists
May 16 12:48:41.415: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 16 12:48:41.422: INFO: Pod pod-with-poststart-http-hook still exists
May 16 12:48:43.415: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 16 12:48:43.423: INFO: Pod pod-with-poststart-http-hook still exists
May 16 12:48:45.415: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 16 12:48:45.424: INFO: Pod pod-with-poststart-http-hook still exists
May 16 12:48:47.415: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 16 12:48:47.422: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:48:47.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9664" for this suite.
May 16 12:49:11.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:49:11.875: INFO: namespace container-lifecycle-hook-9664 deletion completed in 24.445709234s

• [SLOW TEST:46.807 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:49:11.878: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 16 12:49:12.134: INFO: Number of nodes with available pods: 0
May 16 12:49:12.134: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:49:13.146: INFO: Number of nodes with available pods: 0
May 16 12:49:13.147: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:49:14.185: INFO: Number of nodes with available pods: 0
May 16 12:49:14.185: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:49:15.148: INFO: Number of nodes with available pods: 0
May 16 12:49:15.148: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:49:16.152: INFO: Number of nodes with available pods: 1
May 16 12:49:16.152: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u is running more than one daemon pod
May 16 12:49:17.146: INFO: Number of nodes with available pods: 2
May 16 12:49:17.146: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 16 12:49:17.232: INFO: Number of nodes with available pods: 1
May 16 12:49:17.232: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:49:18.248: INFO: Number of nodes with available pods: 1
May 16 12:49:18.248: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:49:19.256: INFO: Number of nodes with available pods: 1
May 16 12:49:19.256: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:49:20.262: INFO: Number of nodes with available pods: 2
May 16 12:49:20.262: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3472, will wait for the garbage collector to delete the pods
May 16 12:49:20.357: INFO: Deleting DaemonSet.extensions daemon-set took: 30.163482ms
May 16 12:49:20.957: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.210589ms
May 16 12:49:27.065: INFO: Number of nodes with available pods: 0
May 16 12:49:27.066: INFO: Number of running nodes: 0, number of available pods: 0
May 16 12:49:27.071: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3472/daemonsets","resourceVersion":"20216"},"items":null}

May 16 12:49:27.076: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3472/pods","resourceVersion":"20216"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:49:27.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3472" for this suite.
May 16 12:49:35.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:49:35.516: INFO: namespace daemonsets-3472 deletion completed in 8.410265463s

• [SLOW TEST:23.639 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:49:35.521: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-z6r2
STEP: Creating a pod to test atomic-volume-subpath
May 16 12:49:35.664: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-z6r2" in namespace "subpath-1499" to be "success or failure"
May 16 12:49:35.671: INFO: Pod "pod-subpath-test-projected-z6r2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.09365ms
May 16 12:49:37.681: INFO: Pod "pod-subpath-test-projected-z6r2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017272312s
May 16 12:49:39.690: INFO: Pod "pod-subpath-test-projected-z6r2": Phase="Running", Reason="", readiness=true. Elapsed: 4.025775747s
May 16 12:49:41.698: INFO: Pod "pod-subpath-test-projected-z6r2": Phase="Running", Reason="", readiness=true. Elapsed: 6.033593708s
May 16 12:49:43.705: INFO: Pod "pod-subpath-test-projected-z6r2": Phase="Running", Reason="", readiness=true. Elapsed: 8.041242057s
May 16 12:49:45.716: INFO: Pod "pod-subpath-test-projected-z6r2": Phase="Running", Reason="", readiness=true. Elapsed: 10.051883451s
May 16 12:49:47.725: INFO: Pod "pod-subpath-test-projected-z6r2": Phase="Running", Reason="", readiness=true. Elapsed: 12.061186005s
May 16 12:49:49.732: INFO: Pod "pod-subpath-test-projected-z6r2": Phase="Running", Reason="", readiness=true. Elapsed: 14.068390414s
May 16 12:49:51.816: INFO: Pod "pod-subpath-test-projected-z6r2": Phase="Running", Reason="", readiness=true. Elapsed: 16.152324366s
May 16 12:49:53.824: INFO: Pod "pod-subpath-test-projected-z6r2": Phase="Running", Reason="", readiness=true. Elapsed: 18.160073306s
May 16 12:49:55.831: INFO: Pod "pod-subpath-test-projected-z6r2": Phase="Running", Reason="", readiness=true. Elapsed: 20.167261071s
May 16 12:49:57.839: INFO: Pod "pod-subpath-test-projected-z6r2": Phase="Running", Reason="", readiness=true. Elapsed: 22.174469729s
May 16 12:49:59.846: INFO: Pod "pod-subpath-test-projected-z6r2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.181494862s
STEP: Saw pod success
May 16 12:49:59.846: INFO: Pod "pod-subpath-test-projected-z6r2" satisfied condition "success or failure"
May 16 12:49:59.853: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-subpath-test-projected-z6r2 container test-container-subpath-projected-z6r2: <nil>
STEP: delete the pod
May 16 12:49:59.999: INFO: Waiting for pod pod-subpath-test-projected-z6r2 to disappear
May 16 12:50:00.007: INFO: Pod pod-subpath-test-projected-z6r2 no longer exists
STEP: Deleting pod pod-subpath-test-projected-z6r2
May 16 12:50:00.007: INFO: Deleting pod "pod-subpath-test-projected-z6r2" in namespace "subpath-1499"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:50:00.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1499" for this suite.
May 16 12:50:06.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:50:06.435: INFO: namespace subpath-1499 deletion completed in 6.417599047s

• [SLOW TEST:30.914 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:50:06.435: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
May 16 12:50:06.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 cluster-info'
May 16 12:50:08.279: INFO: stderr: ""
May 16 12:50:08.279: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.10.10.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.10.10.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://10.10.10.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:50:08.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5544" for this suite.
May 16 12:50:14.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:50:14.723: INFO: namespace kubectl-5544 deletion completed in 6.437601771s

• [SLOW TEST:8.288 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:50:14.723: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-267b2353-77d9-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 12:50:14.913: INFO: Waiting up to 5m0s for pod "pod-configmaps-267de7ce-77d9-11e9-8275-42209be91bd2" in namespace "configmap-7728" to be "success or failure"
May 16 12:50:14.919: INFO: Pod "pod-configmaps-267de7ce-77d9-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.501596ms
May 16 12:50:16.927: INFO: Pod "pod-configmaps-267de7ce-77d9-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014117705s
May 16 12:50:18.934: INFO: Pod "pod-configmaps-267de7ce-77d9-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021215497s
May 16 12:50:20.941: INFO: Pod "pod-configmaps-267de7ce-77d9-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027877342s
STEP: Saw pod success
May 16 12:50:20.941: INFO: Pod "pod-configmaps-267de7ce-77d9-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:50:20.947: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-configmaps-267de7ce-77d9-11e9-8275-42209be91bd2 container configmap-volume-test: <nil>
STEP: delete the pod
May 16 12:50:21.032: INFO: Waiting for pod pod-configmaps-267de7ce-77d9-11e9-8275-42209be91bd2 to disappear
May 16 12:50:21.051: INFO: Pod pod-configmaps-267de7ce-77d9-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:50:21.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7728" for this suite.
May 16 12:50:27.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:50:27.399: INFO: namespace configmap-7728 deletion completed in 6.340714394s

• [SLOW TEST:12.676 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:50:27.401: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 16 12:50:27.534: INFO: Waiting up to 5m0s for pod "pod-2e0238ce-77d9-11e9-8275-42209be91bd2" in namespace "emptydir-2597" to be "success or failure"
May 16 12:50:27.541: INFO: Pod "pod-2e0238ce-77d9-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.191794ms
May 16 12:50:29.551: INFO: Pod "pod-2e0238ce-77d9-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016835781s
May 16 12:50:31.560: INFO: Pod "pod-2e0238ce-77d9-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026133764s
STEP: Saw pod success
May 16 12:50:31.561: INFO: Pod "pod-2e0238ce-77d9-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:50:31.566: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-2e0238ce-77d9-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 12:50:31.637: INFO: Waiting for pod pod-2e0238ce-77d9-11e9-8275-42209be91bd2 to disappear
May 16 12:50:31.643: INFO: Pod pod-2e0238ce-77d9-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:50:31.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2597" for this suite.
May 16 12:50:37.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:50:38.197: INFO: namespace emptydir-2597 deletion completed in 6.546126696s

• [SLOW TEST:10.797 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:50:38.197: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 12:50:38.344: INFO: Create a RollingUpdate DaemonSet
May 16 12:50:38.363: INFO: Check that daemon pods launch on every node of the cluster
May 16 12:50:38.375: INFO: Number of nodes with available pods: 0
May 16 12:50:38.375: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:50:39.391: INFO: Number of nodes with available pods: 0
May 16 12:50:39.391: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:50:40.393: INFO: Number of nodes with available pods: 0
May 16 12:50:40.393: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:50:41.390: INFO: Number of nodes with available pods: 1
May 16 12:50:41.390: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 12:50:42.389: INFO: Number of nodes with available pods: 2
May 16 12:50:42.389: INFO: Number of running nodes: 2, number of available pods: 2
May 16 12:50:42.389: INFO: Update the DaemonSet to trigger a rollout
May 16 12:50:42.427: INFO: Updating DaemonSet daemon-set
May 16 12:50:47.463: INFO: Roll back the DaemonSet before rollout is complete
May 16 12:50:47.477: INFO: Updating DaemonSet daemon-set
May 16 12:50:47.477: INFO: Make sure DaemonSet rollback is complete
May 16 12:50:47.488: INFO: Wrong image for pod: daemon-set-2g2b2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 16 12:50:47.488: INFO: Pod daemon-set-2g2b2 is not available
May 16 12:50:48.523: INFO: Wrong image for pod: daemon-set-2g2b2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 16 12:50:48.524: INFO: Pod daemon-set-2g2b2 is not available
May 16 12:50:49.524: INFO: Pod daemon-set-dvzkz is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1133, will wait for the garbage collector to delete the pods
May 16 12:50:49.619: INFO: Deleting DaemonSet.extensions daemon-set took: 23.547442ms
May 16 12:50:50.120: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.348957ms
May 16 12:50:53.226: INFO: Number of nodes with available pods: 0
May 16 12:50:53.226: INFO: Number of running nodes: 0, number of available pods: 0
May 16 12:50:53.242: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1133/daemonsets","resourceVersion":"20654"},"items":null}

May 16 12:50:53.247: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1133/pods","resourceVersion":"20654"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:50:53.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1133" for this suite.
May 16 12:50:59.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:50:59.692: INFO: namespace daemonsets-1133 deletion completed in 6.419889838s

• [SLOW TEST:21.495 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:50:59.694: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-6801
May 16 12:51:03.868: INFO: Started pod liveness-http in namespace container-probe-6801
STEP: checking the pod's current state and verifying that restartCount is present
May 16 12:51:03.874: INFO: Initial restart count of pod liveness-http is 0
May 16 12:51:23.962: INFO: Restart count of pod container-probe-6801/liveness-http is now 1 (20.087677034s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:51:24.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6801" for this suite.
May 16 12:51:30.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:51:30.467: INFO: namespace container-probe-6801 deletion completed in 6.440023287s

• [SLOW TEST:30.774 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:51:30.468: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 12:51:30.618: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:51:31.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2139" for this suite.
May 16 12:51:37.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:51:38.110: INFO: namespace custom-resource-definition-2139 deletion completed in 6.33739186s

• [SLOW TEST:7.643 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:51:38.111: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 16 12:51:38.327: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9525,SelfLink:/api/v1/namespaces/watch-9525/configmaps/e2e-watch-test-resource-version,UID:58215c23-77d9-11e9-b44f-0a580af42602,ResourceVersion:20880,Generation:0,CreationTimestamp:2019-05-16 12:51:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 16 12:51:38.327: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9525,SelfLink:/api/v1/namespaces/watch-9525/configmaps/e2e-watch-test-resource-version,UID:58215c23-77d9-11e9-b44f-0a580af42602,ResourceVersion:20881,Generation:0,CreationTimestamp:2019-05-16 12:51:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:51:38.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9525" for this suite.
May 16 12:51:46.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:51:46.984: INFO: namespace watch-9525 deletion completed in 8.647876275s

• [SLOW TEST:8.876 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:51:46.988: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 16 12:51:47.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-774'
May 16 12:51:47.498: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 16 12:51:47.498: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
May 16 12:51:47.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete jobs e2e-test-nginx-job --namespace=kubectl-774'
May 16 12:51:47.927: INFO: stderr: ""
May 16 12:51:47.927: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:51:47.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-774" for this suite.
May 16 12:51:53.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:51:54.347: INFO: namespace kubectl-774 deletion completed in 6.411403293s

• [SLOW TEST:7.359 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:51:54.349: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-61db32f6-77d9-11e9-8275-42209be91bd2
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:52:00.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3452" for this suite.
May 16 12:52:24.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:52:25.100: INFO: namespace configmap-3452 deletion completed in 24.428809066s

• [SLOW TEST:30.752 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:52:25.103: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 12:52:25.270: INFO: Waiting up to 5m0s for pod "downwardapi-volume-742e6623-77d9-11e9-8275-42209be91bd2" in namespace "projected-1662" to be "success or failure"
May 16 12:52:25.276: INFO: Pod "downwardapi-volume-742e6623-77d9-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.146506ms
May 16 12:52:27.285: INFO: Pod "downwardapi-volume-742e6623-77d9-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014556194s
May 16 12:52:29.293: INFO: Pod "downwardapi-volume-742e6623-77d9-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022745386s
STEP: Saw pod success
May 16 12:52:29.293: INFO: Pod "downwardapi-volume-742e6623-77d9-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:52:29.309: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-742e6623-77d9-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 12:52:29.514: INFO: Waiting for pod downwardapi-volume-742e6623-77d9-11e9-8275-42209be91bd2 to disappear
May 16 12:52:29.520: INFO: Pod downwardapi-volume-742e6623-77d9-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:52:29.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1662" for this suite.
May 16 12:52:35.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:52:36.041: INFO: namespace projected-1662 deletion completed in 6.512560059s

• [SLOW TEST:10.939 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:52:36.044: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:52:36.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1061" for this suite.
May 16 12:53:00.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:53:00.623: INFO: namespace pods-1061 deletion completed in 24.392313124s

• [SLOW TEST:24.579 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:53:00.623: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-xdlk
STEP: Creating a pod to test atomic-volume-subpath
May 16 12:53:00.801: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xdlk" in namespace "subpath-1134" to be "success or failure"
May 16 12:53:00.809: INFO: Pod "pod-subpath-test-configmap-xdlk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.358139ms
May 16 12:53:02.818: INFO: Pod "pod-subpath-test-configmap-xdlk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016951353s
May 16 12:53:04.826: INFO: Pod "pod-subpath-test-configmap-xdlk": Phase="Running", Reason="", readiness=true. Elapsed: 4.02457836s
May 16 12:53:06.833: INFO: Pod "pod-subpath-test-configmap-xdlk": Phase="Running", Reason="", readiness=true. Elapsed: 6.031624404s
May 16 12:53:08.843: INFO: Pod "pod-subpath-test-configmap-xdlk": Phase="Running", Reason="", readiness=true. Elapsed: 8.04156807s
May 16 12:53:10.851: INFO: Pod "pod-subpath-test-configmap-xdlk": Phase="Running", Reason="", readiness=true. Elapsed: 10.049656504s
May 16 12:53:12.862: INFO: Pod "pod-subpath-test-configmap-xdlk": Phase="Running", Reason="", readiness=true. Elapsed: 12.060369107s
May 16 12:53:14.870: INFO: Pod "pod-subpath-test-configmap-xdlk": Phase="Running", Reason="", readiness=true. Elapsed: 14.068988805s
May 16 12:53:16.879: INFO: Pod "pod-subpath-test-configmap-xdlk": Phase="Running", Reason="", readiness=true. Elapsed: 16.077089732s
May 16 12:53:18.897: INFO: Pod "pod-subpath-test-configmap-xdlk": Phase="Running", Reason="", readiness=true. Elapsed: 18.095295846s
May 16 12:53:20.911: INFO: Pod "pod-subpath-test-configmap-xdlk": Phase="Running", Reason="", readiness=true. Elapsed: 20.109636423s
May 16 12:53:22.921: INFO: Pod "pod-subpath-test-configmap-xdlk": Phase="Running", Reason="", readiness=true. Elapsed: 22.119579575s
May 16 12:53:24.929: INFO: Pod "pod-subpath-test-configmap-xdlk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.127371666s
STEP: Saw pod success
May 16 12:53:24.929: INFO: Pod "pod-subpath-test-configmap-xdlk" satisfied condition "success or failure"
May 16 12:53:24.936: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-subpath-test-configmap-xdlk container test-container-subpath-configmap-xdlk: <nil>
STEP: delete the pod
May 16 12:53:25.028: INFO: Waiting for pod pod-subpath-test-configmap-xdlk to disappear
May 16 12:53:25.050: INFO: Pod pod-subpath-test-configmap-xdlk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xdlk
May 16 12:53:25.050: INFO: Deleting pod "pod-subpath-test-configmap-xdlk" in namespace "subpath-1134"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:53:25.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1134" for this suite.
May 16 12:53:31.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:53:31.502: INFO: namespace subpath-1134 deletion completed in 6.42248585s

• [SLOW TEST:30.880 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:53:31.504: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 16 12:53:31.654: INFO: Waiting up to 5m0s for pod "downward-api-9bc1fa99-77d9-11e9-8275-42209be91bd2" in namespace "downward-api-5439" to be "success or failure"
May 16 12:53:31.664: INFO: Pod "downward-api-9bc1fa99-77d9-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.925209ms
May 16 12:53:33.690: INFO: Pod "downward-api-9bc1fa99-77d9-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03634935s
May 16 12:53:35.697: INFO: Pod "downward-api-9bc1fa99-77d9-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042809361s
STEP: Saw pod success
May 16 12:53:35.697: INFO: Pod "downward-api-9bc1fa99-77d9-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:53:35.706: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downward-api-9bc1fa99-77d9-11e9-8275-42209be91bd2 container dapi-container: <nil>
STEP: delete the pod
May 16 12:53:35.923: INFO: Waiting for pod downward-api-9bc1fa99-77d9-11e9-8275-42209be91bd2 to disappear
May 16 12:53:35.930: INFO: Pod downward-api-9bc1fa99-77d9-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:53:35.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5439" for this suite.
May 16 12:53:41.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:53:42.528: INFO: namespace downward-api-5439 deletion completed in 6.584366095s

• [SLOW TEST:11.024 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:53:42.528: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 12:53:42.642: INFO: Creating deployment "nginx-deployment"
May 16 12:53:42.661: INFO: Waiting for observed generation 1
May 16 12:53:44.684: INFO: Waiting for all required pods to come up
May 16 12:53:44.699: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
May 16 12:53:50.724: INFO: Waiting for deployment "nginx-deployment" to complete
May 16 12:53:50.739: INFO: Updating deployment "nginx-deployment" with a non-existent image
May 16 12:53:50.758: INFO: Updating deployment nginx-deployment
May 16 12:53:50.758: INFO: Waiting for observed generation 2
May 16 12:53:52.771: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 16 12:53:52.776: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 16 12:53:52.782: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 16 12:53:52.819: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 16 12:53:52.819: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 16 12:53:52.825: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 16 12:53:52.873: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
May 16 12:53:52.873: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
May 16 12:53:52.899: INFO: Updating deployment nginx-deployment
May 16 12:53:52.899: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
May 16 12:53:52.926: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 16 12:53:52.990: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 16 12:53:53.083: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-7808,SelfLink:/apis/apps/v1/namespaces/deployment-7808/deployments/nginx-deployment,UID:a24829d1-77d9-11e9-b44f-0a580af42602,ResourceVersion:21681,Generation:3,CreationTimestamp:2019-05-16 12:53:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-05-16 12:53:51 +0000 UTC 2019-05-16 12:53:42 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-05-16 12:53:52 +0000 UTC 2019-05-16 12:53:52 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

May 16 12:53:53.117: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-7808,SelfLink:/apis/apps/v1/namespaces/deployment-7808/replicasets/nginx-deployment-5f9595f595,UID:a71e9d3d-77d9-11e9-b44f-0a580af42602,ResourceVersion:21671,Generation:3,CreationTimestamp:2019-05-16 12:53:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a24829d1-77d9-11e9-b44f-0a580af42602 0xc00293ef27 0xc00293ef28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 16 12:53:53.117: INFO: All old ReplicaSets of Deployment "nginx-deployment":
May 16 12:53:53.117: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-7808,SelfLink:/apis/apps/v1/namespaces/deployment-7808/replicasets/nginx-deployment-6f478d8d8,UID:a24c05ba-77d9-11e9-b44f-0a580af42602,ResourceVersion:21668,Generation:3,CreationTimestamp:2019-05-16 12:53:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a24829d1-77d9-11e9-b44f-0a580af42602 0xc00293f027 0xc00293f028}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
May 16 12:53:53.162: INFO: Pod "nginx-deployment-5f9595f595-7gpv2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-7gpv2,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-5f9595f595-7gpv2,UID:a73bd7ac-77d9-11e9-b44f-0a580af42602,ResourceVersion:21658,Generation:0,CreationTimestamp:2019-05-16 12:53:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 a71e9d3d-77d9-11e9-b44f-0a580af42602 0xc00293ff97 0xc00293ff98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b4010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b4030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-05-16 12:53:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.162: INFO: Pod "nginx-deployment-5f9595f595-7lvzj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-7lvzj,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-5f9595f595-7lvzj,UID:a87b093d-77d9-11e9-b44f-0a580af42602,ResourceVersion:21705,Generation:0,CreationTimestamp:2019-05-16 12:53:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 a71e9d3d-77d9-11e9-b44f-0a580af42602 0xc0029b42d0 0xc0029b42d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b43a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b43c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.162: INFO: Pod "nginx-deployment-5f9595f595-cn4kl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-cn4kl,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-5f9595f595-cn4kl,UID:a8834fd7-77d9-11e9-b44f-0a580af42602,ResourceVersion:21699,Generation:0,CreationTimestamp:2019-05-16 12:53:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 a71e9d3d-77d9-11e9-b44f-0a580af42602 0xc0029b4470 0xc0029b4471}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b44f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b4510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.162: INFO: Pod "nginx-deployment-5f9595f595-jqhq9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-jqhq9,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-5f9595f595-jqhq9,UID:a883884f-77d9-11e9-b44f-0a580af42602,ResourceVersion:21700,Generation:0,CreationTimestamp:2019-05-16 12:53:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 a71e9d3d-77d9-11e9-b44f-0a580af42602 0xc0029b45a7 0xc0029b45a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b4620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b4640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.162: INFO: Pod "nginx-deployment-5f9595f595-k896x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-k896x,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-5f9595f595-k896x,UID:a8836fe5-77d9-11e9-b44f-0a580af42602,ResourceVersion:21698,Generation:0,CreationTimestamp:2019-05-16 12:53:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 a71e9d3d-77d9-11e9-b44f-0a580af42602 0xc0029b46a7 0xc0029b46a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b4710} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b4730}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.163: INFO: Pod "nginx-deployment-5f9595f595-kgrg9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-kgrg9,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-5f9595f595-kgrg9,UID:a883408f-77d9-11e9-b44f-0a580af42602,ResourceVersion:21697,Generation:0,CreationTimestamp:2019-05-16 12:53:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 a71e9d3d-77d9-11e9-b44f-0a580af42602 0xc0029b4797 0xc0029b4798}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b4800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b4820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.163: INFO: Pod "nginx-deployment-5f9595f595-kmr5n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-kmr5n,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-5f9595f595-kmr5n,UID:a87ac084-77d9-11e9-b44f-0a580af42602,ResourceVersion:21696,Generation:0,CreationTimestamp:2019-05-16 12:53:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 a71e9d3d-77d9-11e9-b44f-0a580af42602 0xc0029b4887 0xc0029b4888}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b48f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b4910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.163: INFO: Pod "nginx-deployment-5f9595f595-lpk59" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-lpk59,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-5f9595f595-lpk59,UID:a720fd25-77d9-11e9-b44f-0a580af42602,ResourceVersion:21665,Generation:0,CreationTimestamp:2019-05-16 12:53:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 a71e9d3d-77d9-11e9-b44f-0a580af42602 0xc0029b4990 0xc0029b4991}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b4a00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b4a20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.0.140,StartTime:2019-05-16 12:53:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.163: INFO: Pod "nginx-deployment-5f9595f595-r4m29" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-r4m29,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-5f9595f595-r4m29,UID:a726cfe6-77d9-11e9-b44f-0a580af42602,ResourceVersion:21652,Generation:0,CreationTimestamp:2019-05-16 12:53:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 a71e9d3d-77d9-11e9-b44f-0a580af42602 0xc0029b4b00 0xc0029b4b01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b4b70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b4b90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-05-16 12:53:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.163: INFO: Pod "nginx-deployment-5f9595f595-r9h46" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-r9h46,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-5f9595f595-r9h46,UID:a873510c-77d9-11e9-b44f-0a580af42602,ResourceVersion:21682,Generation:0,CreationTimestamp:2019-05-16 12:53:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 a71e9d3d-77d9-11e9-b44f-0a580af42602 0xc0029b4c70 0xc0029b4c71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b4ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b4d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.163: INFO: Pod "nginx-deployment-5f9595f595-rwdjz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-rwdjz,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-5f9595f595-rwdjz,UID:a7266ceb-77d9-11e9-b44f-0a580af42602,ResourceVersion:21635,Generation:0,CreationTimestamp:2019-05-16 12:53:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 a71e9d3d-77d9-11e9-b44f-0a580af42602 0xc0029b4d80 0xc0029b4d81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b4df0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b4e10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-05-16 12:53:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.164: INFO: Pod "nginx-deployment-5f9595f595-sl7sn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-sl7sn,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-5f9595f595-sl7sn,UID:a7438bf3-77d9-11e9-b44f-0a580af42602,ResourceVersion:21657,Generation:0,CreationTimestamp:2019-05-16 12:53:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 a71e9d3d-77d9-11e9-b44f-0a580af42602 0xc0029b4ee0 0xc0029b4ee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b4f50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b4f70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:50 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:,StartTime:2019-05-16 12:53:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.164: INFO: Pod "nginx-deployment-6f478d8d8-69sjs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-69sjs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-69sjs,UID:a87b1de8-77d9-11e9-b44f-0a580af42602,ResourceVersion:21706,Generation:0,CreationTimestamp:2019-05-16 12:53:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b5040 0xc0029b5041}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b50a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b50c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.164: INFO: Pod "nginx-deployment-6f478d8d8-8m29q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8m29q,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-8m29q,UID:a2683978-77d9-11e9-b44f-0a580af42602,ResourceVersion:21597,Generation:0,CreationTimestamp:2019-05-16 12:53:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b5140 0xc0029b5141}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b51a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b51c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.0.139,StartTime:2019-05-16 12:53:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-16 12:53:47 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://69ee95da4b69cc99c6fd27518669e659397c8726c43a9784649549a5ccd641be}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.164: INFO: Pod "nginx-deployment-6f478d8d8-8tp8s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8tp8s,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-8tp8s,UID:a882d8cc-77d9-11e9-b44f-0a580af42602,ResourceVersion:21710,Generation:0,CreationTimestamp:2019-05-16 12:53:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b5297 0xc0029b5298}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b5300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b5320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.164: INFO: Pod "nginx-deployment-6f478d8d8-9tt74" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9tt74,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-9tt74,UID:a8841fec-77d9-11e9-b44f-0a580af42602,ResourceVersion:21704,Generation:0,CreationTimestamp:2019-05-16 12:53:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b53a0 0xc0029b53a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b5400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b5420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.165: INFO: Pod "nginx-deployment-6f478d8d8-9x4zz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9x4zz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-9x4zz,UID:a873144e-77d9-11e9-b44f-0a580af42602,ResourceVersion:21679,Generation:0,CreationTimestamp:2019-05-16 12:53:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b5487 0xc0029b5488}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b54f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b5510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.165: INFO: Pod "nginx-deployment-6f478d8d8-9xw28" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9xw28,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-9xw28,UID:a872a1d6-77d9-11e9-b44f-0a580af42602,ResourceVersion:21678,Generation:0,CreationTimestamp:2019-05-16 12:53:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b5590 0xc0029b5591}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b55f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b5610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.165: INFO: Pod "nginx-deployment-6f478d8d8-cw7j6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-cw7j6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-cw7j6,UID:a2682c79-77d9-11e9-b44f-0a580af42602,ResourceVersion:21592,Generation:0,CreationTimestamp:2019-05-16 12:53:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b5690 0xc0029b5691}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b56f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b5710}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.0.138,StartTime:2019-05-16 12:53:42 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-16 12:53:47 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://10686a317b26901378f80ac7c11489b7eedb88a48bc8ba14ffaf1ea8821ac094}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.165: INFO: Pod "nginx-deployment-6f478d8d8-hdwjq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-hdwjq,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-hdwjq,UID:a87aeda7-77d9-11e9-b44f-0a580af42602,ResourceVersion:21702,Generation:0,CreationTimestamp:2019-05-16 12:53:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b57f7 0xc0029b57f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b5860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b5880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.166: INFO: Pod "nginx-deployment-6f478d8d8-j5mj5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-j5mj5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-j5mj5,UID:a268418d-77d9-11e9-b44f-0a580af42602,ResourceVersion:21588,Generation:0,CreationTimestamp:2019-05-16 12:53:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b5900 0xc0029b5901}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b5960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b5980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:172.25.1.55,StartTime:2019-05-16 12:53:42 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-16 12:53:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d821a7293f9cf59cbb06374924228b0da81143489e3467fc7eb488563262b107}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.166: INFO: Pod "nginx-deployment-6f478d8d8-j7lqb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-j7lqb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-j7lqb,UID:a25a88ef-77d9-11e9-b44f-0a580af42602,ResourceVersion:21574,Generation:0,CreationTimestamp:2019-05-16 12:53:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b5a50 0xc0029b5a51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b5ab0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b5ad0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.0.136,StartTime:2019-05-16 12:53:42 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-16 12:53:47 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4ad8a79a07c45d8f0c41cc0995b058cd483ac783155a324625cd16b586c9ebad}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.166: INFO: Pod "nginx-deployment-6f478d8d8-pbx9j" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-pbx9j,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-pbx9j,UID:a24ed65a-77d9-11e9-b44f-0a580af42602,ResourceVersion:21569,Generation:0,CreationTimestamp:2019-05-16 12:53:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b5ba7 0xc0029b5ba8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b5c10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b5c30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.0.134,StartTime:2019-05-16 12:53:42 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-16 12:53:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8288dae75e75f9a7cfa8bef8c9ae256d676c0439ea1d559593b68d8c7bce4480}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.166: INFO: Pod "nginx-deployment-6f478d8d8-plbr8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-plbr8,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-plbr8,UID:a87ae1af-77d9-11e9-b44f-0a580af42602,ResourceVersion:21703,Generation:0,CreationTimestamp:2019-05-16 12:53:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b5d07 0xc0029b5d08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b5d70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b5d90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.166: INFO: Pod "nginx-deployment-6f478d8d8-rc2tb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rc2tb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-rc2tb,UID:a25a5366-77d9-11e9-b44f-0a580af42602,ResourceVersion:21578,Generation:0,CreationTimestamp:2019-05-16 12:53:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b5e10 0xc0029b5e11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b5e70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b5e90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.0.137,StartTime:2019-05-16 12:53:42 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-16 12:53:47 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://71e508558bfb0673a70460d9226f159245bd65d31a1826875bc143d22f738489}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.167: INFO: Pod "nginx-deployment-6f478d8d8-rwnmb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rwnmb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-rwnmb,UID:a869805c-77d9-11e9-b44f-0a580af42602,ResourceVersion:21707,Generation:0,CreationTimestamp:2019-05-16 12:53:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc0029b5f67 0xc0029b5f68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029b5fd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029b5ff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:52 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:,StartTime:2019-05-16 12:53:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.167: INFO: Pod "nginx-deployment-6f478d8d8-sg9dw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-sg9dw,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-sg9dw,UID:a882b5ba-77d9-11e9-b44f-0a580af42602,ResourceVersion:21709,Generation:0,CreationTimestamp:2019-05-16 12:53:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc00273e0b7 0xc00273e0b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00273e120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00273e140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:53 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.167: INFO: Pod "nginx-deployment-6f478d8d8-sqspl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-sqspl,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-sqspl,UID:a882c32f-77d9-11e9-b44f-0a580af42602,ResourceVersion:21694,Generation:0,CreationTimestamp:2019-05-16 12:53:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc00273e1c0 0xc00273e1c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00273e220} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00273e240}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.167: INFO: Pod "nginx-deployment-6f478d8d8-tthh5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-tthh5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-tthh5,UID:a25448ab-77d9-11e9-b44f-0a580af42602,ResourceVersion:21585,Generation:0,CreationTimestamp:2019-05-16 12:53:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc00273e2a7 0xc00273e2a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00273e310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00273e330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.10,PodIP:172.25.1.56,StartTime:2019-05-16 12:53:42 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-16 12:53:47 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://dad30c1eb0db88d8d07420752b7cccd81127cbceaf753d27f0162fd3802162f3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.167: INFO: Pod "nginx-deployment-6f478d8d8-vpvww" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-vpvww,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-vpvww,UID:a882f0cb-77d9-11e9-b44f-0a580af42602,ResourceVersion:21695,Generation:0,CreationTimestamp:2019-05-16 12:53:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc00273e400 0xc00273e401}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00273e480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00273e4a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.168: INFO: Pod "nginx-deployment-6f478d8d8-x2jpn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-x2jpn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-x2jpn,UID:a87a7c56-77d9-11e9-b44f-0a580af42602,ResourceVersion:21690,Generation:0,CreationTimestamp:2019-05-16 12:53:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc00273e517 0xc00273e518}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00273e6b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00273e760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 16 12:53:53.168: INFO: Pod "nginx-deployment-6f478d8d8-zw87n" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zw87n,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7808,SelfLink:/api/v1/namespaces/deployment-7808/pods/nginx-deployment-6f478d8d8-zw87n,UID:a2545dae-77d9-11e9-b44f-0a580af42602,ResourceVersion:21563,Generation:0,CreationTimestamp:2019-05-16 12:53:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a24c05ba-77d9-11e9-b44f-0a580af42602 0xc00273e7e0 0xc00273e7e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cfpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cfpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00273e840} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00273e860}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 12:53:42 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.0.135,StartTime:2019-05-16 12:53:42 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-16 12:53:47 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://fe9f4bd2a492b01eaf620f1035b299d015556038cf705f4f8d759de07b9df664}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:53:53.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7808" for this suite.
May 16 12:54:03.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:54:03.679: INFO: namespace deployment-7808 deletion completed in 10.396301643s

• [SLOW TEST:21.151 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:54:03.679: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-aeefb2ef-77d9-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 12:54:03.847: INFO: Waiting up to 5m0s for pod "pod-secrets-aef0cc94-77d9-11e9-8275-42209be91bd2" in namespace "secrets-1132" to be "success or failure"
May 16 12:54:03.854: INFO: Pod "pod-secrets-aef0cc94-77d9-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.308141ms
May 16 12:54:05.860: INFO: Pod "pod-secrets-aef0cc94-77d9-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013157426s
May 16 12:54:07.868: INFO: Pod "pod-secrets-aef0cc94-77d9-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020738933s
May 16 12:54:09.884: INFO: Pod "pod-secrets-aef0cc94-77d9-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036892439s
STEP: Saw pod success
May 16 12:54:09.884: INFO: Pod "pod-secrets-aef0cc94-77d9-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:54:09.889: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-secrets-aef0cc94-77d9-11e9-8275-42209be91bd2 container secret-volume-test: <nil>
STEP: delete the pod
May 16 12:54:09.999: INFO: Waiting for pod pod-secrets-aef0cc94-77d9-11e9-8275-42209be91bd2 to disappear
May 16 12:54:10.006: INFO: Pod pod-secrets-aef0cc94-77d9-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:54:10.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1132" for this suite.
May 16 12:54:16.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:54:16.374: INFO: namespace secrets-1132 deletion completed in 6.361886999s

• [SLOW TEST:12.695 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:54:16.376: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0516 12:54:47.101248      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 16 12:54:47.101: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:54:47.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6057" for this suite.
May 16 12:54:55.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:54:55.476: INFO: namespace gc-6057 deletion completed in 8.36892529s

• [SLOW TEST:39.101 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:54:55.477: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-cdd87548-77d9-11e9-8275-42209be91bd2
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-cdd87548-77d9-11e9-8275-42209be91bd2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:56:29.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6840" for this suite.
May 16 12:56:53.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:56:54.107: INFO: namespace configmap-6840 deletion completed in 24.430678908s

• [SLOW TEST:118.630 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:56:54.107: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 16 12:56:54.276: INFO: Waiting up to 5m0s for pod "pod-1486e0df-77da-11e9-8275-42209be91bd2" in namespace "emptydir-6309" to be "success or failure"
May 16 12:56:54.286: INFO: Pod "pod-1486e0df-77da-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.672968ms
May 16 12:56:56.292: INFO: Pod "pod-1486e0df-77da-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016212975s
May 16 12:56:58.299: INFO: Pod "pod-1486e0df-77da-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023014369s
STEP: Saw pod success
May 16 12:56:58.299: INFO: Pod "pod-1486e0df-77da-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:56:58.307: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-1486e0df-77da-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 12:56:58.386: INFO: Waiting for pod pod-1486e0df-77da-11e9-8275-42209be91bd2 to disappear
May 16 12:56:58.391: INFO: Pod pod-1486e0df-77da-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:56:58.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6309" for this suite.
May 16 12:57:04.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:57:04.786: INFO: namespace emptydir-6309 deletion completed in 6.37800605s

• [SLOW TEST:10.679 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:57:04.788: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 12:57:04.956: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ae5e5a8-77da-11e9-8275-42209be91bd2" in namespace "downward-api-6812" to be "success or failure"
May 16 12:57:04.966: INFO: Pod "downwardapi-volume-1ae5e5a8-77da-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.084211ms
May 16 12:57:06.974: INFO: Pod "downwardapi-volume-1ae5e5a8-77da-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017672152s
May 16 12:57:08.984: INFO: Pod "downwardapi-volume-1ae5e5a8-77da-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027873278s
STEP: Saw pod success
May 16 12:57:08.984: INFO: Pod "downwardapi-volume-1ae5e5a8-77da-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 12:57:08.990: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-1ae5e5a8-77da-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 12:57:09.098: INFO: Waiting for pod downwardapi-volume-1ae5e5a8-77da-11e9-8275-42209be91bd2 to disappear
May 16 12:57:09.105: INFO: Pod downwardapi-volume-1ae5e5a8-77da-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:57:09.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6812" for this suite.
May 16 12:57:15.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:57:15.812: INFO: namespace downward-api-6812 deletion completed in 6.699726202s

• [SLOW TEST:11.024 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:57:15.812: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2852
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 16 12:57:15.898: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 16 12:57:38.108: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.157:8080/dial?request=hostName&protocol=udp&host=172.25.0.156&port=8081&tries=1'] Namespace:pod-network-test-2852 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:57:38.108: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:57:38.755: INFO: Waiting for endpoints: map[]
May 16 12:57:38.763: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.157:8080/dial?request=hostName&protocol=udp&host=172.25.1.66&port=8081&tries=1'] Namespace:pod-network-test-2852 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 12:57:38.763: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 12:57:39.294: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:57:39.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2852" for this suite.
May 16 12:58:03.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 12:58:03.731: INFO: namespace pod-network-test-2852 deletion completed in 24.423190472s

• [SLOW TEST:47.919 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 12:58:03.732: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
May 16 12:58:03.827: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 16 12:58:03.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-8388'
May 16 12:58:04.319: INFO: stderr: ""
May 16 12:58:04.319: INFO: stdout: "service/redis-slave created\n"
May 16 12:58:04.320: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 16 12:58:04.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-8388'
May 16 12:58:04.757: INFO: stderr: ""
May 16 12:58:04.757: INFO: stdout: "service/redis-master created\n"
May 16 12:58:04.757: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 16 12:58:04.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-8388'
May 16 12:58:05.138: INFO: stderr: ""
May 16 12:58:05.138: INFO: stdout: "service/frontend created\n"
May 16 12:58:05.139: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 16 12:58:05.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-8388'
May 16 12:58:05.517: INFO: stderr: ""
May 16 12:58:05.517: INFO: stdout: "deployment.apps/frontend created\n"
May 16 12:58:05.518: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 16 12:58:05.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-8388'
May 16 12:58:05.829: INFO: stderr: ""
May 16 12:58:05.829: INFO: stdout: "deployment.apps/redis-master created\n"
May 16 12:58:05.829: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 16 12:58:05.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-8388'
May 16 12:58:06.880: INFO: stderr: ""
May 16 12:58:06.880: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May 16 12:58:06.880: INFO: Waiting for all frontend pods to be Running.
May 16 12:59:11.983: INFO: Waiting for frontend to serve content.
May 16 12:59:17.425: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

May 16 12:59:22.542: INFO: Trying to add a new entry to the guestbook.
May 16 12:59:22.685: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 16 12:59:22.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete --grace-period=0 --force -f - --namespace=kubectl-8388'
May 16 12:59:22.890: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 12:59:22.890: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 16 12:59:22.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete --grace-period=0 --force -f - --namespace=kubectl-8388'
May 16 12:59:23.055: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 12:59:23.055: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 16 12:59:23.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete --grace-period=0 --force -f - --namespace=kubectl-8388'
May 16 12:59:23.190: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 12:59:23.190: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 16 12:59:23.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete --grace-period=0 --force -f - --namespace=kubectl-8388'
May 16 12:59:23.333: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 12:59:23.333: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 16 12:59:23.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete --grace-period=0 --force -f - --namespace=kubectl-8388'
May 16 12:59:23.471: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 12:59:23.471: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 16 12:59:23.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete --grace-period=0 --force -f - --namespace=kubectl-8388'
May 16 12:59:23.588: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 12:59:23.588: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 12:59:23.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8388" for this suite.
May 16 13:00:03.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:00:04.092: INFO: namespace kubectl-8388 deletion completed in 40.493090154s

• [SLOW TEST:120.361 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:00:04.093: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7834
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 16 13:00:04.321: INFO: Found 0 stateful pods, waiting for 3
May 16 13:00:14.336: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 16 13:00:14.337: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 16 13:00:14.337: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 16 13:00:24.330: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 16 13:00:24.330: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 16 13:00:24.330: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 16 13:00:24.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-7834 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 16 13:00:25.137: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 16 13:00:25.137: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 16 13:00:25.137: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 16 13:00:35.210: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 16 13:00:45.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-7834 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:00:46.048: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 16 13:00:46.048: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 16 13:00:46.048: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 16 13:01:06.110: INFO: Waiting for StatefulSet statefulset-7834/ss2 to complete update
STEP: Rolling back to a previous revision
May 16 13:01:16.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-7834 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 16 13:01:16.950: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 16 13:01:16.950: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 16 13:01:16.950: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 16 13:01:27.049: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 16 13:01:37.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-7834 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:01:37.906: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 16 13:01:37.906: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 16 13:01:37.906: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 16 13:01:47.948: INFO: Waiting for StatefulSet statefulset-7834/ss2 to complete update
May 16 13:01:47.948: INFO: Waiting for Pod statefulset-7834/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
May 16 13:01:47.948: INFO: Waiting for Pod statefulset-7834/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
May 16 13:01:57.974: INFO: Waiting for StatefulSet statefulset-7834/ss2 to complete update
May 16 13:01:57.974: INFO: Waiting for Pod statefulset-7834/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
May 16 13:02:07.960: INFO: Waiting for StatefulSet statefulset-7834/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 16 13:02:17.965: INFO: Deleting all statefulset in ns statefulset-7834
May 16 13:02:17.971: INFO: Scaling statefulset ss2 to 0
May 16 13:02:28.082: INFO: Waiting for statefulset status.replicas updated to 0
May 16 13:02:28.089: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:02:28.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7834" for this suite.
May 16 13:02:36.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:02:36.560: INFO: namespace statefulset-7834 deletion completed in 8.436275986s

• [SLOW TEST:152.467 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:02:36.560: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5738.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5738.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 16 13:02:41.808: INFO: DNS probes using dns-5738/dns-test-e0aaca9f-77da-11e9-8275-42209be91bd2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:02:41.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5738" for this suite.
May 16 13:02:47.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:02:48.344: INFO: namespace dns-5738 deletion completed in 6.462774799s

• [SLOW TEST:11.784 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:02:48.344: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-e7a74e9d-77da-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 13:02:48.517: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e7a94fce-77da-11e9-8275-42209be91bd2" in namespace "projected-4848" to be "success or failure"
May 16 13:02:48.537: INFO: Pod "pod-projected-secrets-e7a94fce-77da-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.747469ms
May 16 13:02:50.545: INFO: Pod "pod-projected-secrets-e7a94fce-77da-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028175666s
May 16 13:02:52.556: INFO: Pod "pod-projected-secrets-e7a94fce-77da-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038790694s
May 16 13:02:54.565: INFO: Pod "pod-projected-secrets-e7a94fce-77da-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047810936s
STEP: Saw pod success
May 16 13:02:54.565: INFO: Pod "pod-projected-secrets-e7a94fce-77da-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:02:54.571: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-projected-secrets-e7a94fce-77da-11e9-8275-42209be91bd2 container secret-volume-test: <nil>
STEP: delete the pod
May 16 13:02:54.679: INFO: Waiting for pod pod-projected-secrets-e7a94fce-77da-11e9-8275-42209be91bd2 to disappear
May 16 13:02:54.685: INFO: Pod pod-projected-secrets-e7a94fce-77da-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:02:54.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4848" for this suite.
May 16 13:03:00.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:03:01.047: INFO: namespace projected-4848 deletion completed in 6.354553781s

• [SLOW TEST:12.704 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:03:01.051: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 16 13:03:01.168: INFO: Waiting up to 5m0s for pod "pod-ef377b14-77da-11e9-8275-42209be91bd2" in namespace "emptydir-7391" to be "success or failure"
May 16 13:03:01.174: INFO: Pod "pod-ef377b14-77da-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.107062ms
May 16 13:03:03.182: INFO: Pod "pod-ef377b14-77da-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014011685s
May 16 13:03:05.190: INFO: Pod "pod-ef377b14-77da-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022159876s
STEP: Saw pod success
May 16 13:03:05.190: INFO: Pod "pod-ef377b14-77da-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:03:05.196: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-ef377b14-77da-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 13:03:05.309: INFO: Waiting for pod pod-ef377b14-77da-11e9-8275-42209be91bd2 to disappear
May 16 13:03:05.317: INFO: Pod pod-ef377b14-77da-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:03:05.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7391" for this suite.
May 16 13:03:11.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:03:11.650: INFO: namespace emptydir-7391 deletion completed in 6.327190165s

• [SLOW TEST:10.599 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:03:11.651: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
May 16 13:03:11.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-2213'
May 16 13:03:13.424: INFO: stderr: ""
May 16 13:03:13.424: INFO: stdout: "pod/pause created\n"
May 16 13:03:13.424: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 16 13:03:13.425: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2213" to be "running and ready"
May 16 13:03:13.457: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 32.695426ms
May 16 13:03:15.465: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040190492s
May 16 13:03:17.472: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.047573819s
May 16 13:03:17.472: INFO: Pod "pause" satisfied condition "running and ready"
May 16 13:03:17.472: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
May 16 13:03:17.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 label pods pause testing-label=testing-label-value --namespace=kubectl-2213'
May 16 13:03:17.626: INFO: stderr: ""
May 16 13:03:17.626: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 16 13:03:17.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pod pause -L testing-label --namespace=kubectl-2213'
May 16 13:03:17.745: INFO: stderr: ""
May 16 13:03:17.745: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 16 13:03:17.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 label pods pause testing-label- --namespace=kubectl-2213'
May 16 13:03:17.901: INFO: stderr: ""
May 16 13:03:17.901: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 16 13:03:17.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pod pause -L testing-label --namespace=kubectl-2213'
May 16 13:03:18.018: INFO: stderr: ""
May 16 13:03:18.018: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
May 16 13:03:18.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete --grace-period=0 --force -f - --namespace=kubectl-2213'
May 16 13:03:18.214: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 13:03:18.214: INFO: stdout: "pod \"pause\" force deleted\n"
May 16 13:03:18.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get rc,svc -l name=pause --no-headers --namespace=kubectl-2213'
May 16 13:03:18.369: INFO: stderr: "No resources found.\n"
May 16 13:03:18.369: INFO: stdout: ""
May 16 13:03:18.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -l name=pause --namespace=kubectl-2213 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 16 13:03:18.476: INFO: stderr: ""
May 16 13:03:18.476: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:03:18.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2213" for this suite.
May 16 13:03:24.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:03:24.884: INFO: namespace kubectl-2213 deletion completed in 6.39717723s

• [SLOW TEST:13.233 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:03:24.885: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 13:03:25.015: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd6bcf21-77da-11e9-8275-42209be91bd2" in namespace "projected-2048" to be "success or failure"
May 16 13:03:25.020: INFO: Pod "downwardapi-volume-fd6bcf21-77da-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.234756ms
May 16 13:03:27.028: INFO: Pod "downwardapi-volume-fd6bcf21-77da-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013373379s
May 16 13:03:29.035: INFO: Pod "downwardapi-volume-fd6bcf21-77da-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020367572s
May 16 13:03:31.044: INFO: Pod "downwardapi-volume-fd6bcf21-77da-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02962316s
STEP: Saw pod success
May 16 13:03:31.045: INFO: Pod "downwardapi-volume-fd6bcf21-77da-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:03:31.050: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh pod downwardapi-volume-fd6bcf21-77da-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 13:03:31.104: INFO: Waiting for pod downwardapi-volume-fd6bcf21-77da-11e9-8275-42209be91bd2 to disappear
May 16 13:03:31.113: INFO: Pod downwardapi-volume-fd6bcf21-77da-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:03:31.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2048" for this suite.
May 16 13:03:37.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:03:37.466: INFO: namespace projected-2048 deletion completed in 6.346982068s

• [SLOW TEST:12.582 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:03:37.468: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 16 13:03:45.744: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 16 13:03:45.752: INFO: Pod pod-with-prestop-http-hook still exists
May 16 13:03:47.752: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 16 13:03:47.761: INFO: Pod pod-with-prestop-http-hook still exists
May 16 13:03:49.752: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 16 13:03:49.764: INFO: Pod pod-with-prestop-http-hook still exists
May 16 13:03:51.752: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 16 13:03:51.759: INFO: Pod pod-with-prestop-http-hook still exists
May 16 13:03:53.752: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 16 13:03:53.759: INFO: Pod pod-with-prestop-http-hook still exists
May 16 13:03:55.752: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 16 13:03:55.760: INFO: Pod pod-with-prestop-http-hook still exists
May 16 13:03:57.752: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 16 13:03:57.759: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:03:57.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3648" for this suite.
May 16 13:04:21.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:04:22.208: INFO: namespace container-lifecycle-hook-3648 deletion completed in 24.416720873s

• [SLOW TEST:44.741 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:04:22.209: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-9777/configmap-test-1f96bda4-77db-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 13:04:22.345: INFO: Waiting up to 5m0s for pod "pod-configmaps-1f9a3203-77db-11e9-8275-42209be91bd2" in namespace "configmap-9777" to be "success or failure"
May 16 13:04:22.351: INFO: Pod "pod-configmaps-1f9a3203-77db-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026378ms
May 16 13:04:24.368: INFO: Pod "pod-configmaps-1f9a3203-77db-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022752109s
May 16 13:04:26.379: INFO: Pod "pod-configmaps-1f9a3203-77db-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034244631s
STEP: Saw pod success
May 16 13:04:26.379: INFO: Pod "pod-configmaps-1f9a3203-77db-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:04:26.386: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-configmaps-1f9a3203-77db-11e9-8275-42209be91bd2 container env-test: <nil>
STEP: delete the pod
May 16 13:04:26.451: INFO: Waiting for pod pod-configmaps-1f9a3203-77db-11e9-8275-42209be91bd2 to disappear
May 16 13:04:26.469: INFO: Pod pod-configmaps-1f9a3203-77db-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:04:26.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9777" for this suite.
May 16 13:04:32.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:04:32.853: INFO: namespace configmap-9777 deletion completed in 6.374363282s

• [SLOW TEST:10.644 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:04:32.853: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 16 13:04:32.951: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 16 13:04:32.992: INFO: Waiting for terminating namespaces to be deleted...
May 16 13:04:32.999: INFO: 
Logging pods the kubelet thinks is on node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh before test
May 16 13:04:33.133: INFO: canal-fvnrd from kube-system started at 2019-05-16 11:43:58 +0000 UTC (3 container statuses recorded)
May 16 13:04:33.133: INFO: 	Container calico-node ready: true, restart count 0
May 16 13:04:33.133: INFO: 	Container install-cni ready: true, restart count 0
May 16 13:04:33.133: INFO: 	Container kube-flannel ready: true, restart count 0
May 16 13:04:33.133: INFO: webterminal-85f9f78784-srg8n from webterminal started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.133: INFO: 	Container webterminal ready: true, restart count 0
May 16 13:04:33.133: INFO: tiller-deploy-796c9d7db6-hwwqm from kube-system started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.133: INFO: 	Container tiller ready: true, restart count 0
May 16 13:04:33.133: INFO: velero-5bcdd86b7c-2qd27 from velero started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.133: INFO: 	Container velero ready: true, restart count 0
May 16 13:04:33.133: INFO: sonobuoy-systemd-logs-daemon-set-8c9cd2b50a37440b-rvwgr from heptio-sonobuoy started at 2019-05-16 11:50:40 +0000 UTC (2 container statuses recorded)
May 16 13:04:33.133: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 16 13:04:33.133: INFO: 	Container systemd-logs ready: true, restart count 1
May 16 13:04:33.133: INFO: coredns-6bd858f7c-qtwtk from kube-system started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.133: INFO: 	Container coredns ready: true, restart count 0
May 16 13:04:33.133: INFO: openvpn-client-5bbcf59684-hk8ll from kube-system started at 2019-05-16 11:44:48 +0000 UTC (2 container statuses recorded)
May 16 13:04:33.133: INFO: 	Container dnat-controller ready: true, restart count 0
May 16 13:04:33.133: INFO: 	Container openvpn-client ready: true, restart count 1
May 16 13:04:33.133: INFO: kubernetes-dashboard-57b5ff8798-8ts2c from kube-system started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.133: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May 16 13:04:33.133: INFO: restic-mx74p from velero started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.133: INFO: 	Container velero ready: true, restart count 0
May 16 13:04:33.133: INFO: cluster-autoscaler-c8c56b5d9-2clck from kube-system started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.133: INFO: 	Container cluster-autoscaler ready: true, restart count 1
May 16 13:04:33.133: INFO: node-exporter-7mblb from kube-system started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.133: INFO: 	Container node-exporter ready: true, restart count 0
May 16 13:04:33.133: INFO: kube-proxy-snhz7 from kube-system started at 2019-05-16 11:43:58 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.133: INFO: 	Container kube-proxy ready: true, restart count 0
May 16 13:04:33.133: INFO: coredns-6bd858f7c-mrns8 from kube-system started at 2019-05-16 11:44:49 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.133: INFO: 	Container coredns ready: true, restart count 0
May 16 13:04:33.133: INFO: 
Logging pods the kubelet thinks is on node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u before test
May 16 13:04:33.185: INFO: restic-fvvxh from velero started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.185: INFO: 	Container velero ready: true, restart count 0
May 16 13:04:33.185: INFO: kube-proxy-wrwbt from kube-system started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.185: INFO: 	Container kube-proxy ready: true, restart count 0
May 16 13:04:33.185: INFO: node-exporter-x2gxg from kube-system started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.185: INFO: 	Container node-exporter ready: true, restart count 0
May 16 13:04:33.185: INFO: sonobuoy-systemd-logs-daemon-set-8c9cd2b50a37440b-5xwn7 from heptio-sonobuoy started at 2019-05-16 11:50:39 +0000 UTC (2 container statuses recorded)
May 16 13:04:33.185: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 16 13:04:33.185: INFO: 	Container systemd-logs ready: true, restart count 1
May 16 13:04:33.186: INFO: canal-vdgj7 from kube-system started at 2019-05-16 11:43:57 +0000 UTC (3 container statuses recorded)
May 16 13:04:33.186: INFO: 	Container calico-node ready: true, restart count 0
May 16 13:04:33.186: INFO: 	Container install-cni ready: true, restart count 0
May 16 13:04:33.186: INFO: 	Container kube-flannel ready: true, restart count 0
May 16 13:04:33.186: INFO: sonobuoy-e2e-job-d26d0e4387684e3e from heptio-sonobuoy started at 2019-05-16 11:50:39 +0000 UTC (2 container statuses recorded)
May 16 13:04:33.186: INFO: 	Container e2e ready: true, restart count 0
May 16 13:04:33.186: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 13:04:33.187: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-16 11:50:23 +0000 UTC (1 container statuses recorded)
May 16 13:04:33.187: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.159f2b8b4b246c8c], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:04:34.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8610" for this suite.
May 16 13:04:40.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:04:40.670: INFO: namespace sched-pred-8610 deletion completed in 6.380291989s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.817 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:04:40.672: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 13:04:40.794: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 16 13:04:40.870: INFO: Number of nodes with available pods: 0
May 16 13:04:40.870: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 13:04:41.897: INFO: Number of nodes with available pods: 0
May 16 13:04:41.897: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 13:04:42.899: INFO: Number of nodes with available pods: 0
May 16 13:04:42.899: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 13:04:43.884: INFO: Number of nodes with available pods: 1
May 16 13:04:43.885: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 13:04:44.885: INFO: Number of nodes with available pods: 1
May 16 13:04:44.885: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 13:04:45.900: INFO: Number of nodes with available pods: 1
May 16 13:04:45.900: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 13:04:46.883: INFO: Number of nodes with available pods: 1
May 16 13:04:46.883: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 13:04:47.885: INFO: Number of nodes with available pods: 2
May 16 13:04:47.885: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 16 13:04:47.962: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:47.962: INFO: Wrong image for pod: daemon-set-s8njj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:49.008: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:49.008: INFO: Wrong image for pod: daemon-set-s8njj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:49.996: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:49.996: INFO: Wrong image for pod: daemon-set-s8njj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:49.996: INFO: Pod daemon-set-s8njj is not available
May 16 13:04:50.995: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:50.995: INFO: Pod daemon-set-qrlfr is not available
May 16 13:04:52.097: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:52.097: INFO: Pod daemon-set-qrlfr is not available
May 16 13:04:52.997: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:52.997: INFO: Pod daemon-set-qrlfr is not available
May 16 13:04:54.000: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:54.000: INFO: Pod daemon-set-qrlfr is not available
May 16 13:04:54.996: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:54.996: INFO: Pod daemon-set-qrlfr is not available
May 16 13:04:55.999: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:55.999: INFO: Pod daemon-set-qrlfr is not available
May 16 13:04:56.996: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:57.997: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:04:59.000: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:05:00.025: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:05:00.026: INFO: Pod daemon-set-2ggr9 is not available
May 16 13:05:00.996: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:05:00.996: INFO: Pod daemon-set-2ggr9 is not available
May 16 13:05:01.997: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:05:01.997: INFO: Pod daemon-set-2ggr9 is not available
May 16 13:05:02.997: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:05:02.997: INFO: Pod daemon-set-2ggr9 is not available
May 16 13:05:03.996: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:05:03.996: INFO: Pod daemon-set-2ggr9 is not available
May 16 13:05:04.997: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:05:04.997: INFO: Pod daemon-set-2ggr9 is not available
May 16 13:05:06.009: INFO: Wrong image for pod: daemon-set-2ggr9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 16 13:05:06.009: INFO: Pod daemon-set-2ggr9 is not available
May 16 13:05:06.996: INFO: Pod daemon-set-dtc7w is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May 16 13:05:07.021: INFO: Number of nodes with available pods: 1
May 16 13:05:07.021: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 13:05:08.032: INFO: Number of nodes with available pods: 1
May 16 13:05:08.032: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 13:05:09.039: INFO: Number of nodes with available pods: 1
May 16 13:05:09.039: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 13:05:10.038: INFO: Number of nodes with available pods: 1
May 16 13:05:10.038: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 13:05:11.047: INFO: Number of nodes with available pods: 1
May 16 13:05:11.047: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 13:05:12.042: INFO: Number of nodes with available pods: 1
May 16 13:05:12.042: INFO: Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh is running more than one daemon pod
May 16 13:05:13.036: INFO: Number of nodes with available pods: 2
May 16 13:05:13.036: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3023, will wait for the garbage collector to delete the pods
May 16 13:05:13.145: INFO: Deleting DaemonSet.extensions daemon-set took: 23.018786ms
May 16 13:05:13.645: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.487919ms
May 16 13:05:26.651: INFO: Number of nodes with available pods: 0
May 16 13:05:26.651: INFO: Number of running nodes: 0, number of available pods: 0
May 16 13:05:26.657: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3023/daemonsets","resourceVersion":"25188"},"items":null}

May 16 13:05:26.663: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3023/pods","resourceVersion":"25188"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:05:26.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3023" for this suite.
May 16 13:05:34.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:05:35.079: INFO: namespace daemonsets-3023 deletion completed in 8.38796082s

• [SLOW TEST:54.407 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:05:35.079: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 13:05:35.250: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b0d7fd5-77db-11e9-8275-42209be91bd2" in namespace "downward-api-9698" to be "success or failure"
May 16 13:05:35.256: INFO: Pod "downwardapi-volume-4b0d7fd5-77db-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.480945ms
May 16 13:05:37.266: INFO: Pod "downwardapi-volume-4b0d7fd5-77db-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015886186s
May 16 13:05:39.274: INFO: Pod "downwardapi-volume-4b0d7fd5-77db-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024187052s
STEP: Saw pod success
May 16 13:05:39.274: INFO: Pod "downwardapi-volume-4b0d7fd5-77db-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:05:39.281: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-4b0d7fd5-77db-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 13:05:39.356: INFO: Waiting for pod downwardapi-volume-4b0d7fd5-77db-11e9-8275-42209be91bd2 to disappear
May 16 13:05:39.365: INFO: Pod downwardapi-volume-4b0d7fd5-77db-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:05:39.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9698" for this suite.
May 16 13:05:45.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:05:45.833: INFO: namespace downward-api-9698 deletion completed in 6.450235473s

• [SLOW TEST:10.754 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:05:45.834: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 16 13:05:50.529: INFO: Successfully updated pod "pod-update-5170a9de-77db-11e9-8275-42209be91bd2"
STEP: verifying the updated pod is in kubernetes
May 16 13:05:50.541: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:05:50.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4443" for this suite.
May 16 13:06:14.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:06:14.960: INFO: namespace pods-4443 deletion completed in 24.409260234s

• [SLOW TEST:29.126 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:06:14.962: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
May 16 13:06:19.704: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5784 pod-service-account-6327ff50-77db-11e9-8275-42209be91bd2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 16 13:06:20.527: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5784 pod-service-account-6327ff50-77db-11e9-8275-42209be91bd2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 16 13:06:21.223: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5784 pod-service-account-6327ff50-77db-11e9-8275-42209be91bd2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:06:21.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5784" for this suite.
May 16 13:06:27.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:06:28.239: INFO: namespace svcaccounts-5784 deletion completed in 6.340133173s

• [SLOW TEST:13.278 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:06:28.244: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-1083
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1083
STEP: Deleting pre-stop pod
May 16 13:06:45.666: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:06:45.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1083" for this suite.
May 16 13:07:25.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:07:26.282: INFO: namespace prestop-1083 deletion completed in 40.402254261s

• [SLOW TEST:58.038 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:07:26.285: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 16 13:07:26.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-2071'
May 16 13:07:26.505: INFO: stderr: ""
May 16 13:07:26.506: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
May 16 13:07:31.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pod e2e-test-nginx-pod --namespace=kubectl-2071 -o json'
May 16 13:07:31.669: INFO: stderr: ""
May 16 13:07:31.669: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-05-16T13:07:26Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-2071\",\n        \"resourceVersion\": \"25731\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2071/pods/e2e-test-nginx-pod\",\n        \"uid\": \"8d6b9473-77db-11e9-982f-0a580af40783\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-wqjml\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-wqjml\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-wqjml\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-16T13:07:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-16T13:07:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-16T13:07:28Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-16T13:07:26Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://54ed6a92b1d772e8d97f3a5a8d46cead72283266183c8141a0a3bbe7a2ebfe16\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-05-16T13:07:28Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.12\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.0.182\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-05-16T13:07:26Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 16 13:07:31.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 replace -f - --namespace=kubectl-2071'
May 16 13:07:31.998: INFO: stderr: ""
May 16 13:07:31.998: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
May 16 13:07:32.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete pods e2e-test-nginx-pod --namespace=kubectl-2071'
May 16 13:07:36.558: INFO: stderr: ""
May 16 13:07:36.558: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:07:36.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2071" for this suite.
May 16 13:07:42.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:07:42.970: INFO: namespace kubectl-2071 deletion completed in 6.404126526s

• [SLOW TEST:16.686 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:07:42.974: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 13:07:43.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 version'
May 16 13:07:43.241: INFO: stderr: ""
May 16 13:07:43.241: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:11:31Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:02:58Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:07:43.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8531" for this suite.
May 16 13:07:49.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:07:49.596: INFO: namespace kubectl-8531 deletion completed in 6.346980096s

• [SLOW TEST:6.622 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:07:49.598: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-9b3882f9-77db-11e9-8275-42209be91bd2
May 16 13:07:49.745: INFO: Pod name my-hostname-basic-9b3882f9-77db-11e9-8275-42209be91bd2: Found 0 pods out of 1
May 16 13:07:54.753: INFO: Pod name my-hostname-basic-9b3882f9-77db-11e9-8275-42209be91bd2: Found 1 pods out of 1
May 16 13:07:54.753: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9b3882f9-77db-11e9-8275-42209be91bd2" are running
May 16 13:07:54.758: INFO: Pod "my-hostname-basic-9b3882f9-77db-11e9-8275-42209be91bd2-97bxk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-16 13:07:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-16 13:07:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-16 13:07:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-16 13:07:49 +0000 UTC Reason: Message:}])
May 16 13:07:54.758: INFO: Trying to dial the pod
May 16 13:07:59.890: INFO: Controller my-hostname-basic-9b3882f9-77db-11e9-8275-42209be91bd2: Got expected result from replica 1 [my-hostname-basic-9b3882f9-77db-11e9-8275-42209be91bd2-97bxk]: "my-hostname-basic-9b3882f9-77db-11e9-8275-42209be91bd2-97bxk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:07:59.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8534" for this suite.
May 16 13:08:07.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:08:08.319: INFO: namespace replication-controller-8534 deletion completed in 8.422345134s

• [SLOW TEST:18.722 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:08:08.320: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 16 13:08:08.471: INFO: Waiting up to 5m0s for pod "pod-a65fa570-77db-11e9-8275-42209be91bd2" in namespace "emptydir-850" to be "success or failure"
May 16 13:08:08.478: INFO: Pod "pod-a65fa570-77db-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.885345ms
May 16 13:08:10.486: INFO: Pod "pod-a65fa570-77db-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014713503s
May 16 13:08:12.494: INFO: Pod "pod-a65fa570-77db-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023667401s
STEP: Saw pod success
May 16 13:08:12.495: INFO: Pod "pod-a65fa570-77db-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:08:12.499: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-a65fa570-77db-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 13:08:12.551: INFO: Waiting for pod pod-a65fa570-77db-11e9-8275-42209be91bd2 to disappear
May 16 13:08:12.560: INFO: Pod pod-a65fa570-77db-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:08:12.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-850" for this suite.
May 16 13:08:18.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:08:18.965: INFO: namespace emptydir-850 deletion completed in 6.396982729s

• [SLOW TEST:10.646 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:08:18.966: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 16 13:08:19.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8103'
May 16 13:08:19.240: INFO: stderr: ""
May 16 13:08:19.240: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
May 16 13:08:19.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete pods e2e-test-nginx-pod --namespace=kubectl-8103'
May 16 13:08:26.517: INFO: stderr: ""
May 16 13:08:26.517: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:08:26.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8103" for this suite.
May 16 13:08:32.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:08:32.889: INFO: namespace kubectl-8103 deletion completed in 6.366567792s

• [SLOW TEST:13.923 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:08:32.890: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-3309/configmap-test-b4ffe7bb-77db-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 13:08:33.027: INFO: Waiting up to 5m0s for pod "pod-configmaps-b50263ba-77db-11e9-8275-42209be91bd2" in namespace "configmap-3309" to be "success or failure"
May 16 13:08:33.033: INFO: Pod "pod-configmaps-b50263ba-77db-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.527126ms
May 16 13:08:35.041: INFO: Pod "pod-configmaps-b50263ba-77db-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013325924s
May 16 13:08:37.049: INFO: Pod "pod-configmaps-b50263ba-77db-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021594569s
STEP: Saw pod success
May 16 13:08:37.049: INFO: Pod "pod-configmaps-b50263ba-77db-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:08:37.076: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-configmaps-b50263ba-77db-11e9-8275-42209be91bd2 container env-test: <nil>
STEP: delete the pod
May 16 13:08:37.140: INFO: Waiting for pod pod-configmaps-b50263ba-77db-11e9-8275-42209be91bd2 to disappear
May 16 13:08:37.147: INFO: Pod pod-configmaps-b50263ba-77db-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:08:37.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3309" for this suite.
May 16 13:08:43.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:08:43.532: INFO: namespace configmap-3309 deletion completed in 6.377396397s

• [SLOW TEST:10.642 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:08:43.535: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 13:08:43.727: INFO: Creating deployment "test-recreate-deployment"
May 16 13:08:43.742: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 16 13:08:43.762: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 16 13:08:45.807: INFO: Waiting deployment "test-recreate-deployment" to complete
May 16 13:08:45.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693608923, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693608923, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693608923, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693608923, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 13:08:47.823: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 16 13:08:47.858: INFO: Updating deployment test-recreate-deployment
May 16 13:08:47.858: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 16 13:08:48.124: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-1886,SelfLink:/apis/apps/v1/namespaces/deployment-1886/deployments/test-recreate-deployment,UID:bb777e4e-77db-11e9-b44f-0a580af42602,ResourceVersion:26145,Generation:2,CreationTimestamp:2019-05-16 13:08:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-05-16 13:08:48 +0000 UTC 2019-05-16 13:08:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-16 13:08:48 +0000 UTC 2019-05-16 13:08:43 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

May 16 13:08:48.133: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-1886,SelfLink:/apis/apps/v1/namespaces/deployment-1886/replicasets/test-recreate-deployment-c9cbd8684,UID:bdfec573-77db-11e9-b44f-0a580af42602,ResourceVersion:26144,Generation:1,CreationTimestamp:2019-05-16 13:08:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment bb777e4e-77db-11e9-b44f-0a580af42602 0xc0024bcdf0 0xc0024bcdf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 16 13:08:48.133: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 16 13:08:48.133: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-1886,SelfLink:/apis/apps/v1/namespaces/deployment-1886/replicasets/test-recreate-deployment-7d57d5ff7c,UID:bb7ac1fc-77db-11e9-b44f-0a580af42602,ResourceVersion:26134,Generation:2,CreationTimestamp:2019-05-16 13:08:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment bb777e4e-77db-11e9-b44f-0a580af42602 0xc0024bcc07 0xc0024bcc08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 16 13:08:48.149: INFO: Pod "test-recreate-deployment-c9cbd8684-bvj48" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-bvj48,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-1886,SelfLink:/api/v1/namespaces/deployment-1886/pods/test-recreate-deployment-c9cbd8684-bvj48,UID:be07aafb-77db-11e9-b44f-0a580af42602,ResourceVersion:26146,Generation:0,CreationTimestamp:2019-05-16 13:08:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 bdfec573-77db-11e9-b44f-0a580af42602 0xc0024bdce0 0xc0024bdce1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cgrdx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cgrdx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cgrdx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024bdd40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024bdd60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:08:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:08:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:08:48 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:,StartTime:2019-05-16 13:08:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:08:48.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1886" for this suite.
May 16 13:08:56.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:08:56.577: INFO: namespace deployment-1886 deletion completed in 8.421560574s

• [SLOW TEST:13.042 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:08:56.578: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 16 13:09:01.340: INFO: Successfully updated pod "annotationupdatec3259092-77db-11e9-8275-42209be91bd2"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:09:03.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7955" for this suite.
May 16 13:09:27.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:09:27.764: INFO: namespace downward-api-7955 deletion completed in 24.358012968s

• [SLOW TEST:31.187 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:09:27.764: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5901
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-5901
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5901
May 16 13:09:27.885: INFO: Found 0 stateful pods, waiting for 1
May 16 13:09:37.893: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 16 13:09:37.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 16 13:09:38.630: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 16 13:09:38.630: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 16 13:09:38.630: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 16 13:09:38.639: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 16 13:09:48.721: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 16 13:09:48.721: INFO: Waiting for statefulset status.replicas updated to 0
May 16 13:09:48.783: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
May 16 13:09:48.783: INFO: ss-0  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  }]
May 16 13:09:48.783: INFO: 
May 16 13:09:48.783: INFO: StatefulSet ss has not reached scale 3, at 1
May 16 13:09:49.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992661484s
May 16 13:09:50.800: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983462721s
May 16 13:09:51.806: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97570194s
May 16 13:09:52.879: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969137203s
May 16 13:09:53.886: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.896660474s
May 16 13:09:54.893: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.889059517s
May 16 13:09:55.901: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.882185858s
May 16 13:09:56.909: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.874166315s
May 16 13:09:57.926: INFO: Verifying statefulset ss doesn't scale past 3 for another 865.977844ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5901
May 16 13:09:58.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:09:59.615: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 16 13:09:59.615: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 16 13:09:59.615: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 16 13:09:59.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:10:00.340: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 16 13:10:00.340: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 16 13:10:00.340: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 16 13:10:00.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:10:01.017: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 16 13:10:01.017: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 16 13:10:01.017: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 16 13:10:01.032: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 16 13:10:01.032: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 16 13:10:01.032: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 16 13:10:01.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 16 13:10:01.729: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 16 13:10:01.729: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 16 13:10:01.729: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 16 13:10:01.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 16 13:10:02.538: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 16 13:10:02.538: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 16 13:10:02.538: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 16 13:10:02.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 16 13:10:03.188: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 16 13:10:03.188: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 16 13:10:03.188: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 16 13:10:03.188: INFO: Waiting for statefulset status.replicas updated to 0
May 16 13:10:03.194: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 16 13:10:13.220: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 16 13:10:13.220: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 16 13:10:13.220: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 16 13:10:13.279: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
May 16 13:10:13.279: INFO: ss-0  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  }]
May 16 13:10:13.279: INFO: ss-1  machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  }]
May 16 13:10:13.279: INFO: ss-2  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  }]
May 16 13:10:13.279: INFO: 
May 16 13:10:13.279: INFO: StatefulSet ss has not reached scale 0, at 3
May 16 13:10:14.290: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
May 16 13:10:14.291: INFO: ss-0  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  }]
May 16 13:10:14.291: INFO: ss-1  machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  }]
May 16 13:10:14.291: INFO: ss-2  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  }]
May 16 13:10:14.291: INFO: 
May 16 13:10:14.291: INFO: StatefulSet ss has not reached scale 0, at 3
May 16 13:10:15.298: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
May 16 13:10:15.298: INFO: ss-0  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  }]
May 16 13:10:15.298: INFO: ss-1  machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  }]
May 16 13:10:15.298: INFO: ss-2  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  }]
May 16 13:10:15.298: INFO: 
May 16 13:10:15.298: INFO: StatefulSet ss has not reached scale 0, at 3
May 16 13:10:16.307: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
May 16 13:10:16.307: INFO: ss-0  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  }]
May 16 13:10:16.307: INFO: ss-1  machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  }]
May 16 13:10:16.307: INFO: ss-2  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  }]
May 16 13:10:16.307: INFO: 
May 16 13:10:16.307: INFO: StatefulSet ss has not reached scale 0, at 3
May 16 13:10:17.314: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
May 16 13:10:17.315: INFO: ss-0  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  }]
May 16 13:10:17.315: INFO: ss-1  machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  }]
May 16 13:10:17.315: INFO: ss-2  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  }]
May 16 13:10:17.315: INFO: 
May 16 13:10:17.315: INFO: StatefulSet ss has not reached scale 0, at 3
May 16 13:10:18.322: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
May 16 13:10:18.322: INFO: ss-0  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  }]
May 16 13:10:18.322: INFO: ss-1  machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  }]
May 16 13:10:18.322: INFO: ss-2  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  }]
May 16 13:10:18.322: INFO: 
May 16 13:10:18.322: INFO: StatefulSet ss has not reached scale 0, at 3
May 16 13:10:19.330: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
May 16 13:10:19.330: INFO: ss-0  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  }]
May 16 13:10:19.330: INFO: ss-1  machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  }]
May 16 13:10:19.330: INFO: ss-2  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  }]
May 16 13:10:19.330: INFO: 
May 16 13:10:19.330: INFO: StatefulSet ss has not reached scale 0, at 3
May 16 13:10:20.337: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
May 16 13:10:20.337: INFO: ss-0  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  }]
May 16 13:10:20.338: INFO: ss-1  machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  }]
May 16 13:10:20.338: INFO: ss-2  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  }]
May 16 13:10:20.338: INFO: 
May 16 13:10:20.338: INFO: StatefulSet ss has not reached scale 0, at 3
May 16 13:10:21.351: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
May 16 13:10:21.351: INFO: ss-0  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  }]
May 16 13:10:21.351: INFO: ss-1  machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  }]
May 16 13:10:21.351: INFO: ss-2  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  }]
May 16 13:10:21.351: INFO: 
May 16 13:10:21.351: INFO: StatefulSet ss has not reached scale 0, at 3
May 16 13:10:22.360: INFO: POD   NODE                                                    PHASE    GRACE  CONDITIONS
May 16 13:10:22.360: INFO: ss-0  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:28 +0000 UTC  }]
May 16 13:10:22.360: INFO: ss-1  machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  }]
May 16 13:10:22.360: INFO: ss-2  machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:10:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:09:49 +0000 UTC  }]
May 16 13:10:22.360: INFO: 
May 16 13:10:22.360: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5901
May 16 13:10:23.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:10:23.664: INFO: rc: 1
May 16 13:10:23.664: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc003035ce0 exit status 1 <nil> <nil> true [0xc002abc308 0xc002abc320 0xc002abc338] [0xc002abc308 0xc002abc320 0xc002abc338] [0xc002abc318 0xc002abc330] [0x9bf9f0 0x9bf9f0] 0xc0020ee900 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

May 16 13:10:33.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:10:33.765: INFO: rc: 1
May 16 13:10:33.765: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002cfda40 exit status 1 <nil> <nil> true [0xc0024a7a70 0xc0024a7a88 0xc0024a7aa0] [0xc0024a7a70 0xc0024a7a88 0xc0024a7aa0] [0xc0024a7a80 0xc0024a7a98] [0x9bf9f0 0x9bf9f0] 0xc0017391a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:10:43.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:10:43.852: INFO: rc: 1
May 16 13:10:43.852: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002cfddd0 exit status 1 <nil> <nil> true [0xc0024a7aa8 0xc0024a7ac0 0xc0024a7ad8] [0xc0024a7aa8 0xc0024a7ac0 0xc0024a7ad8] [0xc0024a7ab8 0xc0024a7ad0] [0x9bf9f0 0x9bf9f0] 0xc001739500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:10:53.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:10:53.952: INFO: rc: 1
May 16 13:10:53.952: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002496330 exit status 1 <nil> <nil> true [0xc00313c010 0xc00313c028 0xc00313c040] [0xc00313c010 0xc00313c028 0xc00313c040] [0xc00313c020 0xc00313c038] [0x9bf9f0 0x9bf9f0] 0xc0022735c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:11:03.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:11:04.064: INFO: rc: 1
May 16 13:11:04.064: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002146330 exit status 1 <nil> <nil> true [0xc0024a6008 0xc0024a6040 0xc0024a6080] [0xc0024a6008 0xc0024a6040 0xc0024a6080] [0xc0024a6030 0xc0024a6070] [0x9bf9f0 0x9bf9f0] 0xc002689320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:11:14.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:11:14.161: INFO: rc: 1
May 16 13:11:14.161: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024966c0 exit status 1 <nil> <nil> true [0xc00313c048 0xc00313c060 0xc00313c090] [0xc00313c048 0xc00313c060 0xc00313c090] [0xc00313c058 0xc00313c078] [0x9bf9f0 0x9bf9f0] 0xc001e62180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:11:24.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:11:24.247: INFO: rc: 1
May 16 13:11:24.247: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0021466c0 exit status 1 <nil> <nil> true [0xc0024a6090 0xc0024a60e0 0xc0024a6120] [0xc0024a6090 0xc0024a60e0 0xc0024a6120] [0xc0024a60d0 0xc0024a6100] [0x9bf9f0 0x9bf9f0] 0xc0021da000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:11:34.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:11:34.338: INFO: rc: 1
May 16 13:11:34.338: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002496a50 exit status 1 <nil> <nil> true [0xc00313c098 0xc00313c0d0 0xc00313c118] [0xc00313c098 0xc00313c0d0 0xc00313c118] [0xc00313c0b0 0xc00313c100] [0x9bf9f0 0x9bf9f0] 0xc001e628a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:11:44.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:11:44.441: INFO: rc: 1
May 16 13:11:44.441: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002496de0 exit status 1 <nil> <nil> true [0xc00313c120 0xc00313c160 0xc00313c190] [0xc00313c120 0xc00313c160 0xc00313c190] [0xc00313c140 0xc00313c188] [0x9bf9f0 0x9bf9f0] 0xc001e63200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:11:54.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:11:54.539: INFO: rc: 1
May 16 13:11:54.540: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024972c0 exit status 1 <nil> <nil> true [0xc00313c1b0 0xc00313c208 0xc00313c228] [0xc00313c1b0 0xc00313c208 0xc00313c228] [0xc00313c1f0 0xc00313c218] [0x9bf9f0 0x9bf9f0] 0xc001e638c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:12:04.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:12:04.622: INFO: rc: 1
May 16 13:12:04.622: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002146a50 exit status 1 <nil> <nil> true [0xc0024a6130 0xc0024a6180 0xc0024a6198] [0xc0024a6130 0xc0024a6180 0xc0024a6198] [0xc0024a6158 0xc0024a6190] [0x9bf9f0 0x9bf9f0] 0xc0021da360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:12:14.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:12:14.701: INFO: rc: 1
May 16 13:12:14.701: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002497680 exit status 1 <nil> <nil> true [0xc00313c230 0xc00313c270 0xc00313c2b8] [0xc00313c230 0xc00313c270 0xc00313c2b8] [0xc00313c260 0xc00313c298] [0x9bf9f0 0x9bf9f0] 0xc001e63d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:12:24.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:12:24.778: INFO: rc: 1
May 16 13:12:24.778: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002146f00 exit status 1 <nil> <nil> true [0xc0024a61a0 0xc0024a61b8 0xc0024a61d0] [0xc0024a61a0 0xc0024a61b8 0xc0024a61d0] [0xc0024a61b0 0xc0024a61c8] [0x9bf9f0 0x9bf9f0] 0xc0021da6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:12:34.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:12:34.874: INFO: rc: 1
May 16 13:12:34.874: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0021472c0 exit status 1 <nil> <nil> true [0xc0024a61e0 0xc0024a6220 0xc0024a6270] [0xc0024a61e0 0xc0024a6220 0xc0024a6270] [0xc0024a6208 0xc0024a6240] [0x9bf9f0 0x9bf9f0] 0xc0021daa20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:12:44.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:12:44.968: INFO: rc: 1
May 16 13:12:44.968: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002147650 exit status 1 <nil> <nil> true [0xc0024a6280 0xc0024a62b0 0xc0024a62e0] [0xc0024a6280 0xc0024a62b0 0xc0024a62e0] [0xc0024a62a0 0xc0024a62d0] [0x9bf9f0 0x9bf9f0] 0xc0021dad80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:12:54.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:12:55.064: INFO: rc: 1
May 16 13:12:55.064: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002496360 exit status 1 <nil> <nil> true [0xc00313c010 0xc00313c028 0xc00313c040] [0xc00313c010 0xc00313c028 0xc00313c040] [0xc00313c020 0xc00313c038] [0x9bf9f0 0x9bf9f0] 0xc002689200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:13:05.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:13:05.172: INFO: rc: 1
May 16 13:13:05.172: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002496720 exit status 1 <nil> <nil> true [0xc00313c048 0xc00313c060 0xc00313c090] [0xc00313c048 0xc00313c060 0xc00313c090] [0xc00313c058 0xc00313c078] [0x9bf9f0 0x9bf9f0] 0xc002689f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:13:15.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:13:15.252: INFO: rc: 1
May 16 13:13:15.252: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002496ae0 exit status 1 <nil> <nil> true [0xc00313c098 0xc00313c0d0 0xc00313c118] [0xc00313c098 0xc00313c0d0 0xc00313c118] [0xc00313c0b0 0xc00313c100] [0x9bf9f0 0x9bf9f0] 0xc002273aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:13:25.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:13:25.335: INFO: rc: 1
May 16 13:13:25.335: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002496ea0 exit status 1 <nil> <nil> true [0xc00313c120 0xc00313c160 0xc00313c190] [0xc00313c120 0xc00313c160 0xc00313c190] [0xc00313c140 0xc00313c188] [0x9bf9f0 0x9bf9f0] 0xc001e62300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:13:35.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:13:35.476: INFO: rc: 1
May 16 13:13:35.476: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024973b0 exit status 1 <nil> <nil> true [0xc00313c1b0 0xc00313c208 0xc00313c228] [0xc00313c1b0 0xc00313c208 0xc00313c228] [0xc00313c1f0 0xc00313c218] [0x9bf9f0 0x9bf9f0] 0xc001e62a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:13:45.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:13:45.578: INFO: rc: 1
May 16 13:13:45.578: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024977a0 exit status 1 <nil> <nil> true [0xc00313c230 0xc00313c270 0xc00313c2b8] [0xc00313c230 0xc00313c270 0xc00313c2b8] [0xc00313c260 0xc00313c298] [0x9bf9f0 0x9bf9f0] 0xc001e63380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:13:55.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:13:55.670: INFO: rc: 1
May 16 13:13:55.670: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002146360 exit status 1 <nil> <nil> true [0xc0024a6008 0xc0024a6040 0xc0024a6080] [0xc0024a6008 0xc0024a6040 0xc0024a6080] [0xc0024a6030 0xc0024a6070] [0x9bf9f0 0x9bf9f0] 0xc0021da2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:14:05.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:14:05.783: INFO: rc: 1
May 16 13:14:05.783: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002497b30 exit status 1 <nil> <nil> true [0xc00313c2d8 0xc00313c300 0xc00313c328] [0xc00313c2d8 0xc00313c300 0xc00313c328] [0xc00313c2f8 0xc00313c310] [0x9bf9f0 0x9bf9f0] 0xc001e63980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:14:15.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:14:15.866: INFO: rc: 1
May 16 13:14:15.866: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002146750 exit status 1 <nil> <nil> true [0xc0024a6090 0xc0024a60e0 0xc0024a6120] [0xc0024a6090 0xc0024a60e0 0xc0024a6120] [0xc0024a60d0 0xc0024a6100] [0x9bf9f0 0x9bf9f0] 0xc0021da600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:14:25.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:14:25.973: INFO: rc: 1
May 16 13:14:25.973: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002146ae0 exit status 1 <nil> <nil> true [0xc0024a6130 0xc0024a6180 0xc0024a6198] [0xc0024a6130 0xc0024a6180 0xc0024a6198] [0xc0024a6158 0xc0024a6190] [0x9bf9f0 0x9bf9f0] 0xc0021da960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:14:35.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:14:36.060: INFO: rc: 1
May 16 13:14:36.060: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002146ff0 exit status 1 <nil> <nil> true [0xc0024a61a0 0xc0024a61b8 0xc0024a61d0] [0xc0024a61a0 0xc0024a61b8 0xc0024a61d0] [0xc0024a61b0 0xc0024a61c8] [0x9bf9f0 0x9bf9f0] 0xc0021dacc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:14:46.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:14:46.161: INFO: rc: 1
May 16 13:14:46.161: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002147380 exit status 1 <nil> <nil> true [0xc0024a61e0 0xc0024a6220 0xc0024a6270] [0xc0024a61e0 0xc0024a6220 0xc0024a6270] [0xc0024a6208 0xc0024a6240] [0x9bf9f0 0x9bf9f0] 0xc0021db020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:14:56.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:14:56.255: INFO: rc: 1
May 16 13:14:56.255: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002496330 exit status 1 <nil> <nil> true [0xc00313c010 0xc00313c028 0xc00313c040] [0xc00313c010 0xc00313c028 0xc00313c040] [0xc00313c020 0xc00313c038] [0x9bf9f0 0x9bf9f0] 0xc0022735c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:15:06.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:15:06.354: INFO: rc: 1
May 16 13:15:06.354: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002146330 exit status 1 <nil> <nil> true [0xc0024a6008 0xc0024a6040 0xc0024a6080] [0xc0024a6008 0xc0024a6040 0xc0024a6080] [0xc0024a6030 0xc0024a6070] [0x9bf9f0 0x9bf9f0] 0xc002689320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:15:16.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:15:16.440: INFO: rc: 1
May 16 13:15:16.440: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0021466f0 exit status 1 <nil> <nil> true [0xc0024a6090 0xc0024a60e0 0xc0024a6120] [0xc0024a6090 0xc0024a60e0 0xc0024a6120] [0xc0024a60d0 0xc0024a6100] [0x9bf9f0 0x9bf9f0] 0xc00009a000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 16 13:15:26.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5901 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:15:26.533: INFO: rc: 1
May 16 13:15:26.533: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
May 16 13:15:26.533: INFO: Scaling statefulset ss to 0
May 16 13:15:26.555: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 16 13:15:26.562: INFO: Deleting all statefulset in ns statefulset-5901
May 16 13:15:26.567: INFO: Scaling statefulset ss to 0
May 16 13:15:26.584: INFO: Waiting for statefulset status.replicas updated to 0
May 16 13:15:26.590: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:15:26.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5901" for this suite.
May 16 13:15:34.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:15:34.997: INFO: namespace statefulset-5901 deletion completed in 8.34874383s

• [SLOW TEST:367.233 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:15:35.000: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 16 13:15:35.269: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6835,SelfLink:/api/v1/namespaces/watch-6835/configmaps/e2e-watch-test-label-changed,UID:b0b81225-77dc-11e9-b44f-0a580af42602,ResourceVersion:27598,Generation:0,CreationTimestamp:2019-05-16 13:15:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 16 13:15:35.269: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6835,SelfLink:/api/v1/namespaces/watch-6835/configmaps/e2e-watch-test-label-changed,UID:b0b81225-77dc-11e9-b44f-0a580af42602,ResourceVersion:27599,Generation:0,CreationTimestamp:2019-05-16 13:15:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 16 13:15:35.270: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6835,SelfLink:/api/v1/namespaces/watch-6835/configmaps/e2e-watch-test-label-changed,UID:b0b81225-77dc-11e9-b44f-0a580af42602,ResourceVersion:27600,Generation:0,CreationTimestamp:2019-05-16 13:15:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 16 13:15:45.353: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6835,SelfLink:/api/v1/namespaces/watch-6835/configmaps/e2e-watch-test-label-changed,UID:b0b81225-77dc-11e9-b44f-0a580af42602,ResourceVersion:27633,Generation:0,CreationTimestamp:2019-05-16 13:15:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 16 13:15:45.353: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6835,SelfLink:/api/v1/namespaces/watch-6835/configmaps/e2e-watch-test-label-changed,UID:b0b81225-77dc-11e9-b44f-0a580af42602,ResourceVersion:27634,Generation:0,CreationTimestamp:2019-05-16 13:15:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 16 13:15:45.353: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6835,SelfLink:/api/v1/namespaces/watch-6835/configmaps/e2e-watch-test-label-changed,UID:b0b81225-77dc-11e9-b44f-0a580af42602,ResourceVersion:27635,Generation:0,CreationTimestamp:2019-05-16 13:15:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:15:45.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6835" for this suite.
May 16 13:15:51.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:15:51.800: INFO: namespace watch-6835 deletion completed in 6.439376361s

• [SLOW TEST:16.800 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:15:51.802: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
May 16 13:15:52.038: INFO: Waiting up to 5m0s for pod "client-containers-bab1d660-77dc-11e9-8275-42209be91bd2" in namespace "containers-5932" to be "success or failure"
May 16 13:15:52.044: INFO: Pod "client-containers-bab1d660-77dc-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.246619ms
May 16 13:15:54.052: INFO: Pod "client-containers-bab1d660-77dc-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013893848s
May 16 13:15:56.059: INFO: Pod "client-containers-bab1d660-77dc-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020936331s
STEP: Saw pod success
May 16 13:15:56.059: INFO: Pod "client-containers-bab1d660-77dc-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:15:56.064: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod client-containers-bab1d660-77dc-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 13:15:56.156: INFO: Waiting for pod client-containers-bab1d660-77dc-11e9-8275-42209be91bd2 to disappear
May 16 13:15:56.161: INFO: Pod client-containers-bab1d660-77dc-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:15:56.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5932" for this suite.
May 16 13:16:02.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:16:02.570: INFO: namespace containers-5932 deletion completed in 6.402657792s

• [SLOW TEST:10.769 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:16:02.575: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
May 16 13:16:02.745: INFO: Waiting up to 5m0s for pod "var-expansion-c1130112-77dc-11e9-8275-42209be91bd2" in namespace "var-expansion-6630" to be "success or failure"
May 16 13:16:02.751: INFO: Pod "var-expansion-c1130112-77dc-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.632421ms
May 16 13:16:04.767: INFO: Pod "var-expansion-c1130112-77dc-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022464937s
May 16 13:16:06.776: INFO: Pod "var-expansion-c1130112-77dc-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031167049s
STEP: Saw pod success
May 16 13:16:06.776: INFO: Pod "var-expansion-c1130112-77dc-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:16:06.793: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod var-expansion-c1130112-77dc-11e9-8275-42209be91bd2 container dapi-container: <nil>
STEP: delete the pod
May 16 13:16:06.899: INFO: Waiting for pod var-expansion-c1130112-77dc-11e9-8275-42209be91bd2 to disappear
May 16 13:16:06.909: INFO: Pod var-expansion-c1130112-77dc-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:16:06.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6630" for this suite.
May 16 13:16:12.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:16:13.288: INFO: namespace var-expansion-6630 deletion completed in 6.367047169s

• [SLOW TEST:10.713 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:16:13.289: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 16 13:16:13.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1969'
May 16 13:16:14.998: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 16 13:16:14.998: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
May 16 13:16:17.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete deployment e2e-test-nginx-deployment --namespace=kubectl-1969'
May 16 13:16:17.187: INFO: stderr: ""
May 16 13:16:17.187: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:16:17.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1969" for this suite.
May 16 13:16:41.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:16:41.571: INFO: namespace kubectl-1969 deletion completed in 24.371723196s

• [SLOW TEST:28.282 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:16:41.572: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
May 16 13:16:41.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 --namespace=kubectl-5194 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 16 13:16:45.655: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 16 13:16:45.655: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:16:47.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5194" for this suite.
May 16 13:16:53.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:16:54.051: INFO: namespace kubectl-5194 deletion completed in 6.376618603s

• [SLOW TEST:12.481 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:16:54.056: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-dfbbd902-77dc-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 13:16:54.212: INFO: Waiting up to 5m0s for pod "pod-configmaps-dfbeded7-77dc-11e9-8275-42209be91bd2" in namespace "configmap-3856" to be "success or failure"
May 16 13:16:54.218: INFO: Pod "pod-configmaps-dfbeded7-77dc-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.812386ms
May 16 13:16:56.230: INFO: Pod "pod-configmaps-dfbeded7-77dc-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01799992s
May 16 13:16:58.247: INFO: Pod "pod-configmaps-dfbeded7-77dc-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034367061s
STEP: Saw pod success
May 16 13:16:58.247: INFO: Pod "pod-configmaps-dfbeded7-77dc-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:16:58.253: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-configmaps-dfbeded7-77dc-11e9-8275-42209be91bd2 container configmap-volume-test: <nil>
STEP: delete the pod
May 16 13:16:58.310: INFO: Waiting for pod pod-configmaps-dfbeded7-77dc-11e9-8275-42209be91bd2 to disappear
May 16 13:16:58.326: INFO: Pod pod-configmaps-dfbeded7-77dc-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:16:58.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3856" for this suite.
May 16 13:17:04.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:17:04.752: INFO: namespace configmap-3856 deletion completed in 6.419437202s

• [SLOW TEST:10.696 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:17:04.754: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5269
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5269
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5269
May 16 13:17:04.904: INFO: Found 0 stateful pods, waiting for 1
May 16 13:17:14.911: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 16 13:17:14.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5269 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 16 13:17:15.575: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 16 13:17:15.575: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 16 13:17:15.575: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 16 13:17:15.588: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 16 13:17:25.598: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 16 13:17:25.598: INFO: Waiting for statefulset status.replicas updated to 0
May 16 13:17:25.638: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999495s
May 16 13:17:26.648: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994420037s
May 16 13:17:27.655: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984716874s
May 16 13:17:28.663: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.977427838s
May 16 13:17:29.674: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.969883295s
May 16 13:17:30.686: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.958855033s
May 16 13:17:31.695: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.946427548s
May 16 13:17:32.702: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.938161357s
May 16 13:17:33.726: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.930913433s
May 16 13:17:34.732: INFO: Verifying statefulset ss doesn't scale past 1 for another 907.197497ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5269
May 16 13:17:35.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5269 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:17:36.396: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 16 13:17:36.396: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 16 13:17:36.396: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 16 13:17:36.403: INFO: Found 1 stateful pods, waiting for 3
May 16 13:17:46.416: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 16 13:17:46.416: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 16 13:17:46.416: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 16 13:17:46.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5269 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 16 13:17:47.142: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 16 13:17:47.142: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 16 13:17:47.142: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 16 13:17:47.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5269 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 16 13:17:47.925: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 16 13:17:47.925: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 16 13:17:47.925: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 16 13:17:47.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5269 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 16 13:17:48.637: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 16 13:17:48.637: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 16 13:17:48.638: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 16 13:17:48.638: INFO: Waiting for statefulset status.replicas updated to 0
May 16 13:17:48.649: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 16 13:17:58.673: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 16 13:17:58.673: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 16 13:17:58.673: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 16 13:17:58.696: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999732s
May 16 13:17:59.709: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993867991s
May 16 13:18:00.721: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980765036s
May 16 13:18:01.770: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.969082947s
May 16 13:18:02.794: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.919931404s
May 16 13:18:03.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.896599049s
May 16 13:18:04.847: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.851886525s
May 16 13:18:05.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.843382091s
May 16 13:18:06.877: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.831022371s
May 16 13:18:07.884: INFO: Verifying statefulset ss doesn't scale past 3 for another 813.154428ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5269
May 16 13:18:08.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5269 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:18:09.589: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 16 13:18:09.589: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 16 13:18:09.589: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 16 13:18:09.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5269 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:18:10.268: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 16 13:18:10.268: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 16 13:18:10.268: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 16 13:18:10.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 exec --namespace=statefulset-5269 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 16 13:18:10.993: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 16 13:18:10.993: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 16 13:18:10.993: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 16 13:18:10.993: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 16 13:18:31.092: INFO: Deleting all statefulset in ns statefulset-5269
May 16 13:18:31.097: INFO: Scaling statefulset ss to 0
May 16 13:18:31.112: INFO: Waiting for statefulset status.replicas updated to 0
May 16 13:18:31.116: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:18:31.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5269" for this suite.
May 16 13:18:39.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:18:39.530: INFO: namespace statefulset-5269 deletion completed in 8.376825192s

• [SLOW TEST:94.776 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:18:39.534: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0516 13:18:49.957152      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 16 13:18:49.957: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:18:49.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4578" for this suite.
May 16 13:18:57.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:18:58.359: INFO: namespace gc-4578 deletion completed in 8.394983871s

• [SLOW TEST:18.824 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:18:58.359: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 13:18:58.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-29d2987f-77dd-11e9-8275-42209be91bd2" in namespace "downward-api-5954" to be "success or failure"
May 16 13:18:58.513: INFO: Pod "downwardapi-volume-29d2987f-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.274287ms
May 16 13:19:00.522: INFO: Pod "downwardapi-volume-29d2987f-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02645677s
May 16 13:19:02.534: INFO: Pod "downwardapi-volume-29d2987f-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038300652s
May 16 13:19:04.542: INFO: Pod "downwardapi-volume-29d2987f-77dd-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046206299s
STEP: Saw pod success
May 16 13:19:04.542: INFO: Pod "downwardapi-volume-29d2987f-77dd-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:19:04.558: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-29d2987f-77dd-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 13:19:04.725: INFO: Waiting for pod downwardapi-volume-29d2987f-77dd-11e9-8275-42209be91bd2 to disappear
May 16 13:19:04.774: INFO: Pod downwardapi-volume-29d2987f-77dd-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:19:04.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5954" for this suite.
May 16 13:19:10.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:19:11.169: INFO: namespace downward-api-5954 deletion completed in 6.384866839s

• [SLOW TEST:12.810 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:19:11.170: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 13:19:11.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-317c5768-77dd-11e9-8275-42209be91bd2" in namespace "projected-1749" to be "success or failure"
May 16 13:19:11.354: INFO: Pod "downwardapi-volume-317c5768-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.87562ms
May 16 13:19:13.360: INFO: Pod "downwardapi-volume-317c5768-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017533199s
May 16 13:19:15.368: INFO: Pod "downwardapi-volume-317c5768-77dd-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025437772s
STEP: Saw pod success
May 16 13:19:15.368: INFO: Pod "downwardapi-volume-317c5768-77dd-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:19:15.378: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-317c5768-77dd-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 13:19:15.441: INFO: Waiting for pod downwardapi-volume-317c5768-77dd-11e9-8275-42209be91bd2 to disappear
May 16 13:19:15.463: INFO: Pod downwardapi-volume-317c5768-77dd-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:19:15.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1749" for this suite.
May 16 13:19:21.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:19:21.854: INFO: namespace projected-1749 deletion completed in 6.383082252s

• [SLOW TEST:10.685 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:19:21.855: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-37f52392-77dd-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 13:19:22.246: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-37f8ab32-77dd-11e9-8275-42209be91bd2" in namespace "projected-6213" to be "success or failure"
May 16 13:19:22.253: INFO: Pod "pod-projected-configmaps-37f8ab32-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.036872ms
May 16 13:19:24.273: INFO: Pod "pod-projected-configmaps-37f8ab32-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027475865s
May 16 13:19:26.282: INFO: Pod "pod-projected-configmaps-37f8ab32-77dd-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036017827s
STEP: Saw pod success
May 16 13:19:26.282: INFO: Pod "pod-projected-configmaps-37f8ab32-77dd-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:19:26.300: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-projected-configmaps-37f8ab32-77dd-11e9-8275-42209be91bd2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 16 13:19:26.444: INFO: Waiting for pod pod-projected-configmaps-37f8ab32-77dd-11e9-8275-42209be91bd2 to disappear
May 16 13:19:26.450: INFO: Pod pod-projected-configmaps-37f8ab32-77dd-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:19:26.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6213" for this suite.
May 16 13:19:32.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:19:32.894: INFO: namespace projected-6213 deletion completed in 6.429747619s

• [SLOW TEST:11.039 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:19:32.894: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 13:19:33.014: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e6675ad-77dd-11e9-8275-42209be91bd2" in namespace "projected-9233" to be "success or failure"
May 16 13:19:33.022: INFO: Pod "downwardapi-volume-3e6675ad-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.899605ms
May 16 13:19:35.029: INFO: Pod "downwardapi-volume-3e6675ad-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014959475s
May 16 13:19:37.036: INFO: Pod "downwardapi-volume-3e6675ad-77dd-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021943117s
STEP: Saw pod success
May 16 13:19:37.036: INFO: Pod "downwardapi-volume-3e6675ad-77dd-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:19:37.042: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-3e6675ad-77dd-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 13:19:37.150: INFO: Waiting for pod downwardapi-volume-3e6675ad-77dd-11e9-8275-42209be91bd2 to disappear
May 16 13:19:37.157: INFO: Pod downwardapi-volume-3e6675ad-77dd-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:19:37.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9233" for this suite.
May 16 13:19:43.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:19:43.615: INFO: namespace projected-9233 deletion completed in 6.449293756s

• [SLOW TEST:10.721 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:19:43.618: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-44d0ca4b-77dd-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 13:19:43.807: INFO: Waiting up to 5m0s for pod "pod-secrets-44d2439b-77dd-11e9-8275-42209be91bd2" in namespace "secrets-3079" to be "success or failure"
May 16 13:19:43.819: INFO: Pod "pod-secrets-44d2439b-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.24653ms
May 16 13:19:45.827: INFO: Pod "pod-secrets-44d2439b-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019063469s
May 16 13:19:47.834: INFO: Pod "pod-secrets-44d2439b-77dd-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026099013s
STEP: Saw pod success
May 16 13:19:47.834: INFO: Pod "pod-secrets-44d2439b-77dd-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:19:47.840: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-secrets-44d2439b-77dd-11e9-8275-42209be91bd2 container secret-volume-test: <nil>
STEP: delete the pod
May 16 13:19:47.904: INFO: Waiting for pod pod-secrets-44d2439b-77dd-11e9-8275-42209be91bd2 to disappear
May 16 13:19:47.918: INFO: Pod pod-secrets-44d2439b-77dd-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:19:47.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3079" for this suite.
May 16 13:19:53.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:19:54.271: INFO: namespace secrets-3079 deletion completed in 6.345861623s

• [SLOW TEST:10.653 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:19:54.272: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 16 13:19:54.385: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 16 13:19:54.398: INFO: Waiting for terminating namespaces to be deleted...
May 16 13:19:54.403: INFO: 
Logging pods the kubelet thinks is on node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh before test
May 16 13:19:54.599: INFO: node-exporter-7mblb from kube-system started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.599: INFO: 	Container node-exporter ready: true, restart count 0
May 16 13:19:54.599: INFO: kube-proxy-snhz7 from kube-system started at 2019-05-16 11:43:58 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.599: INFO: 	Container kube-proxy ready: true, restart count 0
May 16 13:19:54.599: INFO: coredns-6bd858f7c-mrns8 from kube-system started at 2019-05-16 11:44:49 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.599: INFO: 	Container coredns ready: true, restart count 0
May 16 13:19:54.599: INFO: canal-fvnrd from kube-system started at 2019-05-16 11:43:58 +0000 UTC (3 container statuses recorded)
May 16 13:19:54.599: INFO: 	Container calico-node ready: true, restart count 0
May 16 13:19:54.599: INFO: 	Container install-cni ready: true, restart count 0
May 16 13:19:54.599: INFO: 	Container kube-flannel ready: true, restart count 0
May 16 13:19:54.599: INFO: webterminal-85f9f78784-srg8n from webterminal started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.599: INFO: 	Container webterminal ready: true, restart count 0
May 16 13:19:54.599: INFO: tiller-deploy-796c9d7db6-hwwqm from kube-system started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.599: INFO: 	Container tiller ready: true, restart count 0
May 16 13:19:54.599: INFO: velero-5bcdd86b7c-2qd27 from velero started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.599: INFO: 	Container velero ready: true, restart count 0
May 16 13:19:54.599: INFO: sonobuoy-systemd-logs-daemon-set-8c9cd2b50a37440b-rvwgr from heptio-sonobuoy started at 2019-05-16 11:50:40 +0000 UTC (2 container statuses recorded)
May 16 13:19:54.599: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 16 13:19:54.599: INFO: 	Container systemd-logs ready: true, restart count 1
May 16 13:19:54.599: INFO: coredns-6bd858f7c-qtwtk from kube-system started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.599: INFO: 	Container coredns ready: true, restart count 0
May 16 13:19:54.599: INFO: openvpn-client-5bbcf59684-hk8ll from kube-system started at 2019-05-16 11:44:48 +0000 UTC (2 container statuses recorded)
May 16 13:19:54.599: INFO: 	Container dnat-controller ready: true, restart count 0
May 16 13:19:54.599: INFO: 	Container openvpn-client ready: true, restart count 1
May 16 13:19:54.599: INFO: kubernetes-dashboard-57b5ff8798-8ts2c from kube-system started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.599: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May 16 13:19:54.599: INFO: restic-mx74p from velero started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.599: INFO: 	Container velero ready: true, restart count 0
May 16 13:19:54.599: INFO: cluster-autoscaler-c8c56b5d9-2clck from kube-system started at 2019-05-16 11:44:48 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.599: INFO: 	Container cluster-autoscaler ready: true, restart count 1
May 16 13:19:54.599: INFO: 
Logging pods the kubelet thinks is on node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u before test
May 16 13:19:54.646: INFO: canal-vdgj7 from kube-system started at 2019-05-16 11:43:57 +0000 UTC (3 container statuses recorded)
May 16 13:19:54.646: INFO: 	Container calico-node ready: true, restart count 0
May 16 13:19:54.646: INFO: 	Container install-cni ready: true, restart count 0
May 16 13:19:54.646: INFO: 	Container kube-flannel ready: true, restart count 0
May 16 13:19:54.646: INFO: sonobuoy-systemd-logs-daemon-set-8c9cd2b50a37440b-5xwn7 from heptio-sonobuoy started at 2019-05-16 11:50:39 +0000 UTC (2 container statuses recorded)
May 16 13:19:54.646: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 16 13:19:54.646: INFO: 	Container systemd-logs ready: true, restart count 1
May 16 13:19:54.646: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-16 11:50:23 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.646: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 16 13:19:54.646: INFO: sonobuoy-e2e-job-d26d0e4387684e3e from heptio-sonobuoy started at 2019-05-16 11:50:39 +0000 UTC (2 container statuses recorded)
May 16 13:19:54.646: INFO: 	Container e2e ready: true, restart count 0
May 16 13:19:54.646: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 13:19:54.646: INFO: restic-fvvxh from velero started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.646: INFO: 	Container velero ready: true, restart count 0
May 16 13:19:54.646: INFO: node-exporter-x2gxg from kube-system started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.646: INFO: 	Container node-exporter ready: true, restart count 0
May 16 13:19:54.646: INFO: kube-proxy-wrwbt from kube-system started at 2019-05-16 11:43:57 +0000 UTC (1 container statuses recorded)
May 16 13:19:54.646: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
STEP: verifying the node has the label node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u
May 16 13:19:54.752: INFO: Pod sonobuoy requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u
May 16 13:19:54.752: INFO: Pod sonobuoy-e2e-job-d26d0e4387684e3e requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u
May 16 13:19:54.752: INFO: Pod sonobuoy-systemd-logs-daemon-set-8c9cd2b50a37440b-5xwn7 requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u
May 16 13:19:54.752: INFO: Pod sonobuoy-systemd-logs-daemon-set-8c9cd2b50a37440b-rvwgr requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
May 16 13:19:54.752: INFO: Pod canal-fvnrd requesting resource cpu=350m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
May 16 13:19:54.752: INFO: Pod canal-vdgj7 requesting resource cpu=350m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u
May 16 13:19:54.752: INFO: Pod cluster-autoscaler-c8c56b5d9-2clck requesting resource cpu=10m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
May 16 13:19:54.752: INFO: Pod coredns-6bd858f7c-mrns8 requesting resource cpu=100m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
May 16 13:19:54.752: INFO: Pod coredns-6bd858f7c-qtwtk requesting resource cpu=100m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
May 16 13:19:54.752: INFO: Pod kube-proxy-snhz7 requesting resource cpu=75m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
May 16 13:19:54.752: INFO: Pod kube-proxy-wrwbt requesting resource cpu=75m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u
May 16 13:19:54.752: INFO: Pod kubernetes-dashboard-57b5ff8798-8ts2c requesting resource cpu=50m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
May 16 13:19:54.752: INFO: Pod node-exporter-7mblb requesting resource cpu=3m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
May 16 13:19:54.752: INFO: Pod node-exporter-x2gxg requesting resource cpu=3m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u
May 16 13:19:54.752: INFO: Pod openvpn-client-5bbcf59684-hk8ll requesting resource cpu=30m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
May 16 13:19:54.752: INFO: Pod tiller-deploy-796c9d7db6-hwwqm requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
May 16 13:19:54.752: INFO: Pod restic-fvvxh requesting resource cpu=5m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u
May 16 13:19:54.752: INFO: Pod restic-mx74p requesting resource cpu=5m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
May 16 13:19:54.752: INFO: Pod velero-5bcdd86b7c-2qd27 requesting resource cpu=10m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
May 16 13:19:54.752: INFO: Pod webterminal-85f9f78784-srg8n requesting resource cpu=0m on Node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b5f5590-77dd-11e9-8275-42209be91bd2.159f2c61e2f8579d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2022/filler-pod-4b5f5590-77dd-11e9-8275-42209be91bd2 to machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b5f5590-77dd-11e9-8275-42209be91bd2.159f2c624f454a29], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b5f5590-77dd-11e9-8275-42209be91bd2.159f2c62608724de], Reason = [Created], Message = [Created container filler-pod-4b5f5590-77dd-11e9-8275-42209be91bd2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b5f5590-77dd-11e9-8275-42209be91bd2.159f2c627d1f8ca5], Reason = [Started], Message = [Started container filler-pod-4b5f5590-77dd-11e9-8275-42209be91bd2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b62f2d5-77dd-11e9-8275-42209be91bd2.159f2c61e3029883], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2022/filler-pod-4b62f2d5-77dd-11e9-8275-42209be91bd2 to machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b62f2d5-77dd-11e9-8275-42209be91bd2.159f2c6239642f82], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b62f2d5-77dd-11e9-8275-42209be91bd2.159f2c6248efa106], Reason = [Created], Message = [Created container filler-pod-4b62f2d5-77dd-11e9-8275-42209be91bd2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b62f2d5-77dd-11e9-8275-42209be91bd2.159f2c625dc94a70], Reason = [Started], Message = [Started container filler-pod-4b62f2d5-77dd-11e9-8275-42209be91bd2]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.159f2c62d47e677c], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:19:59.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2022" for this suite.
May 16 13:20:06.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:20:06.381: INFO: namespace sched-pred-2022 deletion completed in 6.389531303s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:12.109 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:20:06.381: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:20:10.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4387" for this suite.
May 16 13:20:50.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:20:51.101: INFO: namespace kubelet-test-4387 deletion completed in 40.40781225s

• [SLOW TEST:44.720 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:20:51.102: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
May 16 13:20:51.289: INFO: Waiting up to 5m0s for pod "client-containers-6d0d0187-77dd-11e9-8275-42209be91bd2" in namespace "containers-5416" to be "success or failure"
May 16 13:20:51.295: INFO: Pod "client-containers-6d0d0187-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.74237ms
May 16 13:20:53.304: INFO: Pod "client-containers-6d0d0187-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014599201s
May 16 13:20:55.310: INFO: Pod "client-containers-6d0d0187-77dd-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021039822s
STEP: Saw pod success
May 16 13:20:55.310: INFO: Pod "client-containers-6d0d0187-77dd-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:20:55.317: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod client-containers-6d0d0187-77dd-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 13:20:55.410: INFO: Waiting for pod client-containers-6d0d0187-77dd-11e9-8275-42209be91bd2 to disappear
May 16 13:20:55.419: INFO: Pod client-containers-6d0d0187-77dd-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:20:55.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5416" for this suite.
May 16 13:21:01.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:21:01.784: INFO: namespace containers-5416 deletion completed in 6.356417382s

• [SLOW TEST:10.682 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:21:01.787: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 13:21:02.015: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"73969468-77dd-11e9-b44f-0a580af42602", Controller:(*bool)(0xc001437fc6), BlockOwnerDeletion:(*bool)(0xc001437fc7)}}
May 16 13:21:02.052: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"738e3161-77dd-11e9-b44f-0a580af42602", Controller:(*bool)(0xc002fdfc9e), BlockOwnerDeletion:(*bool)(0xc002fdfc9f)}}
May 16 13:21:02.071: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"7394b520-77dd-11e9-b44f-0a580af42602", Controller:(*bool)(0xc002fdfe36), BlockOwnerDeletion:(*bool)(0xc002fdfe37)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:21:07.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5024" for this suite.
May 16 13:21:13.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:21:13.617: INFO: namespace gc-5024 deletion completed in 6.435604949s

• [SLOW TEST:11.830 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:21:13.619: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 16 13:21:13.774: INFO: Waiting up to 5m0s for pod "pod-7a71b616-77dd-11e9-8275-42209be91bd2" in namespace "emptydir-2859" to be "success or failure"
May 16 13:21:13.780: INFO: Pod "pod-7a71b616-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.259756ms
May 16 13:21:15.799: INFO: Pod "pod-7a71b616-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025610649s
May 16 13:21:17.807: INFO: Pod "pod-7a71b616-77dd-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033650535s
STEP: Saw pod success
May 16 13:21:17.808: INFO: Pod "pod-7a71b616-77dd-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:21:17.813: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-7a71b616-77dd-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 13:21:17.899: INFO: Waiting for pod pod-7a71b616-77dd-11e9-8275-42209be91bd2 to disappear
May 16 13:21:17.905: INFO: Pod pod-7a71b616-77dd-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:21:17.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2859" for this suite.
May 16 13:21:23.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:21:24.320: INFO: namespace emptydir-2859 deletion completed in 6.408215265s

• [SLOW TEST:10.702 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:21:24.321: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 16 13:21:29.524: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:21:30.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8006" for this suite.
May 16 13:21:54.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:21:55.005: INFO: namespace replicaset-8006 deletion completed in 24.412677742s

• [SLOW TEST:30.685 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:21:55.006: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-931ddf29-77dd-11e9-8275-42209be91bd2
STEP: Creating secret with name s-test-opt-upd-931ddf6d-77dd-11e9-8275-42209be91bd2
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-931ddf29-77dd-11e9-8275-42209be91bd2
STEP: Updating secret s-test-opt-upd-931ddf6d-77dd-11e9-8275-42209be91bd2
STEP: Creating secret with name s-test-opt-create-931ddf86-77dd-11e9-8275-42209be91bd2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:23:25.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2992" for this suite.
May 16 13:23:49.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:23:49.470: INFO: namespace projected-2992 deletion completed in 24.41197423s

• [SLOW TEST:114.465 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:23:49.472: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 16 13:23:54.286: INFO: Successfully updated pod "labelsupdated7620712-77dd-11e9-8275-42209be91bd2"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:23:56.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5906" for this suite.
May 16 13:24:20.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:24:20.691: INFO: namespace downward-api-5906 deletion completed in 24.348772272s

• [SLOW TEST:31.220 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:24:20.692: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-e9f872e4-77dd-11e9-8275-42209be91bd2
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:24:20.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-11" for this suite.
May 16 13:24:26.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:24:27.197: INFO: namespace configmap-11 deletion completed in 6.352335981s

• [SLOW TEST:6.505 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:24:27.201: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
May 16 13:24:27.308: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-2730" to be "success or failure"
May 16 13:24:27.315: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.953154ms
May 16 13:24:29.324: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014912948s
May 16 13:24:31.333: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024296983s
STEP: Saw pod success
May 16 13:24:31.333: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 16 13:24:31.398: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 16 13:24:31.471: INFO: Waiting for pod pod-host-path-test to disappear
May 16 13:24:31.479: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:24:31.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-2730" for this suite.
May 16 13:24:37.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:24:37.855: INFO: namespace hostpath-2730 deletion completed in 6.369006054s

• [SLOW TEST:10.654 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:24:37.855: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 16 13:24:38.022: INFO: Waiting up to 5m0s for pod "pod-f4327648-77dd-11e9-8275-42209be91bd2" in namespace "emptydir-5607" to be "success or failure"
May 16 13:24:38.028: INFO: Pod "pod-f4327648-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.529153ms
May 16 13:24:40.037: INFO: Pod "pod-f4327648-77dd-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014338004s
May 16 13:24:42.043: INFO: Pod "pod-f4327648-77dd-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020507003s
STEP: Saw pod success
May 16 13:24:42.043: INFO: Pod "pod-f4327648-77dd-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:24:42.095: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-f4327648-77dd-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 13:24:42.284: INFO: Waiting for pod pod-f4327648-77dd-11e9-8275-42209be91bd2 to disappear
May 16 13:24:42.292: INFO: Pod pod-f4327648-77dd-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:24:42.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5607" for this suite.
May 16 13:24:48.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:24:48.770: INFO: namespace emptydir-5607 deletion completed in 6.453336531s

• [SLOW TEST:10.915 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:24:48.770: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 16 13:24:57.056: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:24:57.069: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:24:59.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:24:59.083: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:01.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:01.077: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:03.070: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:03.079: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:05.070: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:05.076: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:07.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:07.076: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:09.070: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:09.077: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:11.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:11.077: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:13.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:13.076: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:15.070: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:15.134: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:17.070: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:17.082: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:19.070: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:19.083: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:21.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:21.077: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:23.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:23.077: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:25.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:25.077: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 13:25:27.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 13:25:27.082: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:25:27.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7235" for this suite.
May 16 13:25:51.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:25:51.652: INFO: namespace container-lifecycle-hook-7235 deletion completed in 24.5231038s

• [SLOW TEST:62.882 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:25:51.654: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-202fca83-77de-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 13:25:51.830: INFO: Waiting up to 5m0s for pod "pod-secrets-20314253-77de-11e9-8275-42209be91bd2" in namespace "secrets-617" to be "success or failure"
May 16 13:25:51.837: INFO: Pod "pod-secrets-20314253-77de-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.308891ms
May 16 13:25:53.845: INFO: Pod "pod-secrets-20314253-77de-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015273394s
May 16 13:25:55.863: INFO: Pod "pod-secrets-20314253-77de-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032989977s
STEP: Saw pod success
May 16 13:25:55.863: INFO: Pod "pod-secrets-20314253-77de-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:25:55.869: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-secrets-20314253-77de-11e9-8275-42209be91bd2 container secret-volume-test: <nil>
STEP: delete the pod
May 16 13:25:55.924: INFO: Waiting for pod pod-secrets-20314253-77de-11e9-8275-42209be91bd2 to disappear
May 16 13:25:55.935: INFO: Pod pod-secrets-20314253-77de-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:25:55.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-617" for this suite.
May 16 13:26:01.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:26:02.322: INFO: namespace secrets-617 deletion completed in 6.378581097s

• [SLOW TEST:10.668 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:26:02.324: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 13:26:02.478: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 16 13:26:07.485: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 16 13:26:07.485: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 16 13:26:09.491: INFO: Creating deployment "test-rollover-deployment"
May 16 13:26:09.517: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 16 13:26:11.538: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 16 13:26:11.551: INFO: Ensure that both replica sets have 1 created replica
May 16 13:26:11.561: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 16 13:26:11.614: INFO: Updating deployment test-rollover-deployment
May 16 13:26:11.614: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 16 13:26:13.647: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 16 13:26:13.666: INFO: Make sure deployment "test-rollover-deployment" is complete
May 16 13:26:13.679: INFO: all replica sets need to contain the pod-template-hash label
May 16 13:26:13.679: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609970, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609970, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609972, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609969, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 13:26:15.692: INFO: all replica sets need to contain the pod-template-hash label
May 16 13:26:15.693: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609970, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609970, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609974, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609969, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 13:26:17.694: INFO: all replica sets need to contain the pod-template-hash label
May 16 13:26:17.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609970, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609970, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609974, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609969, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 13:26:19.696: INFO: all replica sets need to contain the pod-template-hash label
May 16 13:26:19.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609970, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609970, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609974, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609969, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 13:26:21.698: INFO: all replica sets need to contain the pod-template-hash label
May 16 13:26:21.698: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609970, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609970, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609974, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609969, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 13:26:23.694: INFO: all replica sets need to contain the pod-template-hash label
May 16 13:26:23.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609970, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609970, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609974, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693609969, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 13:26:25.692: INFO: 
May 16 13:26:25.692: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 16 13:26:25.709: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-4799,SelfLink:/apis/apps/v1/namespaces/deployment-4799/deployments/test-rollover-deployment,UID:2aee9ee1-77de-11e9-b44f-0a580af42602,ResourceVersion:30781,Generation:2,CreationTimestamp:2019-05-16 13:26:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-16 13:26:10 +0000 UTC 2019-05-16 13:26:10 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-16 13:26:24 +0000 UTC 2019-05-16 13:26:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 16 13:26:25.715: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-4799,SelfLink:/apis/apps/v1/namespaces/deployment-4799/replicasets/test-rollover-deployment-766b4d6c9d,UID:2c26401e-77de-11e9-b44f-0a580af42602,ResourceVersion:30770,Generation:2,CreationTimestamp:2019-05-16 13:26:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 2aee9ee1-77de-11e9-b44f-0a580af42602 0xc0032b0bc7 0xc0032b0bc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 16 13:26:25.716: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 16 13:26:25.716: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-4799,SelfLink:/apis/apps/v1/namespaces/deployment-4799/replicasets/test-rollover-controller,UID:26bd59d8-77de-11e9-b44f-0a580af42602,ResourceVersion:30780,Generation:2,CreationTimestamp:2019-05-16 13:26:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 2aee9ee1-77de-11e9-b44f-0a580af42602 0xc0032b0a17 0xc0032b0a18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 16 13:26:25.716: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-4799,SelfLink:/apis/apps/v1/namespaces/deployment-4799/replicasets/test-rollover-deployment-6455657675,UID:2af65262-77de-11e9-b44f-0a580af42602,ResourceVersion:30712,Generation:2,CreationTimestamp:2019-05-16 13:26:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 2aee9ee1-77de-11e9-b44f-0a580af42602 0xc0032b0ae7 0xc0032b0ae8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 16 13:26:25.724: INFO: Pod "test-rollover-deployment-766b4d6c9d-vxj4v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-vxj4v,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-4799,SelfLink:/api/v1/namespaces/deployment-4799/pods/test-rollover-deployment-766b4d6c9d-vxj4v,UID:2c3c2995-77de-11e9-b44f-0a580af42602,ResourceVersion:30737,Generation:0,CreationTimestamp:2019-05-16 13:26:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 2c26401e-77de-11e9-b44f-0a580af42602 0xc0032b16e7 0xc0032b16e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-225lp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-225lp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-225lp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032b1750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032b1770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:26:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:26:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:26:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-16 13:26:12 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.0.224,StartTime:2019-05-16 13:26:11 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-16 13:26:14 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://093bed2e681d6b178aaeac92ae7149c3bd155ef5f457571201af46753cd6c8d1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:26:25.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4799" for this suite.
May 16 13:26:33.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:26:34.098: INFO: namespace deployment-4799 deletion completed in 8.367328697s

• [SLOW TEST:31.774 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:26:34.098: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-7048
May 16 13:26:38.239: INFO: Started pod liveness-http in namespace container-probe-7048
STEP: checking the pod's current state and verifying that restartCount is present
May 16 13:26:38.251: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:30:39.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7048" for this suite.
May 16 13:30:45.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:30:46.103: INFO: namespace container-probe-7048 deletion completed in 6.37391964s

• [SLOW TEST:252.004 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:30:46.103: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 13:30:46.203: INFO: Creating ReplicaSet my-hostname-basic-cfaad2cb-77de-11e9-8275-42209be91bd2
May 16 13:30:46.231: INFO: Pod name my-hostname-basic-cfaad2cb-77de-11e9-8275-42209be91bd2: Found 0 pods out of 1
May 16 13:30:51.242: INFO: Pod name my-hostname-basic-cfaad2cb-77de-11e9-8275-42209be91bd2: Found 1 pods out of 1
May 16 13:30:51.242: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-cfaad2cb-77de-11e9-8275-42209be91bd2" is running
May 16 13:30:51.248: INFO: Pod "my-hostname-basic-cfaad2cb-77de-11e9-8275-42209be91bd2-5vkpr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-16 13:30:46 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-16 13:30:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-16 13:30:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-16 13:30:46 +0000 UTC Reason: Message:}])
May 16 13:30:51.248: INFO: Trying to dial the pod
May 16 13:30:56.366: INFO: Controller my-hostname-basic-cfaad2cb-77de-11e9-8275-42209be91bd2: Got expected result from replica 1 [my-hostname-basic-cfaad2cb-77de-11e9-8275-42209be91bd2-5vkpr]: "my-hostname-basic-cfaad2cb-77de-11e9-8275-42209be91bd2-5vkpr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:30:56.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5628" for this suite.
May 16 13:31:02.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:31:02.758: INFO: namespace replicaset-5628 deletion completed in 6.383255196s

• [SLOW TEST:16.655 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:31:02.759: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 16 13:31:02.908: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4069,SelfLink:/api/v1/namespaces/watch-4069/configmaps/e2e-watch-test-configmap-a,UID:d98c4484-77de-11e9-b44f-0a580af42602,ResourceVersion:31715,Generation:0,CreationTimestamp:2019-05-16 13:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 16 13:31:02.908: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4069,SelfLink:/api/v1/namespaces/watch-4069/configmaps/e2e-watch-test-configmap-a,UID:d98c4484-77de-11e9-b44f-0a580af42602,ResourceVersion:31715,Generation:0,CreationTimestamp:2019-05-16 13:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 16 13:31:12.927: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4069,SelfLink:/api/v1/namespaces/watch-4069/configmaps/e2e-watch-test-configmap-a,UID:d98c4484-77de-11e9-b44f-0a580af42602,ResourceVersion:31745,Generation:0,CreationTimestamp:2019-05-16 13:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 16 13:31:12.927: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4069,SelfLink:/api/v1/namespaces/watch-4069/configmaps/e2e-watch-test-configmap-a,UID:d98c4484-77de-11e9-b44f-0a580af42602,ResourceVersion:31745,Generation:0,CreationTimestamp:2019-05-16 13:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 16 13:31:22.949: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4069,SelfLink:/api/v1/namespaces/watch-4069/configmaps/e2e-watch-test-configmap-a,UID:d98c4484-77de-11e9-b44f-0a580af42602,ResourceVersion:31777,Generation:0,CreationTimestamp:2019-05-16 13:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 16 13:31:22.949: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4069,SelfLink:/api/v1/namespaces/watch-4069/configmaps/e2e-watch-test-configmap-a,UID:d98c4484-77de-11e9-b44f-0a580af42602,ResourceVersion:31777,Generation:0,CreationTimestamp:2019-05-16 13:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 16 13:31:32.964: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4069,SelfLink:/api/v1/namespaces/watch-4069/configmaps/e2e-watch-test-configmap-a,UID:d98c4484-77de-11e9-b44f-0a580af42602,ResourceVersion:31809,Generation:0,CreationTimestamp:2019-05-16 13:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 16 13:31:32.964: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4069,SelfLink:/api/v1/namespaces/watch-4069/configmaps/e2e-watch-test-configmap-a,UID:d98c4484-77de-11e9-b44f-0a580af42602,ResourceVersion:31809,Generation:0,CreationTimestamp:2019-05-16 13:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 16 13:31:42.984: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4069,SelfLink:/api/v1/namespaces/watch-4069/configmaps/e2e-watch-test-configmap-b,UID:f1725eb8-77de-11e9-b44f-0a580af42602,ResourceVersion:31840,Generation:0,CreationTimestamp:2019-05-16 13:31:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 16 13:31:42.984: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4069,SelfLink:/api/v1/namespaces/watch-4069/configmaps/e2e-watch-test-configmap-b,UID:f1725eb8-77de-11e9-b44f-0a580af42602,ResourceVersion:31840,Generation:0,CreationTimestamp:2019-05-16 13:31:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 16 13:31:53.011: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4069,SelfLink:/api/v1/namespaces/watch-4069/configmaps/e2e-watch-test-configmap-b,UID:f1725eb8-77de-11e9-b44f-0a580af42602,ResourceVersion:31871,Generation:0,CreationTimestamp:2019-05-16 13:31:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 16 13:31:53.011: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4069,SelfLink:/api/v1/namespaces/watch-4069/configmaps/e2e-watch-test-configmap-b,UID:f1725eb8-77de-11e9-b44f-0a580af42602,ResourceVersion:31871,Generation:0,CreationTimestamp:2019-05-16 13:31:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:32:03.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4069" for this suite.
May 16 13:32:09.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:32:09.378: INFO: namespace watch-4069 deletion completed in 6.35876611s

• [SLOW TEST:66.619 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:32:09.379: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 16 13:32:09.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4804'
May 16 13:32:10.821: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 16 13:32:10.821: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
May 16 13:32:10.848: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-qc6jj]
May 16 13:32:10.848: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-qc6jj" in namespace "kubectl-4804" to be "running and ready"
May 16 13:32:10.869: INFO: Pod "e2e-test-nginx-rc-qc6jj": Phase="Pending", Reason="", readiness=false. Elapsed: 20.555573ms
May 16 13:32:12.875: INFO: Pod "e2e-test-nginx-rc-qc6jj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027224286s
May 16 13:32:14.883: INFO: Pod "e2e-test-nginx-rc-qc6jj": Phase="Running", Reason="", readiness=true. Elapsed: 4.034933015s
May 16 13:32:14.883: INFO: Pod "e2e-test-nginx-rc-qc6jj" satisfied condition "running and ready"
May 16 13:32:14.883: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-qc6jj]
May 16 13:32:14.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 logs rc/e2e-test-nginx-rc --namespace=kubectl-4804'
May 16 13:32:15.015: INFO: stderr: ""
May 16 13:32:15.015: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
May 16 13:32:15.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete rc e2e-test-nginx-rc --namespace=kubectl-4804'
May 16 13:32:15.187: INFO: stderr: ""
May 16 13:32:15.187: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:32:15.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4804" for this suite.
May 16 13:32:21.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:32:21.606: INFO: namespace kubectl-4804 deletion completed in 6.409181758s

• [SLOW TEST:12.227 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:32:21.608: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-089a70d2-77df-11e9-8275-42209be91bd2
STEP: Creating configMap with name cm-test-opt-upd-089a712a-77df-11e9-8275-42209be91bd2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-089a70d2-77df-11e9-8275-42209be91bd2
STEP: Updating configmap cm-test-opt-upd-089a712a-77df-11e9-8275-42209be91bd2
STEP: Creating configMap with name cm-test-opt-create-089a7147-77df-11e9-8275-42209be91bd2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:32:28.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5849" for this suite.
May 16 13:32:52.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:32:52.991: INFO: namespace configmap-5849 deletion completed in 24.303616919s

• [SLOW TEST:31.384 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:32:52.992: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
May 16 13:32:53.134: INFO: Waiting up to 5m0s for pod "var-expansion-1b4f9bd3-77df-11e9-8275-42209be91bd2" in namespace "var-expansion-1068" to be "success or failure"
May 16 13:32:53.141: INFO: Pod "var-expansion-1b4f9bd3-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.419798ms
May 16 13:32:55.151: INFO: Pod "var-expansion-1b4f9bd3-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01758256s
May 16 13:32:57.164: INFO: Pod "var-expansion-1b4f9bd3-77df-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030545019s
STEP: Saw pod success
May 16 13:32:57.164: INFO: Pod "var-expansion-1b4f9bd3-77df-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:32:57.173: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod var-expansion-1b4f9bd3-77df-11e9-8275-42209be91bd2 container dapi-container: <nil>
STEP: delete the pod
May 16 13:32:57.226: INFO: Waiting for pod var-expansion-1b4f9bd3-77df-11e9-8275-42209be91bd2 to disappear
May 16 13:32:57.233: INFO: Pod var-expansion-1b4f9bd3-77df-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:32:57.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1068" for this suite.
May 16 13:33:03.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:33:03.601: INFO: namespace var-expansion-1068 deletion completed in 6.352038237s

• [SLOW TEST:10.609 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:33:03.602: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0516 13:33:43.785705      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 16 13:33:43.785: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:33:43.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6832" for this suite.
May 16 13:33:51.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:33:52.185: INFO: namespace gc-6832 deletion completed in 8.393315404s

• [SLOW TEST:48.583 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:33:52.185: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-3e9577f9-77df-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 13:33:52.324: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3e972782-77df-11e9-8275-42209be91bd2" in namespace "projected-8234" to be "success or failure"
May 16 13:33:52.355: INFO: Pod "pod-projected-configmaps-3e972782-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 30.638703ms
May 16 13:33:54.367: INFO: Pod "pod-projected-configmaps-3e972782-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042802694s
May 16 13:33:56.376: INFO: Pod "pod-projected-configmaps-3e972782-77df-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051270089s
STEP: Saw pod success
May 16 13:33:56.376: INFO: Pod "pod-projected-configmaps-3e972782-77df-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:33:56.468: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-projected-configmaps-3e972782-77df-11e9-8275-42209be91bd2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 16 13:33:56.627: INFO: Waiting for pod pod-projected-configmaps-3e972782-77df-11e9-8275-42209be91bd2 to disappear
May 16 13:33:56.642: INFO: Pod pod-projected-configmaps-3e972782-77df-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:33:56.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8234" for this suite.
May 16 13:34:02.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:34:03.004: INFO: namespace projected-8234 deletion completed in 6.353504961s

• [SLOW TEST:10.820 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:34:03.009: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
May 16 13:34:03.693: INFO: created pod pod-service-account-defaultsa
May 16 13:34:03.693: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 16 13:34:03.721: INFO: created pod pod-service-account-mountsa
May 16 13:34:03.721: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 16 13:34:03.730: INFO: created pod pod-service-account-nomountsa
May 16 13:34:03.730: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 16 13:34:03.769: INFO: created pod pod-service-account-defaultsa-mountspec
May 16 13:34:03.769: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 16 13:34:03.800: INFO: created pod pod-service-account-mountsa-mountspec
May 16 13:34:03.800: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 16 13:34:03.829: INFO: created pod pod-service-account-nomountsa-mountspec
May 16 13:34:03.829: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 16 13:34:03.859: INFO: created pod pod-service-account-defaultsa-nomountspec
May 16 13:34:03.859: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 16 13:34:03.935: INFO: created pod pod-service-account-mountsa-nomountspec
May 16 13:34:03.935: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 16 13:34:03.967: INFO: created pod pod-service-account-nomountsa-nomountspec
May 16 13:34:03.967: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:34:03.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5610" for this suite.
May 16 13:34:27.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:34:28.399: INFO: namespace svcaccounts-5610 deletion completed in 24.423536867s

• [SLOW TEST:25.391 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:34:28.404: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
May 16 13:34:28.673: INFO: Waiting up to 5m0s for pod "pod-5437fd88-77df-11e9-8275-42209be91bd2" in namespace "emptydir-2308" to be "success or failure"
May 16 13:34:28.721: INFO: Pod "pod-5437fd88-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 47.784104ms
May 16 13:34:30.739: INFO: Pod "pod-5437fd88-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065860482s
May 16 13:34:32.755: INFO: Pod "pod-5437fd88-77df-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081260445s
STEP: Saw pod success
May 16 13:34:32.755: INFO: Pod "pod-5437fd88-77df-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:34:32.765: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-5437fd88-77df-11e9-8275-42209be91bd2 container test-container: <nil>
STEP: delete the pod
May 16 13:34:32.916: INFO: Waiting for pod pod-5437fd88-77df-11e9-8275-42209be91bd2 to disappear
May 16 13:34:32.921: INFO: Pod pod-5437fd88-77df-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:34:32.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2308" for this suite.
May 16 13:34:38.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:34:39.402: INFO: namespace emptydir-2308 deletion completed in 6.464216306s

• [SLOW TEST:10.998 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:34:39.404: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2793
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 16 13:34:39.551: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 16 13:35:01.869: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.90:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2793 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 13:35:01.869: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 13:35:02.390: INFO: Found all expected endpoints: [netserver-0]
May 16 13:35:02.397: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.243:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2793 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 16 13:35:02.397: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
May 16 13:35:03.033: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:35:03.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2793" for this suite.
May 16 13:35:27.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:35:27.401: INFO: namespace pod-network-test-2793 deletion completed in 24.358589082s

• [SLOW TEST:47.997 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:35:27.406: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
May 16 13:35:27.605: INFO: Waiting up to 5m0s for pod "var-expansion-775f64e2-77df-11e9-8275-42209be91bd2" in namespace "var-expansion-9364" to be "success or failure"
May 16 13:35:27.616: INFO: Pod "var-expansion-775f64e2-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.427162ms
May 16 13:35:29.637: INFO: Pod "var-expansion-775f64e2-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031599568s
May 16 13:35:31.644: INFO: Pod "var-expansion-775f64e2-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038520587s
May 16 13:35:33.655: INFO: Pod "var-expansion-775f64e2-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050019339s
May 16 13:35:35.671: INFO: Pod "var-expansion-775f64e2-77df-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.066193427s
STEP: Saw pod success
May 16 13:35:35.672: INFO: Pod "var-expansion-775f64e2-77df-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:35:35.683: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod var-expansion-775f64e2-77df-11e9-8275-42209be91bd2 container dapi-container: <nil>
STEP: delete the pod
May 16 13:35:35.755: INFO: Waiting for pod var-expansion-775f64e2-77df-11e9-8275-42209be91bd2 to disappear
May 16 13:35:35.763: INFO: Pod var-expansion-775f64e2-77df-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:35:35.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9364" for this suite.
May 16 13:35:41.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:35:42.459: INFO: namespace var-expansion-9364 deletion completed in 6.683958531s

• [SLOW TEST:15.055 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:35:42.461: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-9020
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9020 to expose endpoints map[]
May 16 13:35:42.626: INFO: Get endpoints failed (15.27057ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
May 16 13:35:43.634: INFO: successfully validated that service endpoint-test2 in namespace services-9020 exposes endpoints map[] (1.023571866s elapsed)
STEP: Creating pod pod1 in namespace services-9020
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9020 to expose endpoints map[pod1:[80]]
May 16 13:35:46.771: INFO: successfully validated that service endpoint-test2 in namespace services-9020 exposes endpoints map[pod1:[80]] (3.089884664s elapsed)
STEP: Creating pod pod2 in namespace services-9020
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9020 to expose endpoints map[pod1:[80] pod2:[80]]
May 16 13:35:49.945: INFO: successfully validated that service endpoint-test2 in namespace services-9020 exposes endpoints map[pod1:[80] pod2:[80]] (3.147862668s elapsed)
STEP: Deleting pod pod1 in namespace services-9020
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9020 to expose endpoints map[pod2:[80]]
May 16 13:35:49.997: INFO: successfully validated that service endpoint-test2 in namespace services-9020 exposes endpoints map[pod2:[80]] (19.30405ms elapsed)
STEP: Deleting pod pod2 in namespace services-9020
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9020 to expose endpoints map[]
May 16 13:35:51.077: INFO: successfully validated that service endpoint-test2 in namespace services-9020 exposes endpoints map[] (1.051235072s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:35:51.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9020" for this suite.
May 16 13:36:15.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:36:15.543: INFO: namespace services-9020 deletion completed in 24.372921295s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:33.082 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:36:15.544: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 16 13:36:15.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-5519'
May 16 13:36:15.972: INFO: stderr: ""
May 16 13:36:15.972: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 16 13:36:15.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5519'
May 16 13:36:16.089: INFO: stderr: ""
May 16 13:36:16.089: INFO: stdout: "update-demo-nautilus-tpnjs update-demo-nautilus-tx6cf "
May 16 13:36:16.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-tpnjs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5519'
May 16 13:36:16.199: INFO: stderr: ""
May 16 13:36:16.199: INFO: stdout: ""
May 16 13:36:16.199: INFO: update-demo-nautilus-tpnjs is created but not running
May 16 13:36:21.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5519'
May 16 13:36:21.336: INFO: stderr: ""
May 16 13:36:21.336: INFO: stdout: "update-demo-nautilus-tpnjs update-demo-nautilus-tx6cf "
May 16 13:36:21.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-tpnjs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5519'
May 16 13:36:21.462: INFO: stderr: ""
May 16 13:36:21.462: INFO: stdout: "true"
May 16 13:36:21.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-tpnjs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5519'
May 16 13:36:21.571: INFO: stderr: ""
May 16 13:36:21.571: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 16 13:36:21.571: INFO: validating pod update-demo-nautilus-tpnjs
May 16 13:36:21.712: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 13:36:21.712: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 13:36:21.712: INFO: update-demo-nautilus-tpnjs is verified up and running
May 16 13:36:21.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-tx6cf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5519'
May 16 13:36:21.809: INFO: stderr: ""
May 16 13:36:21.809: INFO: stdout: "true"
May 16 13:36:21.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods update-demo-nautilus-tx6cf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5519'
May 16 13:36:21.930: INFO: stderr: ""
May 16 13:36:21.930: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 16 13:36:21.930: INFO: validating pod update-demo-nautilus-tx6cf
May 16 13:36:22.068: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 13:36:22.068: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 13:36:22.068: INFO: update-demo-nautilus-tx6cf is verified up and running
STEP: using delete to clean up resources
May 16 13:36:22.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete --grace-period=0 --force -f - --namespace=kubectl-5519'
May 16 13:36:22.172: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 13:36:22.172: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 16 13:36:22.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5519'
May 16 13:36:22.284: INFO: stderr: "No resources found.\n"
May 16 13:36:22.284: INFO: stdout: ""
May 16 13:36:22.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -l name=update-demo --namespace=kubectl-5519 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 16 13:36:22.390: INFO: stderr: ""
May 16 13:36:22.390: INFO: stdout: "update-demo-nautilus-tpnjs\nupdate-demo-nautilus-tx6cf\n"
May 16 13:36:22.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5519'
May 16 13:36:23.026: INFO: stderr: "No resources found.\n"
May 16 13:36:23.026: INFO: stdout: ""
May 16 13:36:23.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -l name=update-demo --namespace=kubectl-5519 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 16 13:36:23.135: INFO: stderr: ""
May 16 13:36:23.135: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:36:23.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5519" for this suite.
May 16 13:36:29.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:36:29.505: INFO: namespace kubectl-5519 deletion completed in 6.358903299s

• [SLOW TEST:13.961 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:36:29.507: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 16 13:36:29.609: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c57bdc3-77df-11e9-8275-42209be91bd2" in namespace "downward-api-6341" to be "success or failure"
May 16 13:36:29.615: INFO: Pod "downwardapi-volume-9c57bdc3-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.858035ms
May 16 13:36:31.633: INFO: Pod "downwardapi-volume-9c57bdc3-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023119039s
May 16 13:36:33.641: INFO: Pod "downwardapi-volume-9c57bdc3-77df-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031554054s
STEP: Saw pod success
May 16 13:36:33.641: INFO: Pod "downwardapi-volume-9c57bdc3-77df-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:36:33.647: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod downwardapi-volume-9c57bdc3-77df-11e9-8275-42209be91bd2 container client-container: <nil>
STEP: delete the pod
May 16 13:36:33.740: INFO: Waiting for pod downwardapi-volume-9c57bdc3-77df-11e9-8275-42209be91bd2 to disappear
May 16 13:36:33.745: INFO: Pod downwardapi-volume-9c57bdc3-77df-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:36:33.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6341" for this suite.
May 16 13:36:39.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:36:40.091: INFO: namespace downward-api-6341 deletion completed in 6.336091814s

• [SLOW TEST:10.584 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:36:40.091: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 16 13:36:40.263: INFO: (0) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 67.03596ms)
May 16 13:36:40.273: INFO: (1) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.562012ms)
May 16 13:36:40.362: INFO: (2) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 89.405141ms)
May 16 13:36:40.381: INFO: (3) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 19.129505ms)
May 16 13:36:40.390: INFO: (4) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.702599ms)
May 16 13:36:40.400: INFO: (5) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.179638ms)
May 16 13:36:40.410: INFO: (6) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.813948ms)
May 16 13:36:40.424: INFO: (7) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.723559ms)
May 16 13:36:40.432: INFO: (8) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.113769ms)
May 16 13:36:40.441: INFO: (9) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.75845ms)
May 16 13:36:40.450: INFO: (10) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.879831ms)
May 16 13:36:40.472: INFO: (11) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 21.826317ms)
May 16 13:36:40.485: INFO: (12) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 11.988093ms)
May 16 13:36:40.493: INFO: (13) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.12251ms)
May 16 13:36:40.503: INFO: (14) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.991272ms)
May 16 13:36:40.513: INFO: (15) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.112189ms)
May 16 13:36:40.527: INFO: (16) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 14.464677ms)
May 16 13:36:40.537: INFO: (17) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.459889ms)
May 16 13:36:40.559: INFO: (18) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 22.270785ms)
May 16 13:36:40.569: INFO: (19) /api/v1/nodes/machine-kubermatic-conformancecluster-kik2tgqc8i-5riyh/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.101689ms)
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:36:40.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1723" for this suite.
May 16 13:36:46.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:36:46.922: INFO: namespace proxy-1723 deletion completed in 6.346150105s

• [SLOW TEST:6.831 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:36:46.930: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 16 13:36:47.041: INFO: namespace kubectl-147
May 16 13:36:47.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 create -f - --namespace=kubectl-147'
May 16 13:36:47.461: INFO: stderr: ""
May 16 13:36:47.461: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 16 13:36:48.469: INFO: Selector matched 1 pods for map[app:redis]
May 16 13:36:48.469: INFO: Found 0 / 1
May 16 13:36:49.468: INFO: Selector matched 1 pods for map[app:redis]
May 16 13:36:49.468: INFO: Found 0 / 1
May 16 13:36:50.471: INFO: Selector matched 1 pods for map[app:redis]
May 16 13:36:50.471: INFO: Found 1 / 1
May 16 13:36:50.471: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 16 13:36:50.478: INFO: Selector matched 1 pods for map[app:redis]
May 16 13:36:50.478: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 16 13:36:50.478: INFO: wait on redis-master startup in kubectl-147 
May 16 13:36:50.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 logs redis-master-kssgb redis-master --namespace=kubectl-147'
May 16 13:36:50.659: INFO: stderr: ""
May 16 13:36:50.659: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 16 May 13:36:49.777 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 May 13:36:49.777 # Server started, Redis version 3.2.12\n1:M 16 May 13:36:49.777 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 May 13:36:49.777 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
May 16 13:36:50.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-147'
May 16 13:36:50.814: INFO: stderr: ""
May 16 13:36:50.814: INFO: stdout: "service/rm2 exposed\n"
May 16 13:36:50.841: INFO: Service rm2 in namespace kubectl-147 found.
STEP: exposing service
May 16 13:36:52.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-147'
May 16 13:36:52.998: INFO: stderr: ""
May 16 13:36:52.999: INFO: stdout: "service/rm3 exposed\n"
May 16 13:36:53.010: INFO: Service rm3 in namespace kubectl-147 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:36:55.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-147" for this suite.
May 16 13:37:19.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:37:19.410: INFO: namespace kubectl-147 deletion completed in 24.38183804s

• [SLOW TEST:32.481 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:37:19.412: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 16 13:37:19.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-8752'
May 16 13:37:19.631: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 16 13:37:19.631: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
May 16 13:37:19.641: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
May 16 13:37:19.651: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May 16 13:37:19.674: INFO: scanned /root for discovery docs: <nil>
May 16 13:37:19.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-8752'
May 16 13:37:35.767: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 16 13:37:35.767: INFO: stdout: "Created e2e-test-nginx-rc-efc775370d4e61921a94c5804aec4651\nScaling up e2e-test-nginx-rc-efc775370d4e61921a94c5804aec4651 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-efc775370d4e61921a94c5804aec4651 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-efc775370d4e61921a94c5804aec4651 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
May 16 13:37:35.767: INFO: stdout: "Created e2e-test-nginx-rc-efc775370d4e61921a94c5804aec4651\nScaling up e2e-test-nginx-rc-efc775370d4e61921a94c5804aec4651 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-efc775370d4e61921a94c5804aec4651 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-efc775370d4e61921a94c5804aec4651 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
May 16 13:37:35.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-8752'
May 16 13:37:35.886: INFO: stderr: ""
May 16 13:37:35.886: INFO: stdout: "e2e-test-nginx-rc-efc775370d4e61921a94c5804aec4651-xhxfw "
May 16 13:37:35.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods e2e-test-nginx-rc-efc775370d4e61921a94c5804aec4651-xhxfw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8752'
May 16 13:37:35.981: INFO: stderr: ""
May 16 13:37:35.981: INFO: stdout: "true"
May 16 13:37:35.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 get pods e2e-test-nginx-rc-efc775370d4e61921a94c5804aec4651-xhxfw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8752'
May 16 13:37:36.078: INFO: stderr: ""
May 16 13:37:36.078: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
May 16 13:37:36.078: INFO: e2e-test-nginx-rc-efc775370d4e61921a94c5804aec4651-xhxfw is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
May 16 13:37:36.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-005417164 delete rc e2e-test-nginx-rc --namespace=kubectl-8752'
May 16 13:37:36.200: INFO: stderr: ""
May 16 13:37:36.200: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:37:36.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8752" for this suite.
May 16 13:37:42.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:37:42.771: INFO: namespace kubectl-8752 deletion completed in 6.56331569s

• [SLOW TEST:23.360 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:37:42.776: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-c8071100-77df-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume secrets
May 16 13:37:42.932: INFO: Waiting up to 5m0s for pod "pod-secrets-c808f199-77df-11e9-8275-42209be91bd2" in namespace "secrets-9079" to be "success or failure"
May 16 13:37:42.945: INFO: Pod "pod-secrets-c808f199-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.606845ms
May 16 13:37:44.952: INFO: Pod "pod-secrets-c808f199-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019752511s
May 16 13:37:46.960: INFO: Pod "pod-secrets-c808f199-77df-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028422647s
STEP: Saw pod success
May 16 13:37:46.960: INFO: Pod "pod-secrets-c808f199-77df-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:37:46.968: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-secrets-c808f199-77df-11e9-8275-42209be91bd2 container secret-volume-test: <nil>
STEP: delete the pod
May 16 13:37:47.070: INFO: Waiting for pod pod-secrets-c808f199-77df-11e9-8275-42209be91bd2 to disappear
May 16 13:37:47.078: INFO: Pod pod-secrets-c808f199-77df-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:37:47.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9079" for this suite.
May 16 13:37:53.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:37:53.489: INFO: namespace secrets-9079 deletion completed in 6.402518808s

• [SLOW TEST:10.714 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 16 13:37:53.492: INFO: >>> kubeConfig: /tmp/kubeconfig-005417164
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-ce69fba3-77df-11e9-8275-42209be91bd2
STEP: Creating a pod to test consume configMaps
May 16 13:37:53.635: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ce6cde77-77df-11e9-8275-42209be91bd2" in namespace "projected-9894" to be "success or failure"
May 16 13:37:53.641: INFO: Pod "pod-projected-configmaps-ce6cde77-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.585241ms
May 16 13:37:55.650: INFO: Pod "pod-projected-configmaps-ce6cde77-77df-11e9-8275-42209be91bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014459098s
May 16 13:37:57.656: INFO: Pod "pod-projected-configmaps-ce6cde77-77df-11e9-8275-42209be91bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020543879s
STEP: Saw pod success
May 16 13:37:57.656: INFO: Pod "pod-projected-configmaps-ce6cde77-77df-11e9-8275-42209be91bd2" satisfied condition "success or failure"
May 16 13:37:57.663: INFO: Trying to get logs from node machine-kubermatic-conformancecluster-kik2tgqc8i-8ca7u pod pod-projected-configmaps-ce6cde77-77df-11e9-8275-42209be91bd2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 16 13:37:57.717: INFO: Waiting for pod pod-projected-configmaps-ce6cde77-77df-11e9-8275-42209be91bd2 to disappear
May 16 13:37:57.723: INFO: Pod pod-projected-configmaps-ce6cde77-77df-11e9-8275-42209be91bd2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 16 13:37:57.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9894" for this suite.
May 16 13:38:03.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 16 13:38:04.121: INFO: namespace projected-9894 deletion completed in 6.391737532s

• [SLOW TEST:10.630 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSMay 16 13:38:04.121: INFO: Running AfterSuite actions on all nodes
May 16 13:38:04.122: INFO: Running AfterSuite actions on node 1
May 16 13:38:04.122: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 6331.635 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h45m33.157103538s
Test Suite Passed
